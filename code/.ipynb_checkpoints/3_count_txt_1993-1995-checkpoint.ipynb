{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\github\\\\narrative_conservatism\\\\code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### import packages\n",
    "import os, nltk, numpy as np, pandas as pd, time, textstat\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from time import process_time\n",
    "\n",
    "##########################################################\n",
    "##################### parameter ##########################\n",
    "##########################################################\n",
    "obj_type = '10-Q'\n",
    "period_start = 1993 # included\n",
    "period_end = 1995 # included\n",
    "\n",
    "############### Set working directory to parent directory\n",
    "os.getcwd()\n",
    "# os.chdir('F:\\\\github\\\\narrative_conservatism\\\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Read LM disctionary\n",
    "LM = pd.read_excel('..\\\\LM\\\\LoughranMcDonald_MasterDictionary_2018.xlsx', encoding = \"utf-8\")\n",
    "\n",
    "############### Create negative, positive, uncertainty, litigious, constraining and modal word lists\n",
    "lm_neg = LM.loc[LM['Negative'] != 0]['Word'].values.tolist()\n",
    "lm_pos = LM.loc[LM['Positive'] != 0]['Word'].values.tolist()\n",
    "lm_uctt = LM.loc[LM['Uncertainty'] != 0]['Word'].values.tolist()\n",
    "lm_lit = LM.loc[LM['Litigious'] != 0]['Word'].values.tolist()\n",
    "lm_cstr = LM.loc[LM['Constraining'] != 0]['Word'].values.tolist()\n",
    "\n",
    "lm_modal1 = LM.loc[LM['Modal'] == 1]['Word'].values.tolist()\n",
    "lm_modal2 = LM.loc[LM['Modal'] == 2]['Word'].values.tolist()\n",
    "lm_modal3 = LM.loc[LM['Modal'] == 3]['Word'].values.tolist()\n",
    "\n",
    "lm_neg = [w.lower() for w in lm_neg]\n",
    "lm_pos = [w.lower() for w in lm_pos]\n",
    "lm_uctt = [w.lower() for w in lm_uctt]\n",
    "lm_lit = [w.lower() for w in lm_lit]\n",
    "lm_cstr = [w.lower() for w in lm_cstr]\n",
    "lm_modal1 = [w.lower() for w in lm_modal1]\n",
    "lm_modal2 = [w.lower() for w in lm_modal2]\n",
    "lm_modal3 = [w.lower() for w in lm_modal3]\n",
    "\n",
    "############## Read and create stop words list\n",
    "lm_stop = list()\n",
    "with open('..\\\\LM\\\\StopWords_Generic.txt', \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n', '')\n",
    "        lm_stop.append(line)\n",
    "        \n",
    "lm_stop = [w.lower() for w in lm_stop]\n",
    "\n",
    "############# Create a negation word list\n",
    "gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Read GI disctionary\n",
    "GI_cols = ['Entry', 'Source', 'Positiv', 'Negativ']\n",
    "GI = pd.read_excel('..\\\\LM\\\\GI\\\\inquirerbasic.xls', encoding = \"utf-8\", usecols = GI_cols)\n",
    "GI = GI[(GI['Entry'].str.endswith('#1') == True) | (GI['Entry'].str.contains('#') == False)]\n",
    "GI['Entry'] = GI['Entry'].str.replace('#1','') \n",
    "\n",
    "############### Create negative, positive, uncertainty, litigious, constraining and modal word lists\n",
    "gi_neg = GI.loc[GI['Negativ'].notnull()]['Entry'].values.tolist()\n",
    "gi_pos = GI.loc[GI['Positiv'].notnull()]['Entry'].values.tolist()\n",
    "\n",
    "gi_neg = [w.lower() for w in gi_neg]\n",
    "gi_pos = [w.lower() for w in gi_pos]\n",
    "\n",
    "############### Create Henry disctionary (HENRY 2008)\n",
    "he_neg = ['negative', 'negatives', 'fail', 'fails', 'failing', 'failure', 'weak', 'weakness', 'weaknesses', 'difficult', 'difficulty', 'hurdle', 'hurdles', 'obstacle', 'obstacles', 'slump', 'slumps', 'slumping', 'slumped', 'uncertain', 'uncertainty', 'unsettled', 'unfavorable', 'downturn', 'depressed', 'disappoint', 'disappoints', 'disappointing', 'disappointed', 'disappointment', 'risk', 'risks', 'risky', 'threat', 'threats', 'penalty', 'penalties', 'down', 'decrease', 'decreases', 'decreasing', 'decreased', 'decline', 'declines', 'declining', 'declined', 'fall', 'falls', 'falling', 'fell', 'fallen', 'drop', 'drops', 'dropping', 'dropped', 'deteriorate', 'deteriorates', 'deteriorating', 'deteriorated', 'worsen', 'worsens', 'worsening', 'weaken', 'weakens', 'weakening', 'weakened', 'worse', 'worst', 'low', 'lower', 'lowest', 'less', 'least', 'smaller', 'smallest', 'shrink']\n",
    "he_pos = ['positive', 'positives', 'success', 'successes', 'successful', 'succeed', 'succeeds', 'succeeding', 'succeeded', 'accomplish', 'accomplishes', 'accomplishing', 'accomplished', 'accomplishment', 'accomplishments', 'strong', 'strength', 'strengths', 'certain', 'certainty', 'definite', 'solid', 'excellent', 'good', 'leading', 'achieve', 'achieves', 'achieved', 'achieving', 'achievement', 'achievements', 'progress', 'progressing', 'deliver', 'delivers', 'delivered', 'delivering', 'leader', 'leading', 'pleased', 'reward', 'rewards', 'rewarding', 'rewarded', 'opportunity', 'opportunities', 'enjoy', 'enjoys', 'enjoying', 'enjoyed', 'encouraged', 'encouraging', 'up', 'increase', 'increases', 'increasing', 'increased', 'rise', 'rises', 'rising', 'rose', 'risen', 'improve', 'improves', 'improving', 'improved', 'improvement', 'improvements', 'strengthen', 'strengthens', 'strengthening', 'strengthened', 'stronger', 'strongest', 'better', 'best', 'more', 'most', 'above', 'record', 'high', 'higher', 'highest', 'greater', 'greatest', 'larger', 'largest', 'grow', 'grows', 'growing', 'grew', 'grown', 'growth', 'expand', 'expands', 'expanding', 'expanded', 'expansion', 'exceed', 'exceeds', 'exceeded', 'exceeding', 'beat', 'beats', 'beating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1051"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################\n",
    "#################### FOR ALL PROCESSED FILES LOOP ###################\n",
    "#####################################################################\n",
    "\n",
    "############# Create input txt file index\n",
    "processed = list()\n",
    "for subdir, dirs, files in os.walk(\"H:\\\\data\\\\edgar\\\\processed\\\\\" + obj_type + '\\\\' + str(period_start) + '-' + str(period_end)):\n",
    "    for file in files:\n",
    "        processed.append(os.path.join(subdir, file))\n",
    "\n",
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define a function count_occurrence to count the number of words in tup that pertaining to a list \n",
    "def count_occurrence(tup, lst): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst: \n",
    "            count+= 1\n",
    "      \n",
    "    return count\n",
    "\n",
    "### Define a function count_negation to count cases where negation occurs within four or fewer words from a word identified in list.\n",
    "def count_negation(tup, lst, negation): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst:\n",
    "            if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "                neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "                for neighborw in neighbor:\n",
    "                    if neighborw in negation:\n",
    "                        count+= 1\n",
    "\n",
    "            if tup.index(item)-4 < 0:\n",
    "                pre = tup[0:tup.index(item)+4]\n",
    "                for prew in pre:\n",
    "                    if prew in negation:\n",
    "                        count+= 1\n",
    "                        \n",
    "            if tup.index(item)+4 > len(tup):\n",
    "                post = tup[tup.index(item)-4:len(tup)]\n",
    "                for postw in post:\n",
    "                    if postw in negation:\n",
    "                        count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [22:27<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "############ Full Text Raw Count\n",
    "accnum = list()\n",
    "\n",
    "nw = list()\n",
    "nvocab = list()\n",
    "\n",
    "n_neg = list()\n",
    "n_pos = list()\n",
    "n_neg_gi = list()\n",
    "n_pos_gi = list()\n",
    "n_neg_he = list()\n",
    "n_pos_he = list()\n",
    "n_uctt = list()\n",
    "n_lit = list()\n",
    "n_cstr = list()\n",
    "n_modal1 = list()\n",
    "n_modal2 = list()\n",
    "n_modal3 = list()\n",
    "n_negation = list()\n",
    "READ = list()\n",
    "\n",
    "############ Word Tokenization, count nword and nvocab, count negative, positive, uncertainty, litigious, constraining and modal words\n",
    "for text in tqdm(processed):\n",
    "    ############# Create an array of accession number\n",
    "    a = text.split(\"\\\\\")[6].split(\".\")[0]\n",
    "    accnum.append(a)\n",
    "    \n",
    "    ############# Read processed txt file\n",
    "    with open(text, 'r',  encoding = \"utf-8\") as file:\n",
    "        contents = file.read().replace('\\n', ' ').replace(u'\\xa0', u' ')\n",
    "        # print(repr(contents))\n",
    "        \n",
    "        ############ Word Tokenization\n",
    "        ## Raw tokens: including punctuations, numbers etc.\n",
    "        tokens = word_tokenize(contents)\n",
    "\n",
    "        ## Convert all words into small cases\n",
    "        ## Keep tokens that purely consist of alphabetic characters only\n",
    "        ## Delete single-character words except for 'I'\n",
    "        words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "        \n",
    "        ########### Delete words with lenth smaller than 1% and largr than 99% of the document\n",
    "        # wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "        # wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "        # words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "        vocab = sorted(set(words))\n",
    "        \n",
    "        ########### Save text statistics\n",
    "        ##### 1. nw 2. nvocab 3. tone 4. readability\n",
    "        \n",
    "        ## 1. nw\n",
    "        a = len(words)\n",
    "        nw.append(a)\n",
    "        \n",
    "        ## 2. nvocab\n",
    "        b = len(vocab)\n",
    "        nvocab.append(b)\n",
    "        \n",
    "        ## 3. tone\n",
    "        neg = count_occurrence(words, lm_neg)\n",
    "        n_neg.append(neg)\n",
    "        pos = count_occurrence(words, lm_pos)\n",
    "        n_pos.append(pos)\n",
    "        uctt = count_occurrence(words, lm_uctt)\n",
    "        n_uctt.append(uctt)\n",
    "        lit = count_occurrence(words, lm_lit)\n",
    "        n_lit.append(lit)\n",
    "        cstr = count_occurrence(words, lm_cstr)\n",
    "        n_cstr.append(cstr)\n",
    "        modal1 = count_occurrence(words, lm_modal1)\n",
    "        n_modal1.append(modal1)\n",
    "        modal2 = count_occurrence(words, lm_modal2)\n",
    "        n_modal2.append(modal2)\n",
    "        modal3 = count_occurrence(words, lm_modal3)\n",
    "        n_modal3.append(modal3)\n",
    "        negation = count_negation(words, lm_pos, gt_negation)\n",
    "        n_negation.append(negation)\n",
    "        \n",
    "        neg_gi = count_occurrence(words, gi_neg)\n",
    "        n_neg_gi.append(neg_gi)\n",
    "        pos_gi = count_occurrence(words, gi_pos)\n",
    "        n_pos_gi.append(pos_gi)\n",
    "        \n",
    "        neg_he = count_occurrence(words, he_neg)\n",
    "        n_neg_he.append(neg_he)\n",
    "        pos_he = count_occurrence(words, he_pos)\n",
    "        n_pos_he.append(pos_he)\n",
    "        \n",
    "        ## 4. readability\n",
    "        read = textstat.gunning_fog(contents)\n",
    "        READ.append(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnum</th>\n",
       "      <th>nw</th>\n",
       "      <th>nvocab</th>\n",
       "      <th>n_neg</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>n_uctt</th>\n",
       "      <th>n_lit</th>\n",
       "      <th>n_cstr</th>\n",
       "      <th>n_modal_week</th>\n",
       "      <th>n_modal_moderate</th>\n",
       "      <th>n_modal_strong</th>\n",
       "      <th>n_negation</th>\n",
       "      <th>READ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0000002969-20-000010</td>\n",
       "      <td>13079</td>\n",
       "      <td>1731</td>\n",
       "      <td>139</td>\n",
       "      <td>90</td>\n",
       "      <td>166</td>\n",
       "      <td>167</td>\n",
       "      <td>81</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>17.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0000003545-20-000039</td>\n",
       "      <td>12047</td>\n",
       "      <td>1594</td>\n",
       "      <td>126</td>\n",
       "      <td>63</td>\n",
       "      <td>158</td>\n",
       "      <td>77</td>\n",
       "      <td>98</td>\n",
       "      <td>67</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>16.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000004127-20-000007</td>\n",
       "      <td>5976</td>\n",
       "      <td>1151</td>\n",
       "      <td>73</td>\n",
       "      <td>23</td>\n",
       "      <td>116</td>\n",
       "      <td>69</td>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>22.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0000004457-20-000027</td>\n",
       "      <td>18956</td>\n",
       "      <td>1886</td>\n",
       "      <td>195</td>\n",
       "      <td>90</td>\n",
       "      <td>215</td>\n",
       "      <td>158</td>\n",
       "      <td>130</td>\n",
       "      <td>35</td>\n",
       "      <td>55</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>18.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000006281-20-000013</td>\n",
       "      <td>16350</td>\n",
       "      <td>2122</td>\n",
       "      <td>403</td>\n",
       "      <td>140</td>\n",
       "      <td>356</td>\n",
       "      <td>204</td>\n",
       "      <td>139</td>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>207</td>\n",
       "      <td>4</td>\n",
       "      <td>26.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1046</td>\n",
       "      <td>0001753926-20-000021</td>\n",
       "      <td>10054</td>\n",
       "      <td>1397</td>\n",
       "      <td>115</td>\n",
       "      <td>62</td>\n",
       "      <td>133</td>\n",
       "      <td>79</td>\n",
       "      <td>40</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>17.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>0001757898-20-000003</td>\n",
       "      <td>16012</td>\n",
       "      <td>1829</td>\n",
       "      <td>273</td>\n",
       "      <td>95</td>\n",
       "      <td>214</td>\n",
       "      <td>211</td>\n",
       "      <td>78</td>\n",
       "      <td>34</td>\n",
       "      <td>47</td>\n",
       "      <td>89</td>\n",
       "      <td>3</td>\n",
       "      <td>19.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048</td>\n",
       "      <td>0001766016-20-000002</td>\n",
       "      <td>8034</td>\n",
       "      <td>1519</td>\n",
       "      <td>113</td>\n",
       "      <td>53</td>\n",
       "      <td>104</td>\n",
       "      <td>67</td>\n",
       "      <td>80</td>\n",
       "      <td>73</td>\n",
       "      <td>33</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>19.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>0001772016-20-000018</td>\n",
       "      <td>10832</td>\n",
       "      <td>1412</td>\n",
       "      <td>151</td>\n",
       "      <td>60</td>\n",
       "      <td>102</td>\n",
       "      <td>131</td>\n",
       "      <td>58</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>20.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0001773383-20-000004</td>\n",
       "      <td>30196</td>\n",
       "      <td>2693</td>\n",
       "      <td>754</td>\n",
       "      <td>219</td>\n",
       "      <td>641</td>\n",
       "      <td>338</td>\n",
       "      <td>214</td>\n",
       "      <td>98</td>\n",
       "      <td>110</td>\n",
       "      <td>400</td>\n",
       "      <td>30</td>\n",
       "      <td>17.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1051 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    accnum     nw  nvocab  n_neg  n_pos  n_uctt  n_lit  \\\n",
       "0     0000002969-20-000010  13079    1731    139     90     166    167   \n",
       "1     0000003545-20-000039  12047    1594    126     63     158     77   \n",
       "2     0000004127-20-000007   5976    1151     73     23     116     69   \n",
       "3     0000004457-20-000027  18956    1886    195     90     215    158   \n",
       "4     0000006281-20-000013  16350    2122    403    140     356    204   \n",
       "...                    ...    ...     ...    ...    ...     ...    ...   \n",
       "1046  0001753926-20-000021  10054    1397    115     62     133     79   \n",
       "1047  0001757898-20-000003  16012    1829    273     95     214    211   \n",
       "1048  0001766016-20-000002   8034    1519    113     53     104     67   \n",
       "1049  0001772016-20-000018  10832    1412    151     60     102    131   \n",
       "1050  0001773383-20-000004  30196    2693    754    219     641    338   \n",
       "\n",
       "      n_cstr  n_modal_week  n_modal_moderate  n_modal_strong  n_negation  \\\n",
       "0         81            20                40              37           1   \n",
       "1         98            67                29              29           1   \n",
       "2         50             9                22              37           1   \n",
       "3        130            35                55              43           0   \n",
       "4        139            52                45             207           4   \n",
       "...      ...           ...               ...             ...         ...   \n",
       "1046      40            18                29              29           2   \n",
       "1047      78            34                47              89           3   \n",
       "1048      80            73                33              42          17   \n",
       "1049      58            24                20              23           1   \n",
       "1050     214            98               110             400          30   \n",
       "\n",
       "       READ  \n",
       "0     17.17  \n",
       "1     16.66  \n",
       "2     22.43  \n",
       "3     18.12  \n",
       "4     26.95  \n",
       "...     ...  \n",
       "1046  17.94  \n",
       "1047  19.02  \n",
       "1048  19.74  \n",
       "1049  20.31  \n",
       "1050  17.23  \n",
       "\n",
       "[1051 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Create Data Frame: full document\n",
    "d = {'accnum': accnum, 'nw': nw, 'nvocab': nvocab, 'n_neg': n_neg, 'n_pos': n_pos, 'n_neg_gi': n_neg_gi, 'n_pos_gi': n_pos_gi, 'n_neg_he': n_neg_he, 'n_pos_he': n_pos_he, 'n_uctt': n_uctt, 'n_lit': n_lit, 'n_cstr': n_cstr, \\\n",
    "     'n_modal_week': n_modal1, 'n_modal_moderate': n_modal2, 'n_modal_strong': n_modal3, 'n_negation': n_negation, 'READ': READ}\n",
    "\n",
    "text_data = pd.DataFrame(data=d)\n",
    "text_data.to_csv('..\\\\filings\\\\text_data_' + obj_type + '_' + str(period_start) + '-' + str(period_end) + '.csv', index=False)\n",
    "\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [05:42<00:00,  3.07it/s]\n"
     ]
    }
   ],
   "source": [
    "############ MDA and NOTE Raw Count\n",
    "accnum = list()\n",
    "\n",
    "nw_mda = list()\n",
    "nvocab_mda = list()\n",
    "\n",
    "n_neg_mda = list()\n",
    "n_pos_mda = list()\n",
    "n_uctt_mda = list()\n",
    "n_lit_mda = list()\n",
    "n_cstr_mda = list()\n",
    "n_modal1_mda = list()\n",
    "n_modal2_mda = list()\n",
    "n_modal3_mda = list()\n",
    "n_negation_mda = list()\n",
    "\n",
    "READ_mda = list()\n",
    "\n",
    "nw_note = list()\n",
    "nvocab_note = list()\n",
    "\n",
    "n_neg_note = list()\n",
    "n_pos_note = list()\n",
    "n_uctt_note = list()\n",
    "n_lit_note = list()\n",
    "n_cstr_note = list()\n",
    "n_modal1_note = list()\n",
    "n_modal2_note = list()\n",
    "n_modal3_note = list()\n",
    "n_negation_note = list()\n",
    "\n",
    "READ_note = list()\n",
    "\n",
    "############ Word Tokenization, count nword and nvocab, count negative, positive, uncertainty, litigious, constraining and modal words\n",
    "for text in tqdm(processed):\n",
    "    ############# Create an array of accession number\n",
    "    a = text.split(\"\\\\\")[6].split(\".\")[0]\n",
    "    accnum.append(a)\n",
    "    \n",
    "    ############# Read processed txt file\n",
    "    with open(text, 'r',  encoding = \"utf-8\") as file:\n",
    "        contents = file.read().replace('\\n', ' ').replace(u'\\xa0', u' ')\n",
    "        ############################## TO EXTRACT MDA AND NOTES SECTION, UNCOMMENT THIS SECTION ################################\n",
    "        try:\n",
    "            mda = contents[contents.index(\"ITEM 2.\"):contents.index(\"ITEM 3.\")]\n",
    "        except:\n",
    "            try:\n",
    "                mda = contents[contents.index(\"Item 2.\"):contents.index(\"Item 3.\")]\n",
    "            except:\n",
    "                try:\n",
    "                    mda = contents[contents.index(\"ITEM 2\"):contents.index(\"ITEM 3\")]\n",
    "                except:\n",
    "                    try:\n",
    "                        mda = contents[contents.index(\"Item 2\"):contents.index(\"Item 3\")]\n",
    "                    except:\n",
    "                        mda = ''\n",
    "                        pass\n",
    "                    \n",
    "        try:\n",
    "            note = contents[contents.index(\"NOTES TO\"):contents.index(\"ITEM 2.\")]\n",
    "        except:\n",
    "            try:\n",
    "                note = contents[contents.index(\"NOTES TO\"):contents.index(\"ITEM 2\")]\n",
    "            except:\n",
    "                try:\n",
    "                    note = contents[contents.index(\"Notes to\"):contents.index(\"Item 2.\")]\n",
    "                except:\n",
    "                    try:\n",
    "                        note = contents[contents.index(\"Notes to\"):contents.index(\"Item 2\")]\n",
    "                    except:\n",
    "                        note = ''\n",
    "                        pass\n",
    "        ###########################################################################################################\n",
    "        \n",
    "        ############ Word Tokenization\n",
    "        ## Raw tokens: including punctuations, numbers etc.\n",
    "        tokens_mda = word_tokenize(mda)\n",
    "        tokens_note = word_tokenize(note)\n",
    "        \n",
    "        ####################################################################\n",
    "\n",
    "        ## Convert all words into small cases\n",
    "        ## Keep tokens that purely consist of alphabetic characters only\n",
    "        ## Delete single-character words except for 'I'\n",
    "        words_mda = [w.lower() for w in tokens_mda if w.isalpha() and len(w)>1 or w =='i']\n",
    "        words_note = [w.lower() for w in tokens_note if w.isalpha() and len(w)>1 or w =='i']\n",
    "        \n",
    "        ########### Delete words with lenth smaller than 1% and largr than 99% of the document\n",
    "        # wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "        # wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "        # words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "        vocab_mda = sorted(set(words_mda))\n",
    "        vocab_note = sorted(set(words_note))\n",
    "        \n",
    "        ########### Save text statistics\n",
    "        ##### 1. nw 2. nvocab 3. tone 4. readability\n",
    "        \n",
    "        ## 1. nw\n",
    "        a_mda = len(words_mda)\n",
    "        nw_mda.append(a_mda)\n",
    "        a_note = len(words_note)\n",
    "        nw_note.append(a_note)\n",
    "        \n",
    "        ## 2. nvocab\n",
    "#         b_mda = len(vocab_mda)\n",
    "#         nvocab_mda.append(b_mda)\n",
    "#         b_note = len(vocab_note)\n",
    "#         nvocab_note.append(b_note)\n",
    "        \n",
    "        ## 3. tone\n",
    "        neg_mda = count_occurrence(words_mda, lm_neg)\n",
    "        n_neg_mda.append(neg_mda)\n",
    "        pos_mda = count_occurrence(words_mda, lm_pos)\n",
    "        n_pos_mda.append(pos_mda)\n",
    "#         uctt_mda = count_occurrence(words_mda, lm_uctt)\n",
    "#         n_uctt_mda.append(uctt_mda)\n",
    "#         lit_mda = count_occurrence(words_mda, lm_lit)\n",
    "#         n_lit_mda.append(lit_mda)\n",
    "#         cstr_mda = count_occurrence(words_mda, lm_cstr)\n",
    "#         n_cstr_mda.append(cstr_mda)\n",
    "#         modal1_mda = count_occurrence(words_mda, lm_modal1)\n",
    "#         n_modal1_mda.append(modal1_mda)\n",
    "#         modal2_mda = count_occurrence(words_mda, lm_modal2)\n",
    "#         n_modal2_mda.append(modal2_mda)\n",
    "#         modal3_mda = count_occurrence(words_mda, lm_modal3)\n",
    "#         n_modal3_mda.append(modal3_mda)\n",
    "        negation_mda = count_negation(words_mda, lm_pos, gt_negation)\n",
    "        n_negation_mda.append(negation_mda)\n",
    "        \n",
    "        neg_note = count_occurrence(words_note, lm_neg)\n",
    "        n_neg_note.append(neg_note)\n",
    "        pos_note = count_occurrence(words_note, lm_pos)\n",
    "        n_pos_note.append(pos_note)\n",
    "#         uctt_note = count_occurrence(words_note, lm_uctt)\n",
    "#         n_uctt_note.append(uctt_note)\n",
    "#         lit_note = count_occurrence(words_note, lm_lit)\n",
    "#         n_lit_note.append(lit_note)\n",
    "#         cstr_note = count_occurrence(words_note, lm_cstr)\n",
    "#         n_cstr_note.append(cstr_note)\n",
    "#         modal1_note = count_occurrence(words_note, lm_modal1)\n",
    "#         n_modal1_note.append(modal1_note)\n",
    "#         modal2_note = count_occurrence(words_note, lm_modal2)\n",
    "#         n_modal2_note.append(modal2_note)\n",
    "#         modal3_note = count_occurrence(words_note, lm_modal3)\n",
    "#         n_modal3_note.append(modal3_note)\n",
    "        negation_note = count_negation(words_note, lm_pos, gt_negation)\n",
    "        n_negation_note.append(negation_note)\n",
    "        \n",
    "        ## 4. readability\n",
    "        read_mda = textstat.gunning_fog(mda)\n",
    "        READ_mda.append(read_mda)\n",
    "        read_note = textstat.gunning_fog(note)\n",
    "        READ_note.append(read_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of filings whose MDA and NOTES are successfully extracted: 0.5975261655566128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnum</th>\n",
       "      <th>nw_mda</th>\n",
       "      <th>n_neg_mda</th>\n",
       "      <th>n_pos_mda</th>\n",
       "      <th>n_negation_mda</th>\n",
       "      <th>nw_note</th>\n",
       "      <th>n_neg_note</th>\n",
       "      <th>n_pos_note</th>\n",
       "      <th>n_negation_note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000004127-20-000007</td>\n",
       "      <td>1544</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2585</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0000004457-20-000027</td>\n",
       "      <td>8667</td>\n",
       "      <td>89</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>7863</td>\n",
       "      <td>73</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000006281-20-000013</td>\n",
       "      <td>3067</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>4851</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0000006951-20-000014</td>\n",
       "      <td>6441</td>\n",
       "      <td>75</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>7804</td>\n",
       "      <td>90</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0000008670-20-000007</td>\n",
       "      <td>5946</td>\n",
       "      <td>46</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>4037</td>\n",
       "      <td>44</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1043</td>\n",
       "      <td>0001744489-20-000046</td>\n",
       "      <td>6929</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>15947</td>\n",
       "      <td>196</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1044</td>\n",
       "      <td>0001748790-20-000010</td>\n",
       "      <td>4016</td>\n",
       "      <td>86</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>5728</td>\n",
       "      <td>103</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>0001757898-20-000003</td>\n",
       "      <td>6543</td>\n",
       "      <td>127</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>7777</td>\n",
       "      <td>119</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>0001772016-20-000018</td>\n",
       "      <td>2997</td>\n",
       "      <td>43</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>6041</td>\n",
       "      <td>74</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0001773383-20-000004</td>\n",
       "      <td>6134</td>\n",
       "      <td>40</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>7024</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>628 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    accnum  nw_mda  n_neg_mda  n_pos_mda  n_negation_mda  \\\n",
       "2     0000004127-20-000007    1544         13          8               1   \n",
       "3     0000004457-20-000027    8667         89         48               0   \n",
       "4     0000006281-20-000013    3067         10         18               1   \n",
       "6     0000006951-20-000014    6441         75         39               0   \n",
       "8     0000008670-20-000007    5946         46         81               0   \n",
       "...                    ...     ...        ...        ...             ...   \n",
       "1043  0001744489-20-000046    6929         80         30               0   \n",
       "1044  0001748790-20-000010    4016         86         28               0   \n",
       "1047  0001757898-20-000003    6543        127         53               1   \n",
       "1049  0001772016-20-000018    2997         43         20               0   \n",
       "1050  0001773383-20-000004    6134         40         46               2   \n",
       "\n",
       "      nw_note  n_neg_note  n_pos_note  n_negation_note  \n",
       "2        2585          34           8                0  \n",
       "3        7863          73          36                0  \n",
       "4        4851          30          32                0  \n",
       "6        7804          90          47                0  \n",
       "8        4037          44          21                0  \n",
       "...       ...         ...         ...              ...  \n",
       "1043    15947         196          73                0  \n",
       "1044     5728         103          33                0  \n",
       "1047     7777         119          38                2  \n",
       "1049     6041          74          30                0  \n",
       "1050     7024          49          20                0  \n",
       "\n",
       "[628 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Create Data Frame: MDA and NOTES\n",
    "d = {'accnum': accnum, 'nw_mda': nw_mda, 'n_neg_mda': n_neg_mda, 'n_pos_mda': n_pos_mda, 'n_negation_mda': n_negation_mda, 'nw_note': nw_note, 'n_neg_note': n_neg_note, 'n_pos_note': n_pos_note, 'n_negation_note': n_negation_note, 'READ_MDA':READ_mda, 'READ_NOTE':READ_note}\n",
    "#      'nvocab_mda': nvocab_mda, 'n_uctt_mda': n_uctt_mda, 'n_lit_mda': n_lit_mda, 'n_cstr_mda': n_cstr_mda, \\\n",
    "#      'n_modal_strong_mda': n_modal1_mda, 'n_modal_moderate_mda': n_modal2_mda, 'n_modal_weak_mda': n_modal3_mda, \\\n",
    "#      'nvocab_note': nvocab_note, 'n_uctt_note': n_uctt_note, 'n_lit_note': n_lit_note, 'n_cstr_note': n_cstr_note, \\\n",
    "#      'n_modal_strong_note': n_modal1_note, 'n_modal_moderate_note': n_modal2_note, 'n_modal_weak_note': n_modal3_note}\n",
    "\n",
    "text_data = pd.DataFrame(data=d)\n",
    "print('percentage of filings whose MDA and NOTES are successfully extracted: ' + str(text_data[(text_data['nw_mda']!=0) & (text_data['nw_note']!=0)].shape[0]/text_data.shape[0]))\n",
    "text_data = text_data[(text_data['nw_mda']!=0) & (text_data['nw_note']!=0)]\n",
    "\n",
    "text_data.to_csv('..\\\\filings\\\\text_data_section_' + str(period_start) + '-' + str(period_end) + '.csv', index=False)\n",
    "text_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################################\n",
    "# ################### FOR SINGLE FILE INSPECTION ######################\n",
    "# #####################################################################\n",
    "\n",
    "# ############ Word Tokenization\n",
    "# ## Raw tokens: including punctuations, numbers etc.\n",
    "# with open(processed[1], 'r',  encoding = \"utf-8\") as file:\n",
    "#     contents = file.read().replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "# tokens = word_tokenize(contents)\n",
    "\n",
    "# #tokens\n",
    "\n",
    "# ## Convert all words into small cases\n",
    "# ## And keep tokens that purely consist of alphabetic characters only\n",
    "# words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# # words[2500:2600]\n",
    "# # vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_occurrence(tup, lst): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst: \n",
    "#             count+= 1\n",
    "      \n",
    "#     return count\n",
    "\n",
    "# count_occurrence(words, lm_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing\n",
    "\n",
    "# def count_negation(tup, lst, negation): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst:\n",
    "#             if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "#                 neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "#                 for neighborw in neighbor:\n",
    "#                     if neighborw in negation:\n",
    "#                         count+= 1\n",
    "\n",
    "#             if tup.index(item)-4 < 0:\n",
    "#                 pre = tup[0:tup.index(item)+4]\n",
    "#                 for prew in pre:\n",
    "#                     if prew in negation:\n",
    "#                         count+= 1\n",
    "                        \n",
    "#             if tup.index(item)+4 > len(tup):\n",
    "#                 post = tup[tup.index(item)-4:len(tup)]\n",
    "#                 for postw in post:\n",
    "#                     if postw in negation:\n",
    "#                         count+= 1\n",
    "#     return count\n",
    "\n",
    "# count_negation(words, lm_pos, gt_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########### Winsorize words with lenth smaller than 1% and largr than 99% of the document\n",
    "# wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "# wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "# words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### See the most common 20 words\n",
    "# fdist = nltk.FreqDist(words)\n",
    "# fdist.most_common(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
