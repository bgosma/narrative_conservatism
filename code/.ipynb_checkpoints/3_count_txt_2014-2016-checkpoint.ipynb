{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\github\\\\narrative_conservatism\\\\code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### import packages\n",
    "import os, nltk, numpy as np, pandas as pd, time, textstat\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from time import process_time\n",
    "\n",
    "##########################################################\n",
    "##################### parameter ##########################\n",
    "##########################################################\n",
    "obj_type = '10-Q'\n",
    "period_start = 2014 # included\n",
    "period_end = 2016 # included\n",
    "\n",
    "############### Set working directory to parent directory\n",
    "os.getcwd()\n",
    "# os.chdir('F:\\\\github\\\\narrative_conservatism\\\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Read LM disctionary\n",
    "LM = pd.read_excel('..\\\\LM\\\\LoughranMcDonald_MasterDictionary_2018.xlsx', encoding = \"utf-8\")\n",
    "\n",
    "############### Create negative, positive, uncertainty, litigious, constraining and modal word lists\n",
    "lm_neg = LM.loc[LM['Negative'] != 0]['Word'].values.tolist()\n",
    "lm_pos = LM.loc[LM['Positive'] != 0]['Word'].values.tolist()\n",
    "lm_uctt = LM.loc[LM['Uncertainty'] != 0]['Word'].values.tolist()\n",
    "lm_lit = LM.loc[LM['Litigious'] != 0]['Word'].values.tolist()\n",
    "lm_cstr = LM.loc[LM['Constraining'] != 0]['Word'].values.tolist()\n",
    "\n",
    "lm_modal1 = LM.loc[LM['Modal'] == 1]['Word'].values.tolist()\n",
    "lm_modal2 = LM.loc[LM['Modal'] == 2]['Word'].values.tolist()\n",
    "lm_modal3 = LM.loc[LM['Modal'] == 3]['Word'].values.tolist()\n",
    "\n",
    "lm_neg = [w.lower() for w in lm_neg]\n",
    "lm_pos = [w.lower() for w in lm_pos]\n",
    "lm_uctt = [w.lower() for w in lm_uctt]\n",
    "lm_lit = [w.lower() for w in lm_lit]\n",
    "lm_cstr = [w.lower() for w in lm_cstr]\n",
    "lm_modal1 = [w.lower() for w in lm_modal1]\n",
    "lm_modal2 = [w.lower() for w in lm_modal2]\n",
    "lm_modal3 = [w.lower() for w in lm_modal3]\n",
    "\n",
    "############## Read and create stop words list\n",
    "lm_stop = list()\n",
    "with open('..\\\\LM\\\\StopWords_Generic.txt', \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n', '')\n",
    "        lm_stop.append(line)\n",
    "        \n",
    "lm_stop = [w.lower() for w in lm_stop]\n",
    "\n",
    "############# Create a negation word list\n",
    "gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Read GI disctionary\n",
    "GI_cols = ['Entry', 'Source', 'Positiv', 'Negativ']\n",
    "GI = pd.read_excel('..\\\\LM\\\\GI\\\\inquirerbasic.xls', encoding = \"utf-8\", usecols = GI_cols)\n",
    "GI = GI[(GI['Entry'].str.endswith('#1') == True) | (GI['Entry'].str.contains('#') == False)]\n",
    "GI['Entry'] = GI['Entry'].str.replace('#1','') \n",
    "\n",
    "############### Create negative, positive, uncertainty, litigious, constraining and modal word lists\n",
    "gi_neg = GI.loc[GI['Negativ'].notnull()]['Entry'].values.tolist()\n",
    "gi_pos = GI.loc[GI['Positiv'].notnull()]['Entry'].values.tolist()\n",
    "\n",
    "gi_neg = [w.lower() for w in gi_neg]\n",
    "gi_pos = [w.lower() for w in gi_pos]\n",
    "\n",
    "############### Create Henry disctionary (HENRY 2008)\n",
    "he_neg = ['negative', 'negatives', 'fail', 'fails', 'failing', 'failure', 'weak', 'weakness', 'weaknesses', 'difficult', 'difficulty', 'hurdle', 'hurdles', 'obstacle', 'obstacles', 'slump', 'slumps', 'slumping', 'slumped', 'uncertain', 'uncertainty', 'unsettled', 'unfavorable', 'downturn', 'depressed', 'disappoint', 'disappoints', 'disappointing', 'disappointed', 'disappointment', 'risk', 'risks', 'risky', 'threat', 'threats', 'penalty', 'penalties', 'down', 'decrease', 'decreases', 'decreasing', 'decreased', 'decline', 'declines', 'declining', 'declined', 'fall', 'falls', 'falling', 'fell', 'fallen', 'drop', 'drops', 'dropping', 'dropped', 'deteriorate', 'deteriorates', 'deteriorating', 'deteriorated', 'worsen', 'worsens', 'worsening', 'weaken', 'weakens', 'weakening', 'weakened', 'worse', 'worst', 'low', 'lower', 'lowest', 'less', 'least', 'smaller', 'smallest', 'shrink']\n",
    "he_pos = ['positive', 'positives', 'success', 'successes', 'successful', 'succeed', 'succeeds', 'succeeding', 'succeeded', 'accomplish', 'accomplishes', 'accomplishing', 'accomplished', 'accomplishment', 'accomplishments', 'strong', 'strength', 'strengths', 'certain', 'certainty', 'definite', 'solid', 'excellent', 'good', 'leading', 'achieve', 'achieves', 'achieved', 'achieving', 'achievement', 'achievements', 'progress', 'progressing', 'deliver', 'delivers', 'delivered', 'delivering', 'leader', 'leading', 'pleased', 'reward', 'rewards', 'rewarding', 'rewarded', 'opportunity', 'opportunities', 'enjoy', 'enjoys', 'enjoying', 'enjoyed', 'encouraged', 'encouraging', 'up', 'increase', 'increases', 'increasing', 'increased', 'rise', 'rises', 'rising', 'rose', 'risen', 'improve', 'improves', 'improving', 'improved', 'improvement', 'improvements', 'strengthen', 'strengthens', 'strengthening', 'strengthened', 'stronger', 'strongest', 'better', 'best', 'more', 'most', 'above', 'record', 'high', 'higher', 'highest', 'greater', 'greatest', 'larger', 'largest', 'grow', 'grows', 'growing', 'grew', 'grown', 'growth', 'expand', 'expands', 'expanding', 'expanded', 'expansion', 'exceed', 'exceeds', 'exceeded', 'exceeding', 'beat', 'beats', 'beating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64077"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################\n",
    "#################### FOR ALL PROCESSED FILES LOOP ###################\n",
    "#####################################################################\n",
    "\n",
    "############# Create input txt file index\n",
    "processed = list()\n",
    "for subdir, dirs, files in os.walk(\"H:\\\\data\\\\edgar\\\\processed\\\\\" + obj_type + '\\\\' + str(period_start) + '-' + str(period_end)):\n",
    "    for file in files:\n",
    "        processed.append(os.path.join(subdir, file))\n",
    "\n",
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define a function count_occurrence to count the number of words in tup that pertaining to a list \n",
    "def count_occurrence(tup, lst): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst: \n",
    "            count+= 1\n",
    "      \n",
    "    return count\n",
    "\n",
    "### Define a function count_negation to count cases where negation occurs within four or fewer words from a word identified in list.\n",
    "def count_negation(tup, lst, negation): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst:\n",
    "            if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "                neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "                for neighborw in neighbor:\n",
    "                    if neighborw in negation:\n",
    "                        count+= 1\n",
    "\n",
    "            if tup.index(item)-4 < 0:\n",
    "                pre = tup[0:tup.index(item)+4]\n",
    "                for prew in pre:\n",
    "                    if prew in negation:\n",
    "                        count+= 1\n",
    "                        \n",
    "            if tup.index(item)+4 > len(tup):\n",
    "                post = tup[tup.index(item)-4:len(tup)]\n",
    "                for postw in post:\n",
    "                    if postw in negation:\n",
    "                        count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 24354/24354 [14:44:56<00:00,  2.18s/it]\n"
     ]
    }
   ],
   "source": [
    "############ Full Text Raw Count\n",
    "accnum = list()\n",
    "\n",
    "nw = list()\n",
    "nvocab = list()\n",
    "\n",
    "n_neg = list()\n",
    "n_pos = list()\n",
    "n_neg_gi = list()\n",
    "n_pos_gi = list()\n",
    "n_neg_he = list()\n",
    "n_pos_he = list()\n",
    "n_uctt = list()\n",
    "n_lit = list()\n",
    "n_cstr = list()\n",
    "n_modal1 = list()\n",
    "n_modal2 = list()\n",
    "n_modal3 = list()\n",
    "n_negation = list()\n",
    "READ = list()\n",
    "\n",
    "############ Word Tokenization, count nword and nvocab, count negative, positive, uncertainty, litigious, constraining and modal words\n",
    "for text in tqdm(processed):\n",
    "    ############# Create an array of accession number\n",
    "    a = text.split(\"\\\\\")[6].split(\".\")[0]\n",
    "    accnum.append(a)\n",
    "    \n",
    "    ############# Read processed txt file\n",
    "    with open(text, 'r',  encoding = \"utf-8\") as file:\n",
    "        contents = file.read().replace('\\n', ' ').replace(u'\\xa0', u' ')\n",
    "        # print(repr(contents))\n",
    "        \n",
    "        ############ Word Tokenization\n",
    "        ## Raw tokens: including punctuations, numbers etc.\n",
    "        tokens = word_tokenize(contents)\n",
    "\n",
    "        ## Convert all words into small cases\n",
    "        ## Keep tokens that purely consist of alphabetic characters only\n",
    "        ## Delete single-character words except for 'I'\n",
    "        words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "        \n",
    "        ########### Delete words with lenth smaller than 1% and largr than 99% of the document\n",
    "        # wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "        # wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "        # words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "        vocab = sorted(set(words))\n",
    "        \n",
    "        ########### Save text statistics\n",
    "        ##### 1. nw 2. nvocab 3. tone 4. readability\n",
    "        \n",
    "        ## 1. nw\n",
    "        a = len(words)\n",
    "        nw.append(a)\n",
    "        \n",
    "        ## 2. nvocab\n",
    "        b = len(vocab)\n",
    "        nvocab.append(b)\n",
    "        \n",
    "        ## 3. tone\n",
    "        neg = count_occurrence(words, lm_neg)\n",
    "        n_neg.append(neg)\n",
    "        pos = count_occurrence(words, lm_pos)\n",
    "        n_pos.append(pos)\n",
    "        uctt = count_occurrence(words, lm_uctt)\n",
    "        n_uctt.append(uctt)\n",
    "        lit = count_occurrence(words, lm_lit)\n",
    "        n_lit.append(lit)\n",
    "        cstr = count_occurrence(words, lm_cstr)\n",
    "        n_cstr.append(cstr)\n",
    "        modal1 = count_occurrence(words, lm_modal1)\n",
    "        n_modal1.append(modal1)\n",
    "        modal2 = count_occurrence(words, lm_modal2)\n",
    "        n_modal2.append(modal2)\n",
    "        modal3 = count_occurrence(words, lm_modal3)\n",
    "        n_modal3.append(modal3)\n",
    "        negation = count_negation(words, lm_pos, gt_negation)\n",
    "        n_negation.append(negation)\n",
    "        \n",
    "        neg_gi = count_occurrence(words, gi_neg)\n",
    "        n_neg_gi.append(neg_gi)\n",
    "        pos_gi = count_occurrence(words, gi_pos)\n",
    "        n_pos_gi.append(pos_gi)\n",
    "        \n",
    "        neg_he = count_occurrence(words, he_neg)\n",
    "        n_neg_he.append(neg_he)\n",
    "        pos_he = count_occurrence(words, he_pos)\n",
    "        n_pos_he.append(pos_he)\n",
    "        \n",
    "        ## 4. readability\n",
    "        read = textstat.gunning_fog(contents)\n",
    "        READ.append(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnum</th>\n",
       "      <th>nw</th>\n",
       "      <th>nvocab</th>\n",
       "      <th>n_neg</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>n_neg_gi</th>\n",
       "      <th>n_pos_gi</th>\n",
       "      <th>n_neg_he</th>\n",
       "      <th>n_pos_he</th>\n",
       "      <th>n_uctt</th>\n",
       "      <th>n_lit</th>\n",
       "      <th>n_cstr</th>\n",
       "      <th>n_modal_week</th>\n",
       "      <th>n_modal_moderate</th>\n",
       "      <th>n_modal_strong</th>\n",
       "      <th>n_negation</th>\n",
       "      <th>READ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0001300514-14-000022</td>\n",
       "      <td>20897</td>\n",
       "      <td>2116</td>\n",
       "      <td>525</td>\n",
       "      <td>82</td>\n",
       "      <td>339</td>\n",
       "      <td>640</td>\n",
       "      <td>89</td>\n",
       "      <td>182</td>\n",
       "      <td>273</td>\n",
       "      <td>553</td>\n",
       "      <td>108</td>\n",
       "      <td>51</td>\n",
       "      <td>54</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>14.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0001300514-15-000009</td>\n",
       "      <td>19416</td>\n",
       "      <td>2133</td>\n",
       "      <td>585</td>\n",
       "      <td>76</td>\n",
       "      <td>369</td>\n",
       "      <td>604</td>\n",
       "      <td>89</td>\n",
       "      <td>108</td>\n",
       "      <td>270</td>\n",
       "      <td>607</td>\n",
       "      <td>101</td>\n",
       "      <td>52</td>\n",
       "      <td>56</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>13.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0001300514-15-000013</td>\n",
       "      <td>21370</td>\n",
       "      <td>2182</td>\n",
       "      <td>624</td>\n",
       "      <td>87</td>\n",
       "      <td>430</td>\n",
       "      <td>673</td>\n",
       "      <td>131</td>\n",
       "      <td>126</td>\n",
       "      <td>284</td>\n",
       "      <td>685</td>\n",
       "      <td>105</td>\n",
       "      <td>56</td>\n",
       "      <td>61</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>13.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0001300514-15-000018</td>\n",
       "      <td>22055</td>\n",
       "      <td>2234</td>\n",
       "      <td>693</td>\n",
       "      <td>90</td>\n",
       "      <td>489</td>\n",
       "      <td>700</td>\n",
       "      <td>142</td>\n",
       "      <td>144</td>\n",
       "      <td>290</td>\n",
       "      <td>735</td>\n",
       "      <td>110</td>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>13.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0001300514-16-000028</td>\n",
       "      <td>20204</td>\n",
       "      <td>2276</td>\n",
       "      <td>800</td>\n",
       "      <td>99</td>\n",
       "      <td>479</td>\n",
       "      <td>661</td>\n",
       "      <td>90</td>\n",
       "      <td>132</td>\n",
       "      <td>250</td>\n",
       "      <td>901</td>\n",
       "      <td>109</td>\n",
       "      <td>49</td>\n",
       "      <td>63</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>13.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24349</td>\n",
       "      <td>0001683168-16-000914</td>\n",
       "      <td>13503</td>\n",
       "      <td>1609</td>\n",
       "      <td>161</td>\n",
       "      <td>99</td>\n",
       "      <td>176</td>\n",
       "      <td>667</td>\n",
       "      <td>77</td>\n",
       "      <td>110</td>\n",
       "      <td>148</td>\n",
       "      <td>125</td>\n",
       "      <td>91</td>\n",
       "      <td>39</td>\n",
       "      <td>45</td>\n",
       "      <td>61</td>\n",
       "      <td>6</td>\n",
       "      <td>21.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24350</td>\n",
       "      <td>0001683168-16-000935</td>\n",
       "      <td>6809</td>\n",
       "      <td>1063</td>\n",
       "      <td>64</td>\n",
       "      <td>24</td>\n",
       "      <td>58</td>\n",
       "      <td>279</td>\n",
       "      <td>36</td>\n",
       "      <td>50</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>39</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>20.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24351</td>\n",
       "      <td>0001683168-16-000952</td>\n",
       "      <td>4497</td>\n",
       "      <td>1071</td>\n",
       "      <td>57</td>\n",
       "      <td>40</td>\n",
       "      <td>71</td>\n",
       "      <td>187</td>\n",
       "      <td>26</td>\n",
       "      <td>49</td>\n",
       "      <td>74</td>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>18.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24352</td>\n",
       "      <td>0001683168-16-000994</td>\n",
       "      <td>9766</td>\n",
       "      <td>1211</td>\n",
       "      <td>108</td>\n",
       "      <td>64</td>\n",
       "      <td>138</td>\n",
       "      <td>428</td>\n",
       "      <td>44</td>\n",
       "      <td>64</td>\n",
       "      <td>92</td>\n",
       "      <td>27</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>15.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24353</td>\n",
       "      <td>0001683168-16-001000</td>\n",
       "      <td>12754</td>\n",
       "      <td>1841</td>\n",
       "      <td>400</td>\n",
       "      <td>141</td>\n",
       "      <td>302</td>\n",
       "      <td>622</td>\n",
       "      <td>90</td>\n",
       "      <td>156</td>\n",
       "      <td>342</td>\n",
       "      <td>125</td>\n",
       "      <td>99</td>\n",
       "      <td>83</td>\n",
       "      <td>73</td>\n",
       "      <td>241</td>\n",
       "      <td>24</td>\n",
       "      <td>18.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24354 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     accnum     nw  nvocab  n_neg  n_pos  n_neg_gi  n_pos_gi  \\\n",
       "0      0001300514-14-000022  20897    2116    525     82       339       640   \n",
       "1      0001300514-15-000009  19416    2133    585     76       369       604   \n",
       "2      0001300514-15-000013  21370    2182    624     87       430       673   \n",
       "3      0001300514-15-000018  22055    2234    693     90       489       700   \n",
       "4      0001300514-16-000028  20204    2276    800     99       479       661   \n",
       "...                     ...    ...     ...    ...    ...       ...       ...   \n",
       "24349  0001683168-16-000914  13503    1609    161     99       176       667   \n",
       "24350  0001683168-16-000935   6809    1063     64     24        58       279   \n",
       "24351  0001683168-16-000952   4497    1071     57     40        71       187   \n",
       "24352  0001683168-16-000994   9766    1211    108     64       138       428   \n",
       "24353  0001683168-16-001000  12754    1841    400    141       302       622   \n",
       "\n",
       "       n_neg_he  n_pos_he  n_uctt  n_lit  n_cstr  n_modal_week  \\\n",
       "0            89       182     273    553     108            51   \n",
       "1            89       108     270    607     101            52   \n",
       "2           131       126     284    685     105            56   \n",
       "3           142       144     290    735     110            59   \n",
       "4            90       132     250    901     109            49   \n",
       "...         ...       ...     ...    ...     ...           ...   \n",
       "24349        77       110     148    125      91            39   \n",
       "24350        36        50      66     23      39            18   \n",
       "24351        26        49      74     19      40            32   \n",
       "24352        44        64      92     27      40            31   \n",
       "24353        90       156     342    125      99            83   \n",
       "\n",
       "       n_modal_moderate  n_modal_strong  n_negation   READ  \n",
       "0                    54              67           1  14.66  \n",
       "1                    56              67           1  13.75  \n",
       "2                    61              71           1  13.75  \n",
       "3                    60              71           1  13.71  \n",
       "4                    63              83           1  13.29  \n",
       "...                 ...             ...         ...    ...  \n",
       "24349                45              61           6  21.82  \n",
       "24350                17              16           1  20.27  \n",
       "24351                40              33           1  18.28  \n",
       "24352                33              33          14  15.60  \n",
       "24353                73             241          24  18.42  \n",
       "\n",
       "[24354 rows x 17 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Create Data Frame: full document\n",
    "d = {'accnum': accnum, 'nw': nw, 'nvocab': nvocab, 'n_neg': n_neg, 'n_pos': n_pos, 'n_neg_gi': n_neg_gi, 'n_pos_gi': n_pos_gi, 'n_neg_he': n_neg_he, 'n_pos_he': n_pos_he, 'n_uctt': n_uctt, 'n_lit': n_lit, 'n_cstr': n_cstr, \\\n",
    "     'n_modal_week': n_modal1, 'n_modal_moderate': n_modal2, 'n_modal_strong': n_modal3, 'n_negation': n_negation, 'READ': READ}\n",
    "\n",
    "text_data = pd.DataFrame(data=d)\n",
    "text_data.to_csv('..\\\\filings\\\\text_data_' + obj_type + '_' + str(period_start) + '-' + str(period_end) + '.csv', index=False)\n",
    "\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [05:42<00:00,  3.07it/s]\n"
     ]
    }
   ],
   "source": [
    "############ MDA and NOTE Raw Count\n",
    "accnum = list()\n",
    "\n",
    "nw_mda = list()\n",
    "nvocab_mda = list()\n",
    "\n",
    "n_neg_mda = list()\n",
    "n_pos_mda = list()\n",
    "n_uctt_mda = list()\n",
    "n_lit_mda = list()\n",
    "n_cstr_mda = list()\n",
    "n_modal1_mda = list()\n",
    "n_modal2_mda = list()\n",
    "n_modal3_mda = list()\n",
    "n_negation_mda = list()\n",
    "\n",
    "READ_mda = list()\n",
    "\n",
    "nw_note = list()\n",
    "nvocab_note = list()\n",
    "\n",
    "n_neg_note = list()\n",
    "n_pos_note = list()\n",
    "n_uctt_note = list()\n",
    "n_lit_note = list()\n",
    "n_cstr_note = list()\n",
    "n_modal1_note = list()\n",
    "n_modal2_note = list()\n",
    "n_modal3_note = list()\n",
    "n_negation_note = list()\n",
    "\n",
    "READ_note = list()\n",
    "\n",
    "############ Word Tokenization, count nword and nvocab, count negative, positive, uncertainty, litigious, constraining and modal words\n",
    "for text in tqdm(processed):\n",
    "    ############# Create an array of accession number\n",
    "    a = text.split(\"\\\\\")[6].split(\".\")[0]\n",
    "    accnum.append(a)\n",
    "    \n",
    "    ############# Read processed txt file\n",
    "    with open(text, 'r',  encoding = \"utf-8\") as file:\n",
    "        contents = file.read().replace('\\n', ' ').replace(u'\\xa0', u' ')\n",
    "        ############################## TO EXTRACT MDA AND NOTES SECTION, UNCOMMENT THIS SECTION ################################\n",
    "        try:\n",
    "            mda = contents[contents.index(\"ITEM 2.\"):contents.index(\"ITEM 3.\")]\n",
    "        except:\n",
    "            try:\n",
    "                mda = contents[contents.index(\"Item 2.\"):contents.index(\"Item 3.\")]\n",
    "            except:\n",
    "                try:\n",
    "                    mda = contents[contents.index(\"ITEM 2\"):contents.index(\"ITEM 3\")]\n",
    "                except:\n",
    "                    try:\n",
    "                        mda = contents[contents.index(\"Item 2\"):contents.index(\"Item 3\")]\n",
    "                    except:\n",
    "                        mda = ''\n",
    "                        pass\n",
    "                    \n",
    "        try:\n",
    "            note = contents[contents.index(\"NOTES TO\"):contents.index(\"ITEM 2.\")]\n",
    "        except:\n",
    "            try:\n",
    "                note = contents[contents.index(\"NOTES TO\"):contents.index(\"ITEM 2\")]\n",
    "            except:\n",
    "                try:\n",
    "                    note = contents[contents.index(\"Notes to\"):contents.index(\"Item 2.\")]\n",
    "                except:\n",
    "                    try:\n",
    "                        note = contents[contents.index(\"Notes to\"):contents.index(\"Item 2\")]\n",
    "                    except:\n",
    "                        note = ''\n",
    "                        pass\n",
    "        ###########################################################################################################\n",
    "        \n",
    "        ############ Word Tokenization\n",
    "        ## Raw tokens: including punctuations, numbers etc.\n",
    "        tokens_mda = word_tokenize(mda)\n",
    "        tokens_note = word_tokenize(note)\n",
    "        \n",
    "        ####################################################################\n",
    "\n",
    "        ## Convert all words into small cases\n",
    "        ## Keep tokens that purely consist of alphabetic characters only\n",
    "        ## Delete single-character words except for 'I'\n",
    "        words_mda = [w.lower() for w in tokens_mda if w.isalpha() and len(w)>1 or w =='i']\n",
    "        words_note = [w.lower() for w in tokens_note if w.isalpha() and len(w)>1 or w =='i']\n",
    "        \n",
    "        ########### Delete words with lenth smaller than 1% and largr than 99% of the document\n",
    "        # wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "        # wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "        # words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "        vocab_mda = sorted(set(words_mda))\n",
    "        vocab_note = sorted(set(words_note))\n",
    "        \n",
    "        ########### Save text statistics\n",
    "        ##### 1. nw 2. nvocab 3. tone 4. readability\n",
    "        \n",
    "        ## 1. nw\n",
    "        a_mda = len(words_mda)\n",
    "        nw_mda.append(a_mda)\n",
    "        a_note = len(words_note)\n",
    "        nw_note.append(a_note)\n",
    "        \n",
    "        ## 2. nvocab\n",
    "#         b_mda = len(vocab_mda)\n",
    "#         nvocab_mda.append(b_mda)\n",
    "#         b_note = len(vocab_note)\n",
    "#         nvocab_note.append(b_note)\n",
    "        \n",
    "        ## 3. tone\n",
    "        neg_mda = count_occurrence(words_mda, lm_neg)\n",
    "        n_neg_mda.append(neg_mda)\n",
    "        pos_mda = count_occurrence(words_mda, lm_pos)\n",
    "        n_pos_mda.append(pos_mda)\n",
    "#         uctt_mda = count_occurrence(words_mda, lm_uctt)\n",
    "#         n_uctt_mda.append(uctt_mda)\n",
    "#         lit_mda = count_occurrence(words_mda, lm_lit)\n",
    "#         n_lit_mda.append(lit_mda)\n",
    "#         cstr_mda = count_occurrence(words_mda, lm_cstr)\n",
    "#         n_cstr_mda.append(cstr_mda)\n",
    "#         modal1_mda = count_occurrence(words_mda, lm_modal1)\n",
    "#         n_modal1_mda.append(modal1_mda)\n",
    "#         modal2_mda = count_occurrence(words_mda, lm_modal2)\n",
    "#         n_modal2_mda.append(modal2_mda)\n",
    "#         modal3_mda = count_occurrence(words_mda, lm_modal3)\n",
    "#         n_modal3_mda.append(modal3_mda)\n",
    "        negation_mda = count_negation(words_mda, lm_pos, gt_negation)\n",
    "        n_negation_mda.append(negation_mda)\n",
    "        \n",
    "        neg_note = count_occurrence(words_note, lm_neg)\n",
    "        n_neg_note.append(neg_note)\n",
    "        pos_note = count_occurrence(words_note, lm_pos)\n",
    "        n_pos_note.append(pos_note)\n",
    "#         uctt_note = count_occurrence(words_note, lm_uctt)\n",
    "#         n_uctt_note.append(uctt_note)\n",
    "#         lit_note = count_occurrence(words_note, lm_lit)\n",
    "#         n_lit_note.append(lit_note)\n",
    "#         cstr_note = count_occurrence(words_note, lm_cstr)\n",
    "#         n_cstr_note.append(cstr_note)\n",
    "#         modal1_note = count_occurrence(words_note, lm_modal1)\n",
    "#         n_modal1_note.append(modal1_note)\n",
    "#         modal2_note = count_occurrence(words_note, lm_modal2)\n",
    "#         n_modal2_note.append(modal2_note)\n",
    "#         modal3_note = count_occurrence(words_note, lm_modal3)\n",
    "#         n_modal3_note.append(modal3_note)\n",
    "        negation_note = count_negation(words_note, lm_pos, gt_negation)\n",
    "        n_negation_note.append(negation_note)\n",
    "        \n",
    "        ## 4. readability\n",
    "        read_mda = textstat.gunning_fog(mda)\n",
    "        READ_mda.append(read_mda)\n",
    "        read_note = textstat.gunning_fog(note)\n",
    "        READ_note.append(read_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of filings whose MDA and NOTES are successfully extracted: 0.5975261655566128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnum</th>\n",
       "      <th>nw_mda</th>\n",
       "      <th>n_neg_mda</th>\n",
       "      <th>n_pos_mda</th>\n",
       "      <th>n_negation_mda</th>\n",
       "      <th>nw_note</th>\n",
       "      <th>n_neg_note</th>\n",
       "      <th>n_pos_note</th>\n",
       "      <th>n_negation_note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000004127-20-000007</td>\n",
       "      <td>1544</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2585</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0000004457-20-000027</td>\n",
       "      <td>8667</td>\n",
       "      <td>89</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>7863</td>\n",
       "      <td>73</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000006281-20-000013</td>\n",
       "      <td>3067</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>4851</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0000006951-20-000014</td>\n",
       "      <td>6441</td>\n",
       "      <td>75</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>7804</td>\n",
       "      <td>90</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0000008670-20-000007</td>\n",
       "      <td>5946</td>\n",
       "      <td>46</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>4037</td>\n",
       "      <td>44</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1043</td>\n",
       "      <td>0001744489-20-000046</td>\n",
       "      <td>6929</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>15947</td>\n",
       "      <td>196</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1044</td>\n",
       "      <td>0001748790-20-000010</td>\n",
       "      <td>4016</td>\n",
       "      <td>86</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>5728</td>\n",
       "      <td>103</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>0001757898-20-000003</td>\n",
       "      <td>6543</td>\n",
       "      <td>127</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>7777</td>\n",
       "      <td>119</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>0001772016-20-000018</td>\n",
       "      <td>2997</td>\n",
       "      <td>43</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>6041</td>\n",
       "      <td>74</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0001773383-20-000004</td>\n",
       "      <td>6134</td>\n",
       "      <td>40</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>7024</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>628 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    accnum  nw_mda  n_neg_mda  n_pos_mda  n_negation_mda  \\\n",
       "2     0000004127-20-000007    1544         13          8               1   \n",
       "3     0000004457-20-000027    8667         89         48               0   \n",
       "4     0000006281-20-000013    3067         10         18               1   \n",
       "6     0000006951-20-000014    6441         75         39               0   \n",
       "8     0000008670-20-000007    5946         46         81               0   \n",
       "...                    ...     ...        ...        ...             ...   \n",
       "1043  0001744489-20-000046    6929         80         30               0   \n",
       "1044  0001748790-20-000010    4016         86         28               0   \n",
       "1047  0001757898-20-000003    6543        127         53               1   \n",
       "1049  0001772016-20-000018    2997         43         20               0   \n",
       "1050  0001773383-20-000004    6134         40         46               2   \n",
       "\n",
       "      nw_note  n_neg_note  n_pos_note  n_negation_note  \n",
       "2        2585          34           8                0  \n",
       "3        7863          73          36                0  \n",
       "4        4851          30          32                0  \n",
       "6        7804          90          47                0  \n",
       "8        4037          44          21                0  \n",
       "...       ...         ...         ...              ...  \n",
       "1043    15947         196          73                0  \n",
       "1044     5728         103          33                0  \n",
       "1047     7777         119          38                2  \n",
       "1049     6041          74          30                0  \n",
       "1050     7024          49          20                0  \n",
       "\n",
       "[628 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Create Data Frame: MDA and NOTES\n",
    "d = {'accnum': accnum, 'nw_mda': nw_mda, 'n_neg_mda': n_neg_mda, 'n_pos_mda': n_pos_mda, 'n_negation_mda': n_negation_mda, 'nw_note': nw_note, 'n_neg_note': n_neg_note, 'n_pos_note': n_pos_note, 'n_negation_note': n_negation_note, 'READ_MDA':READ_mda, 'READ_NOTE':READ_note}\n",
    "#      'nvocab_mda': nvocab_mda, 'n_uctt_mda': n_uctt_mda, 'n_lit_mda': n_lit_mda, 'n_cstr_mda': n_cstr_mda, \\\n",
    "#      'n_modal_strong_mda': n_modal1_mda, 'n_modal_moderate_mda': n_modal2_mda, 'n_modal_weak_mda': n_modal3_mda, \\\n",
    "#      'nvocab_note': nvocab_note, 'n_uctt_note': n_uctt_note, 'n_lit_note': n_lit_note, 'n_cstr_note': n_cstr_note, \\\n",
    "#      'n_modal_strong_note': n_modal1_note, 'n_modal_moderate_note': n_modal2_note, 'n_modal_weak_note': n_modal3_note}\n",
    "\n",
    "text_data = pd.DataFrame(data=d)\n",
    "print('percentage of filings whose MDA and NOTES are successfully extracted: ' + str(text_data[(text_data['nw_mda']!=0) & (text_data['nw_note']!=0)].shape[0]/text_data.shape[0]))\n",
    "text_data = text_data[(text_data['nw_mda']!=0) & (text_data['nw_note']!=0)]\n",
    "\n",
    "text_data.to_csv('..\\\\filings\\\\text_data_section_' + str(period_start) + '-' + str(period_end) + '.csv', index=False)\n",
    "text_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################################\n",
    "# ################### FOR SINGLE FILE INSPECTION ######################\n",
    "# #####################################################################\n",
    "\n",
    "# ############ Word Tokenization\n",
    "# ## Raw tokens: including punctuations, numbers etc.\n",
    "# with open(processed[1], 'r',  encoding = \"utf-8\") as file:\n",
    "#     contents = file.read().replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "# tokens = word_tokenize(contents)\n",
    "\n",
    "# #tokens\n",
    "\n",
    "# ## Convert all words into small cases\n",
    "# ## And keep tokens that purely consist of alphabetic characters only\n",
    "# words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# # words[2500:2600]\n",
    "# # vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_occurrence(tup, lst): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst: \n",
    "#             count+= 1\n",
    "      \n",
    "#     return count\n",
    "\n",
    "# count_occurrence(words, lm_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing\n",
    "\n",
    "# def count_negation(tup, lst, negation): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst:\n",
    "#             if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "#                 neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "#                 for neighborw in neighbor:\n",
    "#                     if neighborw in negation:\n",
    "#                         count+= 1\n",
    "\n",
    "#             if tup.index(item)-4 < 0:\n",
    "#                 pre = tup[0:tup.index(item)+4]\n",
    "#                 for prew in pre:\n",
    "#                     if prew in negation:\n",
    "#                         count+= 1\n",
    "                        \n",
    "#             if tup.index(item)+4 > len(tup):\n",
    "#                 post = tup[tup.index(item)-4:len(tup)]\n",
    "#                 for postw in post:\n",
    "#                     if postw in negation:\n",
    "#                         count+= 1\n",
    "#     return count\n",
    "\n",
    "# count_negation(words, lm_pos, gt_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########### Winsorize words with lenth smaller than 1% and largr than 99% of the document\n",
    "# wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "# wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "# words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### See the most common 20 words\n",
    "# fdist = nltk.FreqDist(words)\n",
    "# fdist.most_common(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
