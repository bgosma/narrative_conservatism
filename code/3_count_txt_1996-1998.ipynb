{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\github\\\\narrative_conservatism\\\\code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### import packages\n",
    "import os, nltk, numpy as np, pandas as pd, time\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from time import process_time\n",
    "\n",
    "##########################################################\n",
    "##################### parameter ##########################\n",
    "##########################################################\n",
    "obj_type = '10-Q'\n",
    "period_start = 1996 # included\n",
    "period_end = 1998 # included\n",
    "\n",
    "############### Set working directory to parent directory\n",
    "os.getcwd()\n",
    "# os.chdir('F:\\\\github\\\\narrative_conservatism\\\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Read LM disctionary\n",
    "LM = pd.read_excel('..\\\\LM\\\\LoughranMcDonald_MasterDictionary_2018.xlsx', encoding = \"utf-8\")\n",
    "\n",
    "############### Create negative, positive, uncertainty, litigious, constraining and modal word lists\n",
    "lm_neg = LM.loc[LM['Negative'] != 0]['Word'].values.tolist()\n",
    "lm_pos = LM.loc[LM['Positive'] != 0]['Word'].values.tolist()\n",
    "lm_uctt = LM.loc[LM['Uncertainty'] != 0]['Word'].values.tolist()\n",
    "lm_lit = LM.loc[LM['Litigious'] != 0]['Word'].values.tolist()\n",
    "lm_cstr = LM.loc[LM['Constraining'] != 0]['Word'].values.tolist()\n",
    "\n",
    "lm_modal1 = LM.loc[LM['Modal'] == 1]['Word'].values.tolist()\n",
    "lm_modal2 = LM.loc[LM['Modal'] == 2]['Word'].values.tolist()\n",
    "lm_modal3 = LM.loc[LM['Modal'] == 3]['Word'].values.tolist()\n",
    "\n",
    "lm_neg = [w.lower() for w in lm_neg]\n",
    "lm_pos = [w.lower() for w in lm_pos]\n",
    "lm_uctt = [w.lower() for w in lm_uctt]\n",
    "lm_lit = [w.lower() for w in lm_lit]\n",
    "lm_cstr = [w.lower() for w in lm_cstr]\n",
    "lm_modal1 = [w.lower() for w in lm_modal1]\n",
    "lm_modal2 = [w.lower() for w in lm_modal2]\n",
    "lm_modal3 = [w.lower() for w in lm_modal3]\n",
    "\n",
    "############## Read and create stop words list\n",
    "lm_stop = list()\n",
    "with open('..\\\\LM\\\\StopWords_Generic.txt', \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n', '')\n",
    "        lm_stop.append(line)\n",
    "        \n",
    "lm_stop = [w.lower() for w in lm_stop]\n",
    "\n",
    "############# Create a negation word list\n",
    "gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81630"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################\n",
    "#################### FOR ALL PROCESSED FILES LOOP ###################\n",
    "#####################################################################\n",
    "\n",
    "############# Create input txt file index\n",
    "processed = list()\n",
    "for subdir, dirs, files in os.walk(\"H:\\\\data\\\\edgar\\\\processed\\\\\" + obj_type + '\\\\' + str(period_start) + '-' + str(period_end)):\n",
    "    for file in files:\n",
    "        processed.append(os.path.join(subdir, file))\n",
    "\n",
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define a function count_occurrence to count the number of words in tup that pertaining to a list \n",
    "def count_occurrence(tup, lst): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst: \n",
    "            count+= 1\n",
    "      \n",
    "    return count\n",
    "\n",
    "### Define a function count_negation to count cases where negation occurs within four or fewer words from a word identified in list.\n",
    "def count_negation(tup, lst, negation): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst:\n",
    "            if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "                neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "                for neighborw in neighbor:\n",
    "                    if neighborw in negation:\n",
    "                        count+= 1\n",
    "\n",
    "            if tup.index(item)-4 < 0:\n",
    "                pre = tup[0:tup.index(item)+4]\n",
    "                for prew in pre:\n",
    "                    if prew in negation:\n",
    "                        count+= 1\n",
    "                        \n",
    "            if tup.index(item)+4 > len(tup):\n",
    "                post = tup[tup.index(item)-4:len(tup)]\n",
    "                for postw in post:\n",
    "                    if postw in negation:\n",
    "                        count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [05:49<00:00,  3.01it/s]\n"
     ]
    }
   ],
   "source": [
    "############ Full Text Raw Count\n",
    "accnum = list()\n",
    "\n",
    "nw = list()\n",
    "nvocab = list()\n",
    "\n",
    "n_neg = list()\n",
    "n_pos = list()\n",
    "n_uctt = list()\n",
    "n_lit = list()\n",
    "n_cstr = list()\n",
    "n_modal1 = list()\n",
    "n_modal2 = list()\n",
    "n_modal3 = list()\n",
    "n_negation = list()\n",
    "\n",
    "############ Word Tokenization, count nword and nvocab, count negative, positive, uncertainty, litigious, constraining and modal words\n",
    "for text in tqdm(processed):\n",
    "    ############# Create an array of accession number\n",
    "    a = text.split(\"\\\\\")[6].split(\".\")[0]\n",
    "    accnum.append(a)\n",
    "    \n",
    "    ############# Read processed txt file\n",
    "    with open(text, 'r',  encoding = \"utf-8\") as file:\n",
    "        contents = file.read().replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "        # print(repr(contents))\n",
    "        \n",
    "        ############ Word Tokenization\n",
    "        ## Raw tokens: including punctuations, numbers etc.\n",
    "        tokens = word_tokenize(contents)\n",
    "\n",
    "        ## Convert all words into small cases\n",
    "        ## Keep tokens that purely consist of alphabetic characters only\n",
    "        ## Delete single-character words except for 'I'\n",
    "        words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "        \n",
    "        ########### Delete words with lenth smaller than 1% and largr than 99% of the document\n",
    "        # wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "        # wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "        # words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "        vocab = sorted(set(words))\n",
    "        \n",
    "        ########### Save text statistics\n",
    "        ##### 1. nw: 1) nw 2) nw_mda 3) nw_notes\n",
    "        ##### 2. nvocab: 1) nvvocab 2) nvocab_mda 3) nvocab_notes\n",
    "        ##### 3. tone: 1) tone 2) tone_mda 3) tone_notes\n",
    "        \n",
    "        ## 1.1) nw\n",
    "        a = len(words)\n",
    "        nw.append(a)\n",
    "        \n",
    "        ## 2.1) nvocab\n",
    "        b = len(vocab)\n",
    "        nvocab.append(b)\n",
    "        \n",
    "        ## 3.1) tone\n",
    "        neg = count_occurrence(words, lm_neg)\n",
    "        n_neg.append(neg)\n",
    "        pos = count_occurrence(words, lm_pos)\n",
    "        n_pos.append(pos)\n",
    "        uctt = count_occurrence(words, lm_uctt)\n",
    "        n_uctt.append(uctt)\n",
    "        lit = count_occurrence(words, lm_lit)\n",
    "        n_lit.append(lit)\n",
    "        cstr = count_occurrence(words, lm_cstr)\n",
    "        n_cstr.append(cstr)\n",
    "        modal1 = count_occurrence(words, lm_modal1)\n",
    "        n_modal1.append(modal1)\n",
    "        modal2 = count_occurrence(words, lm_modal2)\n",
    "        n_modal2.append(modal2)\n",
    "        modal3 = count_occurrence(words, lm_modal3)\n",
    "        n_modal3.append(modal3)\n",
    "        negation = count_negation(words, lm_pos, gt_negation)\n",
    "        n_negation.append(negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnum</th>\n",
       "      <th>nw_mda</th>\n",
       "      <th>nvocab_mda</th>\n",
       "      <th>n_neg_mda</th>\n",
       "      <th>n_pos_mda</th>\n",
       "      <th>n_uctt_mda</th>\n",
       "      <th>n_lit_mda</th>\n",
       "      <th>n_cstr_mda</th>\n",
       "      <th>n_modal_strong_mda</th>\n",
       "      <th>n_modal_moderate_mda</th>\n",
       "      <th>n_modal_weak_mda</th>\n",
       "      <th>n_negation_mda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0000002969-20-000010</td>\n",
       "      <td>4088</td>\n",
       "      <td>738</td>\n",
       "      <td>43</td>\n",
       "      <td>52</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0000003545-20-000039</td>\n",
       "      <td>3662</td>\n",
       "      <td>923</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>65</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000004127-20-000007</td>\n",
       "      <td>1544</td>\n",
       "      <td>497</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0000004457-20-000027</td>\n",
       "      <td>8667</td>\n",
       "      <td>1158</td>\n",
       "      <td>89</td>\n",
       "      <td>48</td>\n",
       "      <td>93</td>\n",
       "      <td>50</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000006281-20-000013</td>\n",
       "      <td>3067</td>\n",
       "      <td>757</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>49</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1046</td>\n",
       "      <td>0001753926-20-000021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>0001757898-20-000003</td>\n",
       "      <td>6543</td>\n",
       "      <td>1064</td>\n",
       "      <td>127</td>\n",
       "      <td>53</td>\n",
       "      <td>92</td>\n",
       "      <td>64</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048</td>\n",
       "      <td>0001766016-20-000002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>0001772016-20-000018</td>\n",
       "      <td>2997</td>\n",
       "      <td>756</td>\n",
       "      <td>43</td>\n",
       "      <td>20</td>\n",
       "      <td>38</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0001773383-20-000004</td>\n",
       "      <td>6134</td>\n",
       "      <td>996</td>\n",
       "      <td>40</td>\n",
       "      <td>46</td>\n",
       "      <td>77</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1051 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    accnum  nw_mda  nvocab_mda  n_neg_mda  n_pos_mda  \\\n",
       "0     0000002969-20-000010    4088         738         43         52   \n",
       "1     0000003545-20-000039    3662         923         25         19   \n",
       "2     0000004127-20-000007    1544         497         13          8   \n",
       "3     0000004457-20-000027    8667        1158         89         48   \n",
       "4     0000006281-20-000013    3067         757         10         18   \n",
       "...                    ...     ...         ...        ...        ...   \n",
       "1046  0001753926-20-000021       0           0          0          0   \n",
       "1047  0001757898-20-000003    6543        1064        127         53   \n",
       "1048  0001766016-20-000002       0           0          0          0   \n",
       "1049  0001772016-20-000018    2997         756         43         20   \n",
       "1050  0001773383-20-000004    6134         996         40         46   \n",
       "\n",
       "      n_uctt_mda  n_lit_mda  n_cstr_mda  n_modal_strong_mda  \\\n",
       "0             30         33          14                   1   \n",
       "1             65         20          19                  12   \n",
       "2             22          9           9                   3   \n",
       "3             93         50          36                  16   \n",
       "4             49         16          21                  14   \n",
       "...          ...        ...         ...                 ...   \n",
       "1046           0          0           0                   0   \n",
       "1047          92         64          24                  13   \n",
       "1048           0          0           0                   0   \n",
       "1049          38         23          19                   4   \n",
       "1050          77         22          26                  12   \n",
       "\n",
       "      n_modal_moderate_mda  n_modal_weak_mda  n_negation_mda  \n",
       "0                        9                 9               0  \n",
       "1                       13                12               0  \n",
       "2                        5                 7               1  \n",
       "3                       19                22               0  \n",
       "4                        4                10               1  \n",
       "...                    ...               ...             ...  \n",
       "1046                     0                 0               0  \n",
       "1047                    13                33               1  \n",
       "1048                     0                 0               0  \n",
       "1049                     8                10               0  \n",
       "1050                    19                18               2  \n",
       "\n",
       "[1051 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Create Data Frame: full document\n",
    "d = {'accnum': accnum, 'nw': nw, 'nvocab': nvocab, 'n_neg': n_neg, 'n_pos': n_pos, 'n_uctt': n_uctt, 'n_lit': n_lit, 'n_cstr': n_cstr, \\\n",
    "     'n_modal_week': n_modal1, 'n_modal_moderate': n_modal2, 'n_modal_strong': n_modal3, 'n_negation': n_negation}\n",
    "\n",
    "text_data = pd.DataFrame(data=d)\n",
    "text_data.to_csv('..\\\\filings\\\\text_data_' + obj_type + '_' + str(period_start) + '-' + str(period_end) + '.csv', index=False)\n",
    "\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 81630/81630 [3:31:07<00:00,  6.44it/s]\n"
     ]
    }
   ],
   "source": [
    "############ MDA and NOTE Raw Count\n",
    "accnum = list()\n",
    "\n",
    "nw_mda = list()\n",
    "nvocab_mda = list()\n",
    "\n",
    "n_neg_mda = list()\n",
    "n_pos_mda = list()\n",
    "n_uctt_mda = list()\n",
    "n_lit_mda = list()\n",
    "n_cstr_mda = list()\n",
    "n_modal1_mda = list()\n",
    "n_modal2_mda = list()\n",
    "n_modal3_mda = list()\n",
    "n_negation_mda = list()\n",
    "\n",
    "nw_note = list()\n",
    "nvocab_note = list()\n",
    "\n",
    "n_neg_note = list()\n",
    "n_pos_note = list()\n",
    "n_uctt_note = list()\n",
    "n_lit_note = list()\n",
    "n_cstr_note = list()\n",
    "n_modal1_note = list()\n",
    "n_modal2_note = list()\n",
    "n_modal3_note = list()\n",
    "n_negation_note = list()\n",
    "\n",
    "############ Word Tokenization, count nword and nvocab, count negative, positive, uncertainty, litigious, constraining and modal words\n",
    "for text in tqdm(processed):\n",
    "    ############# Create an array of accession number\n",
    "    a = text.split(\"\\\\\")[6].split(\".\")[0]\n",
    "    accnum.append(a)\n",
    "    \n",
    "    ############# Read processed txt file\n",
    "    with open(text, 'r',  encoding = \"utf-8\") as file:\n",
    "        contents = file.read().replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "        ############################## TO EXTRACT MDA AND NOTES SECTION, UNCOMMENT THIS SECTION ################################\n",
    "        try:\n",
    "            mda = contents[contents.index(\"ITEM 2.\"):contents.index(\"ITEM 3.\")]\n",
    "        except:\n",
    "            try:\n",
    "                mda = contents[contents.index(\"Item 2.\"):contents.index(\"Item 3.\")]\n",
    "            except:\n",
    "                try:\n",
    "                    mda = contents[contents.index(\"ITEM 2\"):contents.index(\"ITEM 3\")]\n",
    "                except:\n",
    "                    try:\n",
    "                        mda = contents[contents.index(\"Item 2\"):contents.index(\"Item 3\")]\n",
    "                    except:\n",
    "                        mda = ''\n",
    "                        pass\n",
    "                    \n",
    "        try:\n",
    "            note = contents[contents.index(\"NOTES TO\"):contents.index(\"ITEM 2.\")]\n",
    "        except:\n",
    "            try:\n",
    "                note = contents[contents.index(\"NOTES TO\"):contents.index(\"ITEM 2\")]\n",
    "            except:\n",
    "                try:\n",
    "                    note = contents[contents.index(\"Notes to\"):contents.index(\"Item 2.\")]\n",
    "                except:\n",
    "                    try:\n",
    "                        note = contents[contents.index(\"Notes to\"):contents.index(\"Item 2\")]\n",
    "                    except:\n",
    "                        note = ''\n",
    "                        pass\n",
    "        ###########################################################################################################\n",
    "        # print(repr(contents))\n",
    "        \n",
    "        ############ Word Tokenization\n",
    "        ## Raw tokens: including punctuations, numbers etc.\n",
    "        tokens_mda = word_tokenize(mda)\n",
    "        tokens_note = word_tokenize(note)\n",
    "        \n",
    "        ####################################################################\n",
    "\n",
    "        ## Convert all words into small cases\n",
    "        ## Keep tokens that purely consist of alphabetic characters only\n",
    "        ## Delete single-character words except for 'I'\n",
    "        words_mda = [w.lower() for w in tokens_mda if w.isalpha() and len(w)>1 or w =='i']\n",
    "        words_note = [w.lower() for w in tokens_note if w.isalpha() and len(w)>1 or w =='i']\n",
    "        \n",
    "        ########### Delete words with lenth smaller than 1% and largr than 99% of the document\n",
    "        # wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "        # wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "        # words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "        vocab_mda = sorted(set(words_mda))\n",
    "        vocab_note = sorted(set(words_note))\n",
    "        \n",
    "        ########### Save text statistics\n",
    "        ##### 1. nw: 1) nw 2) nw_mda 3) nw_notes\n",
    "        ##### 2. nvocab: 1) nvvocab 2) nvocab_mda 3) nvocab_notes\n",
    "        ##### 3. tone: 1) tone 2) tone_mda 3) tone_notes\n",
    "        \n",
    "        ## 1.1) nw\n",
    "        a_mda = len(words_mda)\n",
    "        nw_mda.append(a_mda)\n",
    "        a_note = len(words_note)\n",
    "        nw_note.append(a_note)\n",
    "        \n",
    "        ## 2.1) nvocab\n",
    "#         b_mda = len(vocab_mda)\n",
    "#         nvocab_mda.append(b_mda)\n",
    "#         b_note = len(vocab_note)\n",
    "#         nvocab_note.append(b_note)\n",
    "        \n",
    "        ## 3.1) tone\n",
    "        neg_mda = count_occurrence(words_mda, lm_neg)\n",
    "        n_neg_mda.append(neg_mda)\n",
    "        pos_mda = count_occurrence(words_mda, lm_pos)\n",
    "        n_pos_mda.append(pos_mda)\n",
    "#         uctt_mda = count_occurrence(words_mda, lm_uctt)\n",
    "#         n_uctt_mda.append(uctt_mda)\n",
    "#         lit_mda = count_occurrence(words_mda, lm_lit)\n",
    "#         n_lit_mda.append(lit_mda)\n",
    "#         cstr_mda = count_occurrence(words_mda, lm_cstr)\n",
    "#         n_cstr_mda.append(cstr_mda)\n",
    "#         modal1_mda = count_occurrence(words_mda, lm_modal1)\n",
    "#         n_modal1_mda.append(modal1_mda)\n",
    "#         modal2_mda = count_occurrence(words_mda, lm_modal2)\n",
    "#         n_modal2_mda.append(modal2_mda)\n",
    "#         modal3_mda = count_occurrence(words_mda, lm_modal3)\n",
    "#         n_modal3_mda.append(modal3_mda)\n",
    "        negation_mda = count_negation(words_mda, lm_pos, gt_negation)\n",
    "        n_negation_mda.append(negation_mda)\n",
    "        \n",
    "        neg_note = count_occurrence(words_note, lm_neg)\n",
    "        n_neg_note.append(neg_note)\n",
    "        pos_note = count_occurrence(words_note, lm_pos)\n",
    "        n_pos_note.append(pos_note)\n",
    "#         uctt_note = count_occurrence(words_note, lm_uctt)\n",
    "#         n_uctt_note.append(uctt_note)\n",
    "#         lit_note = count_occurrence(words_note, lm_lit)\n",
    "#         n_lit_note.append(lit_note)\n",
    "#         cstr_note = count_occurrence(words_note, lm_cstr)\n",
    "#         n_cstr_note.append(cstr_note)\n",
    "#         modal1_note = count_occurrence(words_note, lm_modal1)\n",
    "#         n_modal1_note.append(modal1_note)\n",
    "#         modal2_note = count_occurrence(words_note, lm_modal2)\n",
    "#         n_modal2_note.append(modal2_note)\n",
    "#         modal3_note = count_occurrence(words_note, lm_modal3)\n",
    "#         n_modal3_note.append(modal3_note)\n",
    "        negation_note = count_negation(words_note, lm_pos, gt_negation)\n",
    "        n_negation_note.append(negation_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of filings whose MDA and NOTES are successfully extracted: 0.2590469190248683\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnum</th>\n",
       "      <th>nw_mda</th>\n",
       "      <th>n_neg_mda</th>\n",
       "      <th>n_pos_mda</th>\n",
       "      <th>n_negation_mda</th>\n",
       "      <th>nw_note</th>\n",
       "      <th>n_neg_note</th>\n",
       "      <th>n_pos_note</th>\n",
       "      <th>n_negation_note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0000002601-98-000012</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0000003116-97-000001</td>\n",
       "      <td>2086</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0000003116-97-000002</td>\n",
       "      <td>2932</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0000003116-97-000003</td>\n",
       "      <td>2949</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0000003116-98-000003</td>\n",
       "      <td>1704</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81604</td>\n",
       "      <td>0001068800-98-000023</td>\n",
       "      <td>306</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81607</td>\n",
       "      <td>0001068800-98-000028</td>\n",
       "      <td>1638</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1345</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81608</td>\n",
       "      <td>0001068800-98-000029</td>\n",
       "      <td>3157</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1470</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81610</td>\n",
       "      <td>0001068800-98-000032</td>\n",
       "      <td>3176</td>\n",
       "      <td>38</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81615</td>\n",
       "      <td>0001069727-98-000005</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21146 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     accnum  nw_mda  n_neg_mda  n_pos_mda  n_negation_mda  \\\n",
       "77     0000002601-98-000012     140          0          2               0   \n",
       "99     0000003116-97-000001    2086         21          8               0   \n",
       "100    0000003116-97-000002    2932         25         14               0   \n",
       "101    0000003116-97-000003    2949         23         12               0   \n",
       "103    0000003116-98-000003    1704          4         14               0   \n",
       "...                     ...     ...        ...        ...             ...   \n",
       "81604  0001068800-98-000023     306          4          1               0   \n",
       "81607  0001068800-98-000028    1638         26          6               1   \n",
       "81608  0001068800-98-000029    3157         24         29               1   \n",
       "81610  0001068800-98-000032    3176         38         24               0   \n",
       "81615  0001069727-98-000005      23          0          0               0   \n",
       "\n",
       "       nw_note  n_neg_note  n_pos_note  n_negation_note  \n",
       "77          25           0           0                0  \n",
       "99           6           0           0                0  \n",
       "100          6           0           0                0  \n",
       "101          6           0           0                0  \n",
       "103          6           0           0                0  \n",
       "...        ...         ...         ...              ...  \n",
       "81604        4           0           0                0  \n",
       "81607     1345           6           2                0  \n",
       "81608     1470           8           6                0  \n",
       "81610      268           1           1                0  \n",
       "81615        5           0           0                0  \n",
       "\n",
       "[21146 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Create Data Frame: MDA and NOTES\n",
    "d = {'accnum': accnum, 'nw_mda': nw_mda, 'n_neg_mda': n_neg_mda, 'n_pos_mda': n_pos_mda, 'n_negation_mda': n_negation_mda, 'nw_note': nw_note, 'n_neg_note': n_neg_note, 'n_pos_note': n_pos_note, 'n_negation_note': n_negation_note}\n",
    "#      'nvocab_mda': nvocab_mda, 'n_uctt_mda': n_uctt_mda, 'n_lit_mda': n_lit_mda, 'n_cstr_mda': n_cstr_mda, \\\n",
    "#      'n_modal_strong_mda': n_modal1_mda, 'n_modal_moderate_mda': n_modal2_mda, 'n_modal_weak_mda': n_modal3_mda, \\\n",
    "#      'nvocab_note': nvocab_note, 'n_uctt_note': n_uctt_note, 'n_lit_note': n_lit_note, 'n_cstr_note': n_cstr_note, \\\n",
    "#      'n_modal_strong_note': n_modal1_note, 'n_modal_moderate_note': n_modal2_note, 'n_modal_weak_note': n_modal3_note}\n",
    "\n",
    "text_data = pd.DataFrame(data=d)\n",
    "print('percentage of filings whose MDA and NOTES are successfully extracted: ' + str(text_data[(text_data['nw_mda']!=0) & (text_data['nw_note']!=0)].shape[0]/text_data.shape[0]))\n",
    "text_data = text_data[(text_data['nw_mda']!=0) & (text_data['nw_note']!=0)]\n",
    "\n",
    "text_data.to_csv('..\\\\filings\\\\text_data_section_' + str(period_start) + '-' + str(period_end) + '.csv', index=False)\n",
    "text_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################################\n",
    "# ################### FOR SINGLE FILE INSPECTION ######################\n",
    "# #####################################################################\n",
    "\n",
    "# ############ Word Tokenization\n",
    "# ## Raw tokens: including punctuations, numbers etc.\n",
    "# with open(processed[6], 'r',  encoding = \"utf-8\") as file:\n",
    "#     contents = file.read().replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "# tokens = word_tokenize(contents)\n",
    "\n",
    "# #tokens\n",
    "\n",
    "# ## Convert all words into small cases\n",
    "# ## And keep tokens that purely consist of alphabetic characters only\n",
    "# words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# # words[2500:2600]\n",
    "# # vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_occurrence(tup, lst): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst: \n",
    "#             count+= 1\n",
    "      \n",
    "#     return count\n",
    "\n",
    "# count_occurrence(words, lm_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing\n",
    "\n",
    "# def count_negation(tup, lst, negation): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst:\n",
    "#             if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "#                 neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "#                 for neighborw in neighbor:\n",
    "#                     if neighborw in negation:\n",
    "#                         count+= 1\n",
    "\n",
    "#             if tup.index(item)-4 < 0:\n",
    "#                 pre = tup[0:tup.index(item)+4]\n",
    "#                 for prew in pre:\n",
    "#                     if prew in negation:\n",
    "#                         count+= 1\n",
    "                        \n",
    "#             if tup.index(item)+4 > len(tup):\n",
    "#                 post = tup[tup.index(item)-4:len(tup)]\n",
    "#                 for postw in post:\n",
    "#                     if postw in negation:\n",
    "#                         count+= 1\n",
    "#     return count\n",
    "\n",
    "# count_negation(words, lm_pos, gt_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########### Winsorize words with lenth smaller than 1% and largr than 99% of the document\n",
    "# wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "# wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "# words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### See the most common 20 words\n",
    "# fdist = nltk.FreqDist(words)\n",
    "# fdist.most_common(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
