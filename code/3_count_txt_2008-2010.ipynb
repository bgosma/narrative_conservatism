{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\github\\\\narrative_conservatism\\\\code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### import packages\n",
    "import os, nltk, numpy as np, pandas as pd, time\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from time import process_time\n",
    "\n",
    "##########################################################\n",
    "##################### parameter ##########################\n",
    "##########################################################\n",
    "obj_type = '10-Q'\n",
    "period_start = 2008 # included\n",
    "period_end = 2010 # included\n",
    "\n",
    "############### Set working directory to parent directory\n",
    "os.getcwd()\n",
    "# os.chdir('F:\\\\github\\\\narrative_conservatism\\\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Read LM disctionary\n",
    "LM = pd.read_excel('..\\\\LM\\\\LoughranMcDonald_MasterDictionary_2018.xlsx', encoding = \"utf-8\")\n",
    "\n",
    "############### Create negative, positive, uncertainty, litigious, constraining and modal word lists\n",
    "lm_neg = LM.loc[LM['Negative'] != 0]['Word'].values.tolist()\n",
    "lm_pos = LM.loc[LM['Positive'] != 0]['Word'].values.tolist()\n",
    "lm_uctt = LM.loc[LM['Uncertainty'] != 0]['Word'].values.tolist()\n",
    "lm_lit = LM.loc[LM['Litigious'] != 0]['Word'].values.tolist()\n",
    "lm_cstr = LM.loc[LM['Constraining'] != 0]['Word'].values.tolist()\n",
    "\n",
    "lm_modal1 = LM.loc[LM['Modal'] == 1]['Word'].values.tolist()\n",
    "lm_modal2 = LM.loc[LM['Modal'] == 2]['Word'].values.tolist()\n",
    "lm_modal3 = LM.loc[LM['Modal'] == 3]['Word'].values.tolist()\n",
    "\n",
    "lm_neg = [w.lower() for w in lm_neg]\n",
    "lm_pos = [w.lower() for w in lm_pos]\n",
    "lm_uctt = [w.lower() for w in lm_uctt]\n",
    "lm_lit = [w.lower() for w in lm_lit]\n",
    "lm_cstr = [w.lower() for w in lm_cstr]\n",
    "lm_modal1 = [w.lower() for w in lm_modal1]\n",
    "lm_modal2 = [w.lower() for w in lm_modal2]\n",
    "lm_modal3 = [w.lower() for w in lm_modal3]\n",
    "\n",
    "############## Read and create stop words list\n",
    "lm_stop = list()\n",
    "with open('..\\\\LM\\\\StopWords_Generic.txt', \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n', '')\n",
    "        lm_stop.append(line)\n",
    "        \n",
    "lm_stop = [w.lower() for w in lm_stop]\n",
    "\n",
    "############# Create a negation word list\n",
    "gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79355"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################\n",
    "#################### FOR ALL PROCESSED FILES LOOP ###################\n",
    "#####################################################################\n",
    "\n",
    "############# Create processed txt file index\n",
    "processed = list()\n",
    "for subdir, dirs, files in os.walk(\"H:\\\\data\\\\edgar\\\\processed\\\\\" + obj_type + '\\\\' + str(period_start) + '-' + str(period_end)):\n",
    "    for file in files:\n",
    "        processed.append(os.path.join(subdir, file))\n",
    "\n",
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 79355/79355 [44:03:20<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time during the whole program in seconds: 158600.6146080494\n"
     ]
    }
   ],
   "source": [
    "############ Word Tokenization, count nword and nvocab, count negative, positive, uncertainty, litigious, constraining and modal words\n",
    "accnum = list()\n",
    "\n",
    "nw = list()\n",
    "nvocab = list()\n",
    "\n",
    "### Define a function count_occurrence to count the number of words in tup that pertaining to a lst \n",
    "def count_occurrence(tup, lst): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst: \n",
    "            count+= 1\n",
    "      \n",
    "    return count\n",
    "\n",
    "### Define a function count_negation to count cases where negation occurs within four or fewer words from a word identified in list.\n",
    "def count_negation(tup, lst, negation): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst:\n",
    "            if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "                neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "                for neighborw in neighbor:\n",
    "                    if neighborw in negation:\n",
    "                        count+= 1\n",
    "\n",
    "            if tup.index(item)-4 < 0:\n",
    "                pre = tup[0:tup.index(item)+4]\n",
    "                for prew in pre:\n",
    "                    if prew in negation:\n",
    "                        count+= 1\n",
    "                        \n",
    "            if tup.index(item)+4 > len(tup):\n",
    "                post = tup[tup.index(item)-4:len(tup)]\n",
    "                for postw in post:\n",
    "                    if postw in negation:\n",
    "                        count+= 1\n",
    "    return count\n",
    "\n",
    "n_neg = list()\n",
    "n_pos = list()\n",
    "n_uctt = list()\n",
    "n_lit = list()\n",
    "n_cstr = list()\n",
    "n_modal1 = list()\n",
    "n_modal2 = list()\n",
    "n_modal3 = list()\n",
    "n_negation = list()\n",
    "net_pos = list()\n",
    "\n",
    "# t1_start = process_time()\n",
    "t1_start = time.time()\n",
    "\n",
    "for text in tqdm(processed):\n",
    "    ############# Create an array of accession number\n",
    "    a = text.split(\"\\\\\")[6].split(\".\")[0]\n",
    "    accnum.append(a)\n",
    "    \n",
    "    ############# Read processed txt file\n",
    "    with open(text, 'r',  encoding = \"utf-8\") as file:\n",
    "        contents = file.read().replace('\\n', '').replace('\\xa0', '')\n",
    "        # print(repr(contents))\n",
    "        \n",
    "        ############ Word Tokenization\n",
    "        ## Raw tokens: including punctuations, numbers etc.\n",
    "        tokens = word_tokenize(contents)\n",
    "\n",
    "        ## Convert all words into small cases\n",
    "        ## Keep tokens that purely consist of alphabetic characters only\n",
    "        ## Delete single-character words except for 'I'\n",
    "        words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "        \n",
    "        ########### Delete words with lenth smaller than 1% and largr than 99% of the document\n",
    "        # wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "        # wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "        # words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "        vocab = sorted(set(words))\n",
    "        \n",
    "        ########### Save text statistics\n",
    "        ##### 1. nw: 1) nw 2) nw_mda 3) nw_notes\n",
    "        ##### 2. nvocab: 1) nvvocab 2) nvocab_mda 3) nvocab_notes\n",
    "        ##### 3. tone: 1) tone 2) tone_mda 3) tone_notes\n",
    "        \n",
    "        ## 1.1) nw\n",
    "        a = len(words)\n",
    "        nw.append(a)\n",
    "        \n",
    "        ## 2.1) nvocab\n",
    "        b = len(vocab)\n",
    "        nvocab.append(b)\n",
    "        \n",
    "        ## 3.1) tone\n",
    "        neg = count_occurrence(words, lm_neg)\n",
    "        n_neg.append(neg)\n",
    "        pos = count_occurrence(words, lm_pos)\n",
    "        n_pos.append(pos)\n",
    "        uctt = count_occurrence(words, lm_uctt)\n",
    "        n_uctt.append(uctt)\n",
    "        lit = count_occurrence(words, lm_lit)\n",
    "        n_lit.append(lit)\n",
    "        cstr = count_occurrence(words, lm_cstr)\n",
    "        n_cstr.append(cstr)\n",
    "        modal1 = count_occurrence(words, lm_modal1)\n",
    "        n_modal1.append(modal1)\n",
    "        modal2 = count_occurrence(words, lm_modal2)\n",
    "        n_modal2.append(modal2)\n",
    "        modal3 = count_occurrence(words, lm_modal3)\n",
    "        n_modal3.append(modal3)\n",
    "        negation = count_negation(words, lm_pos, gt_negation)\n",
    "        n_negation.append(negation)\n",
    "        netpos = pos - negation\n",
    "        net_pos.append(netpos)\n",
    "\n",
    "# t1_end = process_time()\n",
    "t1_end = time.time()\n",
    "print(\"Elapsed time during the whole program in seconds:\", t1_end - t1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnum</th>\n",
       "      <th>nw</th>\n",
       "      <th>nvocab</th>\n",
       "      <th>n_neg</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>n_uctt</th>\n",
       "      <th>n_lit</th>\n",
       "      <th>n_cstr</th>\n",
       "      <th>n_modal_week</th>\n",
       "      <th>n_modal_moderate</th>\n",
       "      <th>n_modal_strong</th>\n",
       "      <th>n_negation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0000002178-08-000019</td>\n",
       "      <td>6169</td>\n",
       "      <td>1606</td>\n",
       "      <td>99</td>\n",
       "      <td>31</td>\n",
       "      <td>86</td>\n",
       "      <td>68</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0000002178-08-000026</td>\n",
       "      <td>6481</td>\n",
       "      <td>1679</td>\n",
       "      <td>112</td>\n",
       "      <td>28</td>\n",
       "      <td>99</td>\n",
       "      <td>69</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000002178-08-000036</td>\n",
       "      <td>7369</td>\n",
       "      <td>1812</td>\n",
       "      <td>133</td>\n",
       "      <td>31</td>\n",
       "      <td>120</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0000002178-09-000016</td>\n",
       "      <td>5069</td>\n",
       "      <td>1439</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000002178-09-000022</td>\n",
       "      <td>5775</td>\n",
       "      <td>1236</td>\n",
       "      <td>80</td>\n",
       "      <td>22</td>\n",
       "      <td>68</td>\n",
       "      <td>51</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79350</td>\n",
       "      <td>0001504167-10-000033</td>\n",
       "      <td>3240</td>\n",
       "      <td>886</td>\n",
       "      <td>57</td>\n",
       "      <td>17</td>\n",
       "      <td>45</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79351</td>\n",
       "      <td>0001504167-10-000040</td>\n",
       "      <td>3293</td>\n",
       "      <td>839</td>\n",
       "      <td>54</td>\n",
       "      <td>29</td>\n",
       "      <td>49</td>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79352</td>\n",
       "      <td>0001504167-10-000042</td>\n",
       "      <td>3823</td>\n",
       "      <td>902</td>\n",
       "      <td>69</td>\n",
       "      <td>19</td>\n",
       "      <td>53</td>\n",
       "      <td>20</td>\n",
       "      <td>35</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79353</td>\n",
       "      <td>0001504167-10-000044</td>\n",
       "      <td>2958</td>\n",
       "      <td>769</td>\n",
       "      <td>56</td>\n",
       "      <td>21</td>\n",
       "      <td>37</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79354</td>\n",
       "      <td>9999999997-09-027041</td>\n",
       "      <td>142</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79355 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     accnum    nw  nvocab  n_neg  n_pos  n_uctt  n_lit  \\\n",
       "0      0000002178-08-000019  6169    1606     99     31      86     68   \n",
       "1      0000002178-08-000026  6481    1679    112     28      99     69   \n",
       "2      0000002178-08-000036  7369    1812    133     31     120     75   \n",
       "3      0000002178-09-000016  5069    1439     60     20      53     40   \n",
       "4      0000002178-09-000022  5775    1236     80     22      68     51   \n",
       "...                     ...   ...     ...    ...    ...     ...    ...   \n",
       "79350  0001504167-10-000033  3240     886     57     17      45     12   \n",
       "79351  0001504167-10-000040  3293     839     54     29      49     14   \n",
       "79352  0001504167-10-000042  3823     902     69     19      53     20   \n",
       "79353  0001504167-10-000044  2958     769     56     21      37     14   \n",
       "79354  9999999997-09-027041   142      73      0      0       0      0   \n",
       "\n",
       "       n_cstr  n_modal_week  n_modal_moderate  n_modal_strong  n_negation  \n",
       "0          39            14                18              21           5  \n",
       "1          38            13                18              22           4  \n",
       "2          40            15                20              33           3  \n",
       "3          35             4                12              15           5  \n",
       "4          39            15                13              18           0  \n",
       "...       ...           ...               ...             ...         ...  \n",
       "79350      27            19                26              13           0  \n",
       "79351      29            18                26              13           1  \n",
       "79352      35            21                27              22           3  \n",
       "79353      24            21                25               9           2  \n",
       "79354       0             0                 0               0           0  \n",
       "\n",
       "[79355 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Create Data Frame\n",
    "d = {'accnum': accnum, 'nw': nw, 'nvocab': nvocab, 'n_neg': n_neg, 'n_pos': n_pos, 'n_uctt': n_uctt, 'n_lit': n_lit, 'n_cstr': n_cstr, \\\n",
    "     'n_modal_week': n_modal1, 'n_modal_moderate': n_modal2, 'n_modal_strong': n_modal3, 'n_negation': n_negation}\n",
    "\n",
    "text_data = pd.DataFrame(data=d)\n",
    "text_data.to_csv('..\\\\filings\\\\text_data_' + obj_type + '_' + str(period_start) + '-' + str(period_end) + '.csv', index=False)\n",
    "\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################################\n",
    "# ################### FOR SINGLE FILE INSPECTION ######################\n",
    "# #####################################################################\n",
    "\n",
    "# ############ Word Tokenization\n",
    "# ## Raw tokens: including punctuations, numbers etc.\n",
    "# with open(processed[1], 'r',  encoding = \"utf-8\") as file:\n",
    "#     contents = file.read().replace('\\n', '').replace('\\xa0', '')\n",
    "# tokens = word_tokenize(contents)\n",
    "\n",
    "# #tokens\n",
    "\n",
    "# ## Convert all words into small cases\n",
    "# ## And keep tokens that purely consist of alphabetic characters only\n",
    "# words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# # words[2500:2600]\n",
    "# # vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_occurrence(tup, lst): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst: \n",
    "#             count+= 1\n",
    "      \n",
    "#     return count\n",
    "\n",
    "# count_occurrence(words, lm_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing\n",
    "\n",
    "# def count_negation(tup, lst, negation): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst:\n",
    "#             if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "#                 neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "#                 for neighborw in neighbor:\n",
    "#                     if neighborw in negation:\n",
    "#                         count+= 1\n",
    "\n",
    "#             if tup.index(item)-4 < 0:\n",
    "#                 pre = tup[0:tup.index(item)+4]\n",
    "#                 for prew in pre:\n",
    "#                     if prew in negation:\n",
    "#                         count+= 1\n",
    "                        \n",
    "#             if tup.index(item)+4 > len(tup):\n",
    "#                 post = tup[tup.index(item)-4:len(tup)]\n",
    "#                 for postw in post:\n",
    "#                     if postw in negation:\n",
    "#                         count+= 1\n",
    "#     return count\n",
    "\n",
    "# count_negation(words, lm_pos, gt_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########### Winsorize words with lenth smaller than 1% and largr than 99% of the document\n",
    "# wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "# wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "# words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### See the most common 20 words\n",
    "# fdist = nltk.FreqDist(words)\n",
    "# fdist.most_common(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
