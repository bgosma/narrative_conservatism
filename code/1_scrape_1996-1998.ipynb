{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\github\\\\narrative_conservatism\\\\code'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### import packages\n",
    "import os, requests, sys, re, pandas as pd, time\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from time import process_time\n",
    "\n",
    "##########################################################\n",
    "##################### parameter ##########################\n",
    "##########################################################\n",
    "obj_type = '10-Q'\n",
    "period_start = 1996 # included\n",
    "period_end = 1998 # included\n",
    "\n",
    "############### Set working directory to parent directory\n",
    "os.getcwd()\n",
    "# os.chdir('F:\\\\github\\\\narrative_conservatism\\\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83984"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### Access all fillings through SEC master index #################################\n",
    "####### indexes downloaded using python-edgar: https://github.com/edouardswiac/python-edgar #######\n",
    "#### open terminal, and run the following lines:\n",
    "#### cd F:\\github\\python-edgar-master (switch dir to where the run.py script is located)\n",
    "#### python run.py -y 1993 -d edgar_idx (downloading all quarterly master index from 1993 into folder edgar_idx)\n",
    "\n",
    "#### cd F:\\github\\python-edgar-master\\edgar-idx (switch dir to where the downloaded indexes are located)\n",
    "#### cat *.tsv > master.tsv (stitch all quarterly indexes into one master index)\n",
    "#### du -h master.tsv (inspect how large the master index file is)\n",
    "\n",
    "index_edgar = list()\n",
    "doc_url = list()\n",
    "\n",
    "# create an index of downloaded local quarterly master indexes\n",
    "for subdir, dirs, files in os.walk(\"F:\\\\github\\\\python-edgar-master\\\\edgar-idx\"):\n",
    "    for file in files:\n",
    "        file_year = int(file.split('-')[0])\n",
    "        if file_year >= period_start and file_year <= period_end:\n",
    "            index_edgar.append(os.path.join(subdir, file))\n",
    "\n",
    "# read each index file, select rows with matched file type, and store matched doc_links\n",
    "for filenameTSV in index_edgar:\n",
    "    tsv_read = pd.read_csv(filenameTSV, sep='|', header=None, encoding = \"utf-8\")\n",
    "    tsv_read.columns = ['1', '2', '3', '4', '5', '6']\n",
    "    \n",
    "    # select the rows with filetype equal to predefined type\n",
    "    tsv_type = tsv_read.loc[tsv_read['3'] == obj_type]\n",
    "    doc_link = tsv_type['6'].values.tolist()\n",
    "    doc_link = ['https://www.sec.gov/Archives/' + w for w in doc_link]\n",
    "    for doc in doc_link:\n",
    "        doc_url.append(doc)\n",
    "        \n",
    "len(doc_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### Access all fillings through SEC search engine ####################################\n",
    "# ################## NOT RECOMMENDED AT ALL #############################################################\n",
    "# cik = '0000051143'\n",
    "# obj_type = '8-K'\n",
    "# number of documents listed per page\n",
    "# count = '100'\n",
    "# # index of first document listed in the current page\n",
    "# start = '0'\n",
    "# # find filings prior to the date 2016y01m01d\n",
    "# dateb = ''\n",
    "\n",
    "# # Obtain url for intial search result page\n",
    "# base_url = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type={}&dateb={}&start={}&count={}\"\n",
    "# init_url = base_url.format(cik, obj_type, dateb, start, count)\n",
    "\n",
    "# # define a function that takes the input url and returns next search page url\n",
    "# def get_next_url(input_url):\n",
    "#     edgar_resp = requests.get(input_url)\n",
    "#     edgar_str = edgar_resp.text\n",
    "#     soup = BeautifulSoup(edgar_str, 'html.parser')\n",
    "\n",
    "#     div_tag = soup.find('div', style='margin-top: 5px; margin-bottom: 5px;')\n",
    "#     button = div_tag.find('td', style='text-align: right;')\n",
    "#     fbutton = button.find_all('input')[0]['value']\n",
    "#     if re.findall(r'Next', fbutton) == ['Next']:\n",
    "#         next_url = button.find_all('input')[0]['onclick'][:-1]\n",
    "#     elif len(button.find_all('input')) == 2:\n",
    "#         next_url = button.find_all('input')[1]['onclick'][:-1]\n",
    "#     else:\n",
    "#         next_url = 'NA'\n",
    "        \n",
    "#     next_url = next_url.replace('parent.location=\\'', 'https://www.sec.gov')\n",
    "#     return next_url\n",
    "\n",
    "# # create a search result page url list\n",
    "# search_url = [init_url]\n",
    "\n",
    "# while get_next_url(init_url) != 'NA':\n",
    "#     search_url.append(get_next_url(init_url))\n",
    "#     init_url = get_next_url(init_url)\n",
    "    \n",
    "# ############### Create a document link list of a given CIK and file type\n",
    "# doc_link = list()\n",
    "\n",
    "# for url in search_url:\n",
    "#     edgar_resp = requests.get(url)\n",
    "#     edgar_str = edgar_resp.text\n",
    "#     soup = BeautifulSoup(edgar_str, 'html.parser')\n",
    "#     table_tag = soup.find('table', class_='tableFile2')\n",
    "#     rows = table_tag.find_all('tr')\n",
    "\n",
    "#     for row in rows[1:]:\n",
    "#         cells = row.find_all('td')\n",
    "#         doc_link.append('https://www.sec.gov' + cells[1].a['href'])\n",
    "        \n",
    "# len(doc_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 83984/83984 [6:43:20<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time during the whole program in seconds: 24200.02544403076\n"
     ]
    }
   ],
   "source": [
    "############### Extract file identification info from doc_url\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
    "\n",
    "accnum = list()\n",
    "fd = list()\n",
    "rp = list()\n",
    "name = list()\n",
    "cik = list()\n",
    "sic = list()\n",
    "file_type = list()\n",
    "fye = list()\n",
    "state = list()\n",
    "bazip = list()\n",
    "item8k = list()\n",
    "web_url = list()\n",
    "\n",
    "# t1_start = process_time()\n",
    "t1_start = time.time()\n",
    "\n",
    "for doc in tqdm(doc_url):\n",
    "    doc_resp = requests.get(doc, headers=headers)\n",
    "    doc_str = doc_resp.text\n",
    "    soup = BeautifulSoup(doc_str, 'html.parser')\n",
    "    \n",
    "    # Save the SEC accession number (accnum)\n",
    "    try:\n",
    "        div_tag = soup.find('div', id='formHeader')\n",
    "        secnum = div_tag.find('div', id='secNum')\n",
    "        a = secnum.get_text().split()[3]\n",
    "        accnum.append(a)\n",
    "    except:\n",
    "        accnum.append('NA')\n",
    "        pass\n",
    "\n",
    "    # Save the Filing Date and Reporting Period\n",
    "    try:\n",
    "        div_tag = soup.find('div', class_='formContent')\n",
    "        dates = div_tag.find_all('div', class_='info')\n",
    "        # Filing Date\n",
    "        a = dates[0].get_text()\n",
    "        fd.append(a)\n",
    "    except:\n",
    "        fd.append('NA')\n",
    "        pass\n",
    "    \n",
    "        # Reporting Period\n",
    "    try:\n",
    "        b = dates[3].get_text()\n",
    "        rp.append(b)\n",
    "    except:\n",
    "        rp.append('NA')\n",
    "        pass\n",
    "    \n",
    "    # For 8K files, Save item info\n",
    "    if obj_type == '8-K':\n",
    "        c = dates[4].get_text()\n",
    "        clist = re.findall(r'\\d.\\d\\d', c)\n",
    "        if clist != []:\n",
    "            c = ', '.join(clist)\n",
    "            item8k.append(c)\n",
    "        else:\n",
    "            clist = re.findall(r'\\d', c)\n",
    "            c = ', '.join(clist)\n",
    "            item8k.append(c)\n",
    "    else :\n",
    "        c = 'NA'\n",
    "        item8k.append(c)\n",
    "\n",
    "        # Save the Company name and CIK\n",
    "    try:\n",
    "        div_tag = soup.find('div', class_='companyInfo')\n",
    "        comname = div_tag.find('span', class_='companyName')\n",
    "        # Company Name\n",
    "        a = comname.get_text().split(\"\\n\")[0].replace(' (Filer)', '')\n",
    "        name.append(a)\n",
    "    except:\n",
    "        name.append('NA')\n",
    "        pass\n",
    "    \n",
    "        # CIK\n",
    "    try:\n",
    "        b = comname.get_text().split(\"\\n\")[1].replace('CIK: ', '').replace(' (see all company filings)', '')\n",
    "        cik.append(b)\n",
    "    except:\n",
    "        cik.append('NA')\n",
    "        pass\n",
    "\n",
    "        # Save Business Address ZIP \n",
    "    try:\n",
    "        div_tag = soup.find_all('div', class_='mailer')[1].find_all('span', class_='mailerAddress')[1]\n",
    "        ba = div_tag.get_text()\n",
    "        alist = re.findall(r'\\d\\d\\d\\d\\d', ba)\n",
    "        if alist == []:\n",
    "            div_tag = soup.find_all('div', class_='mailer')[1].find_all('span', class_='mailerAddress')[2]\n",
    "            ba = div_tag.get_text()\n",
    "            alist = re.findall(r'\\d\\d\\d\\d\\d', ba)\n",
    "        a = ', '.join(alist)\n",
    "        bazip.append(a)\n",
    "    except:\n",
    "        bazip.append('NA')\n",
    "        pass\n",
    "\n",
    "        # Save SIC, File Type, Fiscal Year End and State of Incorporation\n",
    "    try:\n",
    "        div_tag = soup.find('div', class_='companyInfo')\n",
    "        filinginfo = div_tag.find('p', class_='identInfo')\n",
    "        # SIC\n",
    "        a = filinginfo.get_text().split(\"|\")[5].split(\"SIC\")[1].split()[1]\n",
    "        sic.append(a)\n",
    "    except:\n",
    "        sic.append('NA')\n",
    "        pass\n",
    "    \n",
    "    # File Type\n",
    "    b = obj_type\n",
    "    file_type.append(b)\n",
    "        \n",
    "        # Fiscal Year End\n",
    "    try:\n",
    "        c = filinginfo.get_text().split(\"|\")[2].split(\"Type\")[0].split(\":\")[1]\n",
    "        fye.append(c)\n",
    "    except:\n",
    "        fye.append('NA')\n",
    "        pass\n",
    "    \n",
    "        # State\n",
    "    try:\n",
    "        d = filinginfo.get_text().split(\"|\")[1].split(\":\")[1]\n",
    "        state.append(d)\n",
    "    except:\n",
    "        state.append('NA')\n",
    "        pass\n",
    "\n",
    "    # Save the HTML/TXT website urls from doc_url to raw data folder\n",
    "    table_tag = soup.find('table', class_='tableFile', summary='Document Format Files')\n",
    "    rows = table_tag.find_all('tr')\n",
    "    cell_html = rows[1].find_all('td')\n",
    "    html = cell_html[2].a['href'].replace('ix?doc=/', '')\n",
    "    cell_txt = rows[-1].find_all('td')\n",
    "    txt = cell_txt[2].a['href']\n",
    "\n",
    "    if html.endswith(\"htm\") or html.endswith(\"txt\"):\n",
    "        web_url.append('https://www.sec.gov' + html)\n",
    "    else:\n",
    "        web_url.append('https://www.sec.gov' + txt)\n",
    "\n",
    "# t1_end = process_time()\n",
    "t1_end = time.time()\n",
    "print(\"Elapsed time during the whole program in seconds:\", t1_end - t1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnum</th>\n",
       "      <th>file_type</th>\n",
       "      <th>cik</th>\n",
       "      <th>name</th>\n",
       "      <th>sic</th>\n",
       "      <th>fd</th>\n",
       "      <th>rp</th>\n",
       "      <th>fye</th>\n",
       "      <th>item8k</th>\n",
       "      <th>bazip</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0000100063-96-000002</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>0000100063</td>\n",
       "      <td>TSI INC /MN/</td>\n",
       "      <td>3823</td>\n",
       "      <td>1996-02-15</td>\n",
       "      <td>1995-12-31</td>\n",
       "      <td>0331</td>\n",
       "      <td>NA</td>\n",
       "      <td>55126</td>\n",
       "      <td>MN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0001001039-96-000005</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>0001001039</td>\n",
       "      <td>WALT DISNEY CO/</td>\n",
       "      <td>NA</td>\n",
       "      <td>1996-02-14</td>\n",
       "      <td>1995-12-31</td>\n",
       "      <td>0930</td>\n",
       "      <td>NA</td>\n",
       "      <td>91521</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0001001606-96-000003</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>0001001606</td>\n",
       "      <td>BLOUNT INTERNATIONAL INC</td>\n",
       "      <td>NA</td>\n",
       "      <td>1996-01-16</td>\n",
       "      <td>1995-11-30</td>\n",
       "      <td>0228</td>\n",
       "      <td>NA</td>\n",
       "      <td>36109</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0000100378-96-000005</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>0000100378</td>\n",
       "      <td>TWIN DISC INC</td>\n",
       "      <td>3560</td>\n",
       "      <td>1996-02-14</td>\n",
       "      <td>1995-12-31</td>\n",
       "      <td>0630</td>\n",
       "      <td>NA</td>\n",
       "      <td>53403</td>\n",
       "      <td>WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000950112-96-000403</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>0000100441</td>\n",
       "      <td>TYCO INTERNATIONAL LTD</td>\n",
       "      <td>3569</td>\n",
       "      <td>1996-02-13</td>\n",
       "      <td>1995-12-31</td>\n",
       "      <td>0630</td>\n",
       "      <td>NA</td>\n",
       "      <td>03833</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83979</td>\n",
       "      <td>0000910680-98-000388</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>0000099703</td>\n",
       "      <td>AUDITS &amp; SURVEYS WORLDWIDE INC</td>\n",
       "      <td>8700</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>1998-09-30</td>\n",
       "      <td>1231</td>\n",
       "      <td>NA</td>\n",
       "      <td>10011</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83980</td>\n",
       "      <td>0000099780-98-000005</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>0000099780</td>\n",
       "      <td>TRINITY INDUSTRIES INC</td>\n",
       "      <td>3743</td>\n",
       "      <td>1998-11-13</td>\n",
       "      <td>1998-09-30</td>\n",
       "      <td>0331</td>\n",
       "      <td>NA</td>\n",
       "      <td>75207</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83981</td>\n",
       "      <td>0000099802-98-000006</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>0000099802</td>\n",
       "      <td>TRION INC</td>\n",
       "      <td>3564</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>1998-09-30</td>\n",
       "      <td>1231</td>\n",
       "      <td>NA</td>\n",
       "      <td>27331</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83982</td>\n",
       "      <td>0000009984-98-000007</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>0000009984</td>\n",
       "      <td>BARNES GROUP INC</td>\n",
       "      <td>3490</td>\n",
       "      <td>1998-11-13</td>\n",
       "      <td>1998-09-30</td>\n",
       "      <td>1231</td>\n",
       "      <td>NA</td>\n",
       "      <td>06011</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83983</td>\n",
       "      <td>0001031523-98-000009</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>0000099974</td>\n",
       "      <td>TJ INTERNATIONAL INC</td>\n",
       "      <td>2430</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>1998-10-03</td>\n",
       "      <td>1231</td>\n",
       "      <td>NA</td>\n",
       "      <td>83706</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83984 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     accnum file_type          cik  \\\n",
       "0      0000100063-96-000002      10-Q   0000100063   \n",
       "1      0001001039-96-000005      10-Q   0001001039   \n",
       "2      0001001606-96-000003      10-Q   0001001606   \n",
       "3      0000100378-96-000005      10-Q   0000100378   \n",
       "4      0000950112-96-000403      10-Q   0000100441   \n",
       "...                     ...       ...          ...   \n",
       "83979  0000910680-98-000388      10-Q   0000099703   \n",
       "83980  0000099780-98-000005      10-Q   0000099780   \n",
       "83981  0000099802-98-000006      10-Q   0000099802   \n",
       "83982  0000009984-98-000007      10-Q   0000009984   \n",
       "83983  0001031523-98-000009      10-Q   0000099974   \n",
       "\n",
       "                                 name   sic          fd          rp    fye  \\\n",
       "0                        TSI INC /MN/  3823  1996-02-15  1995-12-31   0331   \n",
       "1                     WALT DISNEY CO/    NA  1996-02-14  1995-12-31   0930   \n",
       "2            BLOUNT INTERNATIONAL INC    NA  1996-01-16  1995-11-30   0228   \n",
       "3                       TWIN DISC INC  3560  1996-02-14  1995-12-31   0630   \n",
       "4              TYCO INTERNATIONAL LTD  3569  1996-02-13  1995-12-31   0630   \n",
       "...                               ...   ...         ...         ...    ...   \n",
       "83979  AUDITS & SURVEYS WORLDWIDE INC  8700  1998-11-16  1998-09-30   1231   \n",
       "83980          TRINITY INDUSTRIES INC  3743  1998-11-13  1998-09-30   0331   \n",
       "83981                       TRION INC  3564  1998-11-16  1998-09-30   1231   \n",
       "83982                BARNES GROUP INC  3490  1998-11-13  1998-09-30   1231   \n",
       "83983            TJ INTERNATIONAL INC  2430  1998-11-16  1998-10-03   1231   \n",
       "\n",
       "      item8k  bazip state  \n",
       "0         NA  55126   MN   \n",
       "1         NA  91521   DE   \n",
       "2         NA  36109   DE   \n",
       "3         NA  53403   WI   \n",
       "4         NA  03833   MA   \n",
       "...      ...    ...   ...  \n",
       "83979     NA  10011   DE   \n",
       "83980     NA  75207   DE   \n",
       "83981     NA  27331   PA   \n",
       "83982     NA  06011   DE   \n",
       "83983     NA  83706   DE   \n",
       "\n",
       "[83984 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Save web_url to local index\n",
    "path_web_url_index = '..\\\\filings\\\\web_url_index_'+ obj_type + '_' + str(period_start) + '-' + str(period_end) + '.txt'\n",
    "with open(path_web_url_index, \"w\") as f:\n",
    "    for s in web_url:\n",
    "        f.write(s +\"\\n\")\n",
    "\n",
    "############### Scraping adjustments for some exceptional data\n",
    "for w in state:\n",
    "    if re.findall(r'\\DType', w) != []:\n",
    "        state[state.index(w)] = w.split('Type')[0]\n",
    "    if re.findall(r'\\dType', w) != []:\n",
    "        fye[state.index(w)] = w.split('Type')[0]\n",
    "        state[state.index(w)] = 'NA'\n",
    "    if w == ' 34 ':\n",
    "        state[state.index(w)] = 'NA'\n",
    "        \n",
    "for date in fye:\n",
    "    if re.findall(r'[A-Z]', date) != []:\n",
    "        state[fye.index(date)] = date\n",
    "        fye[fye.index(date)] = 'NA'\n",
    "    if re.findall('-', date) != [] and date != ' 333-05744-LA ' and date != ' 001-09550-2B ':\n",
    "        fye[fye.index(date)] = 'NA'\n",
    "    if date == ' 34 ':\n",
    "        fye[fye.index(date)] = 'NA'\n",
    "\n",
    "for zipcode in bazip:\n",
    "    if zipcode == '00000' or zipcode == '':\n",
    "        bazip[bazip.index(zipcode)] = 'NA'\n",
    "        \n",
    "############### Create Data Frame\n",
    "d = {'accnum': accnum, 'file_type': file_type, 'cik': cik, 'name': name, 'sic': sic, 'fd': fd, 'rp': rp, 'fye': fye, 'item8k': item8k, \\\n",
    "     'bazip': bazip, 'state': state}\n",
    "id_data = pd.DataFrame(data=d)\n",
    "id_data.to_csv('..\\\\filings\\\\id_data_' + obj_type + '_' + str(period_start) + '-' + str(period_end) +'.csv', index=False)\n",
    "\n",
    "id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############### Download HTML into TXT files (NOT RECOMMANDED DUE TO LARGE FILE SIZE)\n",
    "# for link in web_url:\n",
    "#     if os.path.exists('..\\\\filings\\\\raw\\\\'+str(accnum[web_url.index(link)])+'.txt') == False:\n",
    "#         urllib.request.urlretrieve(link, '..\\\\filings\\\\raw\\\\'+str(accnum[web_url.index(link)])+'.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
