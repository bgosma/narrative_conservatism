{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\github\\\\narrative_conservatism\\\\code'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### import packages\n",
    "import os, requests, sys, re, pandas as pd, time\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from time import process_time\n",
    "\n",
    "##########################################################\n",
    "##################### parameter ##########################\n",
    "##########################################################\n",
    "obj_type = '10-Q'\n",
    "period_start = 1993 # included\n",
    "period_end = 1995 # included\n",
    "\n",
    "############### Set working directory to parent directory\n",
    "os.getcwd()\n",
    "# os.chdir('F:\\\\github\\\\narrative_conservatism\\\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20755"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### Access all fillings through SEC master index #################################\n",
    "####### indexes downloaded using python-edgar: https://github.com/edouardswiac/python-edgar #######\n",
    "#### open terminal, and run the following lines:\n",
    "#### cd F:\\github\\python-edgar-master (switch dir to where the run.py script is located)\n",
    "#### python run.py -y 1993 -d edgar_idx (downloading all quarterly master index from 1993 into folder edgar_idx)\n",
    "\n",
    "#### cd F:\\github\\python-edgar-master\\edgar-idx (switch dir to where the downloaded indexes are located)\n",
    "#### cat *.tsv > master.tsv (stitch all quarterly indexes into one master index)\n",
    "#### du -h master.tsv (inspect how large the master index file is)\n",
    "\n",
    "index_edgar = list()\n",
    "doc_url = list()\n",
    "\n",
    "# create an index of downloaded local quarterly master indexes\n",
    "for subdir, dirs, files in os.walk(\"F:\\\\github\\\\python-edgar-master\\\\edgar-idx\"):\n",
    "    for file in files:\n",
    "        file_year = int(file.split('-')[0])\n",
    "        if file_year >= period_start and file_year <= period_end:\n",
    "            index_edgar.append(os.path.join(subdir, file))\n",
    "\n",
    "# read each index file, select rows with matched file type, and store matched doc_links\n",
    "for filenameTSV in index_edgar:\n",
    "    tsv_read = pd.read_csv(filenameTSV, sep='|', header=None, encoding = \"utf-8\")\n",
    "    tsv_read.columns = ['1', '2', '3', '4', '5', '6']\n",
    "    \n",
    "    # select the rows with filetype equal to predefined type\n",
    "    tsv_type = tsv_read.loc[tsv_read['3'] == obj_type]\n",
    "    doc_link = tsv_type['6'].values.tolist()\n",
    "    doc_link = ['https://www.sec.gov/Archives/' + w for w in doc_link]\n",
    "    for doc in doc_link:\n",
    "        doc_url.append(doc)\n",
    "        \n",
    "len(doc_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### Access all fillings through SEC search engine ####################################\n",
    "# ################## NOT RECOMMENDED AT ALL #############################################################\n",
    "# cik = '0000051143'\n",
    "# obj_type = '8-K'\n",
    "# number of documents listed per page\n",
    "# count = '100'\n",
    "# # index of first document listed in the current page\n",
    "# start = '0'\n",
    "# # find filings prior to the date 2016y01m01d\n",
    "# dateb = ''\n",
    "\n",
    "# # Obtain url for intial search result page\n",
    "# base_url = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type={}&dateb={}&start={}&count={}\"\n",
    "# init_url = base_url.format(cik, obj_type, dateb, start, count)\n",
    "\n",
    "# # define a function that takes the input url and returns next search page url\n",
    "# def get_next_url(input_url):\n",
    "#     edgar_resp = requests.get(input_url)\n",
    "#     edgar_str = edgar_resp.text\n",
    "#     soup = BeautifulSoup(edgar_str, 'html.parser')\n",
    "\n",
    "#     div_tag = soup.find('div', style='margin-top: 5px; margin-bottom: 5px;')\n",
    "#     button = div_tag.find('td', style='text-align: right;')\n",
    "#     fbutton = button.find_all('input')[0]['value']\n",
    "#     if re.findall(r'Next', fbutton) == ['Next']:\n",
    "#         next_url = button.find_all('input')[0]['onclick'][:-1]\n",
    "#     elif len(button.find_all('input')) == 2:\n",
    "#         next_url = button.find_all('input')[1]['onclick'][:-1]\n",
    "#     else:\n",
    "#         next_url = 'NA'\n",
    "        \n",
    "#     next_url = next_url.replace('parent.location=\\'', 'https://www.sec.gov')\n",
    "#     return next_url\n",
    "\n",
    "# # create a search result page url list\n",
    "# search_url = [init_url]\n",
    "\n",
    "# while get_next_url(init_url) != 'NA':\n",
    "#     search_url.append(get_next_url(init_url))\n",
    "#     init_url = get_next_url(init_url)\n",
    "    \n",
    "# ############### Create a document link list of a given CIK and file type\n",
    "# doc_link = list()\n",
    "\n",
    "# for url in search_url:\n",
    "#     edgar_resp = requests.get(url)\n",
    "#     edgar_str = edgar_resp.text\n",
    "#     soup = BeautifulSoup(edgar_str, 'html.parser')\n",
    "#     table_tag = soup.find('table', class_='tableFile2')\n",
    "#     rows = table_tag.find_all('tr')\n",
    "\n",
    "#     for row in rows[1:]:\n",
    "#         cells = row.find_all('td')\n",
    "#         doc_link.append('https://www.sec.gov' + cells[1].a['href'])\n",
    "        \n",
    "# len(doc_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:10<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time during the whole program in seconds: 27.3125\n"
     ]
    }
   ],
   "source": [
    "################## Save the HTML/TXT website urls from doc_url to raw data folder\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
    "web_url = list()\n",
    "\n",
    "# t1_start = process_time()\n",
    "t1_start = time.time()\n",
    "\n",
    "for doc in tqdm(doc_url[:1000]):\n",
    "    doc_resp = requests.get(doc, headers=headers)\n",
    "    doc_str = doc_resp.text\n",
    "    soup = BeautifulSoup(doc_str, 'html.parser')\n",
    "    \n",
    "    table_tag = soup.find('table', class_='tableFile', summary='Document Format Files')\n",
    "    rows = table_tag.find_all('tr')\n",
    "    cell_html = rows[1].find_all('td')\n",
    "    html = cell_html[2].a['href'].replace('ix?doc=/', '')\n",
    "    cell_txt = rows[-1].find_all('td')\n",
    "    txt = cell_txt[2].a['href']\n",
    "    \n",
    "    if html.endswith(\"htm\"):\n",
    "        web_url.append('https://www.sec.gov' + html)\n",
    "    else:\n",
    "        web_url.append('https://www.sec.gov' + txt)\n",
    "\n",
    "## Save to local index\n",
    "path_web_url_index = '..\\\\filings\\\\web_url_index_'+ obj_type + '.txt'\n",
    "with open(path_web_url_index, \"w\") as f:\n",
    "    for s in web_url:\n",
    "        f.write(s +\"\\n\")\n",
    "        \n",
    "# t1_end = process_time()\n",
    "t1_end = time.time()\n",
    "print(\"Elapsed time during the whole program in seconds:\", t1_end - t1_start)\n",
    "\n",
    "# web_url[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:11<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time during the whole program in seconds: 31.015625\n"
     ]
    }
   ],
   "source": [
    "############### Extract file identification info from doc_url\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
    "\n",
    "accnum = list()\n",
    "fd = list()\n",
    "rp = list()\n",
    "name = list()\n",
    "cik = list()\n",
    "# sic = list()\n",
    "# filetype = list()\n",
    "fye = list()\n",
    "state = list()\n",
    "bazip = list()\n",
    "item8k = list()\n",
    "\n",
    "# t2_start = process_time()\n",
    "t2_start = time.time()\n",
    "\n",
    "for doc in tqdm(doc_url[:1000]):\n",
    "    doc_resp = requests.get(doc, headers=headers)\n",
    "    doc_str = doc_resp.text\n",
    "    soup = BeautifulSoup(doc_str, 'html.parser')\n",
    "# Save the SEC accession number (accnum)\n",
    "    div_tag = soup.find('div', id='formHeader')\n",
    "    secnum = div_tag.find('div', id='secNum')\n",
    "    a = secnum.get_text().split()[3]\n",
    "    accnum.append(a)\n",
    "    \n",
    "# Save the Filing Date and Reporting Period\n",
    "    div_tag = soup.find('div', class_='formContent')\n",
    "    dates = div_tag.find_all('div', class_='info')\n",
    "    # Filing Date\n",
    "    a = dates[0].get_text()\n",
    "    fd.append(a)\n",
    "    # Reporting Period\n",
    "    b = dates[3].get_text()\n",
    "    rp.append(b)\n",
    "    # For 8K files, Save item info\n",
    "    if obj_type == '8-K':\n",
    "        c = dates[4].get_text()\n",
    "        clist = re.findall(r'\\d.\\d\\d', c)\n",
    "        if clist != []:\n",
    "            c = ', '.join(clist)\n",
    "            item8k.append(c)\n",
    "        else:\n",
    "            clist = re.findall(r'\\d', c)\n",
    "            c = ', '.join(clist)\n",
    "            item8k.append(c)\n",
    "    else :\n",
    "        c = 'NA'\n",
    "        item8k.append(c)\n",
    "        \n",
    "# Save the Company name and CIK\n",
    "    div_tag = soup.find('div', class_='companyInfo')\n",
    "    comname = div_tag.find('span', class_='companyName')\n",
    "    # Company Name\n",
    "    a = comname.get_text().split(\"\\n\")[0].replace(' (Filer)', '')\n",
    "    name.append(a)\n",
    "    # CIK\n",
    "    b = comname.get_text().split(\"\\n\")[1].replace('CIK: ', '').replace(' (see all company filings)', '')\n",
    "    cik.append(b)\n",
    "    \n",
    "# Save Business Address ZIP \n",
    "    div_tag = soup.find_all('div', class_='mailer')[1].find_all('span', class_='mailerAddress')[1]\n",
    "    ba = div_tag.get_text()\n",
    "    alist = re.findall(r'\\d\\d\\d\\d\\d', ba)\n",
    "    if alist == []:\n",
    "        div_tag = soup.find_all('div', class_='mailer')[1].find_all('span', class_='mailerAddress')[2]\n",
    "        ba = div_tag.get_text()\n",
    "        alist = re.findall(r'\\d\\d\\d\\d\\d', ba)\n",
    "    a = ', '.join(alist)\n",
    "    bazip.append(a)\n",
    "    \n",
    "# Save SIC, File Type, Fiscal Year End and State of Incorporation\n",
    "    div_tag = soup.find('div', class_='companyInfo')\n",
    "    filinginfo = div_tag.find('p', class_='identInfo')\n",
    "    # SIC\n",
    "#   a = filinginfo.get_text().split(\"|\")[5].split(\"SIC\")[1].split()[1]\n",
    "#   sic.append(a)\n",
    "    # File Type\n",
    "#   b = filinginfo.get_text().split(\"|\")[2].split(\"Type\")[1].split(\":\")[1]\n",
    "#   filetype.append(b)\n",
    "    # Fiscal Year End\n",
    "    c = filinginfo.get_text().split(\"|\")[2].split(\"Type\")[0].split(\":\")[1]\n",
    "    fye.append(c)\n",
    "    # State\n",
    "    d = filinginfo.get_text().split(\"|\")[1].split(\":\")[1]\n",
    "    state.append(d)\n",
    "\n",
    "# t2_end = process_time()\n",
    "t2_end = time.time()\n",
    "print(\"Elapsed time during the whole program in seconds:\", t2_end - t2_start)\n",
    "\n",
    "## Save accnum to local index\n",
    "# with open('..\\\\filings\\\\web_url_index_'+ obj_type + '.txt', \"w\") as f:\n",
    "#    for s in accnum:\n",
    "#        f.write(s +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>name</th>\n",
       "      <th>fd</th>\n",
       "      <th>rp</th>\n",
       "      <th>filetype</th>\n",
       "      <th>item8k</th>\n",
       "      <th>accnum</th>\n",
       "      <th>bazip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0000060512</td>\n",
       "      <td>LOUISIANA LAND &amp; EXPLORATION CO</td>\n",
       "      <td>1993-08-13</td>\n",
       "      <td>1993-06-30</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>NA</td>\n",
       "      <td>0000060512-94-000005</td>\n",
       "      <td>70112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0000066740</td>\n",
       "      <td>MINNESOTA MINING &amp; MANUFACTURING CO</td>\n",
       "      <td>1993-08-13</td>\n",
       "      <td>1993-06-30</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>NA</td>\n",
       "      <td>0000066740-94-000015</td>\n",
       "      <td>55144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000011860</td>\n",
       "      <td>BETHLEHEM STEEL CORP /DE/</td>\n",
       "      <td>1993-11-12</td>\n",
       "      <td>1993-09-30</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>NA</td>\n",
       "      <td>0000011860-94-000005</td>\n",
       "      <td>18016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0000205239</td>\n",
       "      <td>DATAPOINT CORP</td>\n",
       "      <td>1993-12-14</td>\n",
       "      <td>1993-10-30</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>NA</td>\n",
       "      <td>0000205239-94-000003</td>\n",
       "      <td>75008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000020762</td>\n",
       "      <td>CLARK REFINING &amp; MARKETING INC</td>\n",
       "      <td>1993-11-12</td>\n",
       "      <td>1993-09-30</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>NA</td>\n",
       "      <td>0000950131-94-000025</td>\n",
       "      <td>63105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>0000003370</td>\n",
       "      <td>ALCO STANDARD CORP</td>\n",
       "      <td>1994-05-13</td>\n",
       "      <td>1994-03-31</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>NA</td>\n",
       "      <td>0000950109-94-000821</td>\n",
       "      <td>19482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>0000033798</td>\n",
       "      <td>GROSSMANS INC</td>\n",
       "      <td>1994-05-09</td>\n",
       "      <td>1994-03-31</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>NA</td>\n",
       "      <td>0000950135-94-000329</td>\n",
       "      <td>02184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>0000033837</td>\n",
       "      <td>EVEREST &amp; JENNINGS INTERNATIONAL LTD</td>\n",
       "      <td>1994-05-16</td>\n",
       "      <td>1994-03-31</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>NA</td>\n",
       "      <td>0000033837-94-000009</td>\n",
       "      <td>63132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>0000034088</td>\n",
       "      <td>EXXON CORP</td>\n",
       "      <td>1994-05-11</td>\n",
       "      <td>1994-03-31</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>NA</td>\n",
       "      <td>0000034088-94-000002</td>\n",
       "      <td>75062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>0000034136</td>\n",
       "      <td>FAB INDUSTRIES INC</td>\n",
       "      <td>1994-04-11</td>\n",
       "      <td>1994-02-26</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>NA</td>\n",
       "      <td>0000034136-94-000002</td>\n",
       "      <td>10016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             cik                                  name          fd  \\\n",
       "0     0000060512       LOUISIANA LAND & EXPLORATION CO  1993-08-13   \n",
       "1     0000066740   MINNESOTA MINING & MANUFACTURING CO  1993-08-13   \n",
       "2     0000011860             BETHLEHEM STEEL CORP /DE/  1993-11-12   \n",
       "3     0000205239                        DATAPOINT CORP  1993-12-14   \n",
       "4     0000020762        CLARK REFINING & MARKETING INC  1993-11-12   \n",
       "..           ...                                   ...         ...   \n",
       "995   0000003370                    ALCO STANDARD CORP  1994-05-13   \n",
       "996   0000033798                         GROSSMANS INC  1994-05-09   \n",
       "997   0000033837  EVEREST & JENNINGS INTERNATIONAL LTD  1994-05-16   \n",
       "998   0000034088                            EXXON CORP  1994-05-11   \n",
       "999   0000034136                    FAB INDUSTRIES INC  1994-04-11   \n",
       "\n",
       "             rp filetype item8k                accnum  bazip  \n",
       "0    1993-06-30     10-Q     NA  0000060512-94-000005  70112  \n",
       "1    1993-06-30     10-Q     NA  0000066740-94-000015  55144  \n",
       "2    1993-09-30     10-Q     NA  0000011860-94-000005  18016  \n",
       "3    1993-10-30     10-Q     NA  0000205239-94-000003  75008  \n",
       "4    1993-09-30     10-Q     NA  0000950131-94-000025  63105  \n",
       "..          ...      ...    ...                   ...    ...  \n",
       "995  1994-03-31     10-Q     NA  0000950109-94-000821  19482  \n",
       "996  1994-03-31     10-Q     NA  0000950135-94-000329  02184  \n",
       "997  1994-03-31     10-Q     NA  0000033837-94-000009  63132  \n",
       "998  1994-03-31     10-Q     NA  0000034088-94-000002  75062  \n",
       "999  1994-02-26     10-Q     NA  0000034136-94-000002  10016  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Create Data Frame\n",
    "d = {'cik': cik, 'name': name, 'fd': fd, 'rp': rp, 'filetype': obj_type, 'item8k': item8k, 'accnum': accnum, 'bazip': bazip}\n",
    "id_data = pd.DataFrame(data=d)\n",
    "id_data.to_csv('..\\\\filings\\\\id_data_' + obj_type +'.csv', index=False)\n",
    "\n",
    "id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############### Download HTML into TXT files (NOT RECOMMANDED DUE TO LARGE FILE SIZE)\n",
    "# for link in web_url:\n",
    "#     if os.path.exists('..\\\\filings\\\\raw\\\\'+str(accnum[web_url.index(link)])+'.txt') == False:\n",
    "#         urllib.request.urlretrieve(link, '..\\\\filings\\\\raw\\\\'+str(accnum[web_url.index(link)])+'.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
