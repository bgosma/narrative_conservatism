{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\github\\\\narrative_conservatism\\\\code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### import packages\n",
    "import os, nltk, numpy as np, pandas as pd, time, textstat\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from time import process_time\n",
    "\n",
    "##########################################################\n",
    "##################### parameter ##########################\n",
    "##########################################################\n",
    "obj_type = '10-Q'\n",
    "period_start = 2017 # included\n",
    "period_end = 2019 # included\n",
    "\n",
    "############### Set working directory to parent directory\n",
    "os.getcwd()\n",
    "# os.chdir('F:\\\\github\\\\narrative_conservatism\\\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Read LM disctionary\n",
    "LM = pd.read_excel('..\\\\LM\\\\LoughranMcDonald_MasterDictionary_2018.xlsx', encoding = \"utf-8\")\n",
    "\n",
    "############### Create negative, positive, uncertainty, litigious, constraining and modal word lists\n",
    "lm_neg = LM.loc[LM['Negative'] != 0]['Word'].values.tolist()\n",
    "lm_pos = LM.loc[LM['Positive'] != 0]['Word'].values.tolist()\n",
    "lm_uctt = LM.loc[LM['Uncertainty'] != 0]['Word'].values.tolist()\n",
    "lm_lit = LM.loc[LM['Litigious'] != 0]['Word'].values.tolist()\n",
    "lm_cstr = LM.loc[LM['Constraining'] != 0]['Word'].values.tolist()\n",
    "\n",
    "lm_modal1 = LM.loc[LM['Modal'] == 1]['Word'].values.tolist()\n",
    "lm_modal2 = LM.loc[LM['Modal'] == 2]['Word'].values.tolist()\n",
    "lm_modal3 = LM.loc[LM['Modal'] == 3]['Word'].values.tolist()\n",
    "\n",
    "lm_neg = [w.lower() for w in lm_neg]\n",
    "lm_pos = [w.lower() for w in lm_pos]\n",
    "lm_uctt = [w.lower() for w in lm_uctt]\n",
    "lm_lit = [w.lower() for w in lm_lit]\n",
    "lm_cstr = [w.lower() for w in lm_cstr]\n",
    "lm_modal1 = [w.lower() for w in lm_modal1]\n",
    "lm_modal2 = [w.lower() for w in lm_modal2]\n",
    "lm_modal3 = [w.lower() for w in lm_modal3]\n",
    "\n",
    "############## Read and create stop words list\n",
    "lm_stop = list()\n",
    "with open('..\\\\LM\\\\StopWords_Generic.txt', \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n', '')\n",
    "        lm_stop.append(line)\n",
    "        \n",
    "lm_stop = [w.lower() for w in lm_stop]\n",
    "\n",
    "############# Create a negation word list\n",
    "gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55909"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################\n",
    "#################### FOR ALL PROCESSED FILES LOOP ###################\n",
    "#####################################################################\n",
    "\n",
    "############# Create input txt file index\n",
    "processed = list()\n",
    "for subdir, dirs, files in os.walk(\"H:\\\\data\\\\edgar\\\\processed\\\\\" + obj_type + '\\\\' + str(period_start) + '-' + str(period_end)):\n",
    "    for file in files:\n",
    "        processed.append(os.path.join(subdir, file))\n",
    "\n",
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define a function count_occurrence to count the number of words in tup that pertaining to a list \n",
    "def count_occurrence(tup, lst): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst: \n",
    "            count+= 1\n",
    "      \n",
    "    return count\n",
    "\n",
    "### Define a function count_negation to count cases where negation occurs within four or fewer words from a word identified in list.\n",
    "def count_negation(tup, lst, negation): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst:\n",
    "            if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "                neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "                for neighborw in neighbor:\n",
    "                    if neighborw in negation:\n",
    "                        count+= 1\n",
    "\n",
    "            if tup.index(item)-4 < 0:\n",
    "                pre = tup[0:tup.index(item)+4]\n",
    "                for prew in pre:\n",
    "                    if prew in negation:\n",
    "                        count+= 1\n",
    "                        \n",
    "            if tup.index(item)+4 > len(tup):\n",
    "                post = tup[tup.index(item)-4:len(tup)]\n",
    "                for postw in post:\n",
    "                    if postw in negation:\n",
    "                        count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 55909/55909 [38:06:46<00:00,  2.45s/it]\n"
     ]
    }
   ],
   "source": [
    "############ Full Text Raw Count\n",
    "accnum = list()\n",
    "\n",
    "nw = list()\n",
    "nvocab = list()\n",
    "\n",
    "n_neg = list()\n",
    "n_pos = list()\n",
    "n_uctt = list()\n",
    "n_lit = list()\n",
    "n_cstr = list()\n",
    "n_modal1 = list()\n",
    "n_modal2 = list()\n",
    "n_modal3 = list()\n",
    "n_negation = list()\n",
    "READ = list()\n",
    "\n",
    "############ Word Tokenization, count nword and nvocab, count negative, positive, uncertainty, litigious, constraining and modal words\n",
    "for text in tqdm(processed):\n",
    "    ############# Create an array of accession number\n",
    "    a = text.split(\"\\\\\")[6].split(\".\")[0]\n",
    "    accnum.append(a)\n",
    "    \n",
    "    ############# Read processed txt file\n",
    "    with open(text, 'r',  encoding = \"utf-8\") as file:\n",
    "        contents = file.read().replace('\\n', ' ').replace(u'\\xa0', u' ')\n",
    "        # print(repr(contents))\n",
    "        \n",
    "        ############ Word Tokenization\n",
    "        ## Raw tokens: including punctuations, numbers etc.\n",
    "        tokens = word_tokenize(contents)\n",
    "\n",
    "        ## Convert all words into small cases\n",
    "        ## Keep tokens that purely consist of alphabetic characters only\n",
    "        ## Delete single-character words except for 'I'\n",
    "        words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "        \n",
    "        ########### Delete words with lenth smaller than 1% and largr than 99% of the document\n",
    "        # wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "        # wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "        # words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "        vocab = sorted(set(words))\n",
    "        \n",
    "        ########### Save text statistics\n",
    "        ##### 1. nw 2. nvocab 3. tone 4. readability\n",
    "        \n",
    "        ## 1. nw\n",
    "        a = len(words)\n",
    "        nw.append(a)\n",
    "        \n",
    "        ## 2. nvocab\n",
    "        b = len(vocab)\n",
    "        nvocab.append(b)\n",
    "        \n",
    "        ## 3. tone\n",
    "        neg = count_occurrence(words, lm_neg)\n",
    "        n_neg.append(neg)\n",
    "        pos = count_occurrence(words, lm_pos)\n",
    "        n_pos.append(pos)\n",
    "        uctt = count_occurrence(words, lm_uctt)\n",
    "        n_uctt.append(uctt)\n",
    "        lit = count_occurrence(words, lm_lit)\n",
    "        n_lit.append(lit)\n",
    "        cstr = count_occurrence(words, lm_cstr)\n",
    "        n_cstr.append(cstr)\n",
    "        modal1 = count_occurrence(words, lm_modal1)\n",
    "        n_modal1.append(modal1)\n",
    "        modal2 = count_occurrence(words, lm_modal2)\n",
    "        n_modal2.append(modal2)\n",
    "        modal3 = count_occurrence(words, lm_modal3)\n",
    "        n_modal3.append(modal3)\n",
    "        negation = count_negation(words, lm_pos, gt_negation)\n",
    "        n_negation.append(negation)\n",
    "        \n",
    "        ## 4. readability\n",
    "        read = textstat.gunning_fog(contents)\n",
    "        READ.append(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnum</th>\n",
       "      <th>nw</th>\n",
       "      <th>nvocab</th>\n",
       "      <th>n_neg</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>n_uctt</th>\n",
       "      <th>n_lit</th>\n",
       "      <th>n_cstr</th>\n",
       "      <th>n_modal_week</th>\n",
       "      <th>n_modal_moderate</th>\n",
       "      <th>n_modal_strong</th>\n",
       "      <th>n_negation</th>\n",
       "      <th>READ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0000002178-17-000038</td>\n",
       "      <td>8563</td>\n",
       "      <td>1462</td>\n",
       "      <td>150</td>\n",
       "      <td>34</td>\n",
       "      <td>118</td>\n",
       "      <td>128</td>\n",
       "      <td>54</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>66.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0000002178-17-000050</td>\n",
       "      <td>9100</td>\n",
       "      <td>1494</td>\n",
       "      <td>163</td>\n",
       "      <td>37</td>\n",
       "      <td>107</td>\n",
       "      <td>121</td>\n",
       "      <td>53</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>70.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000002178-17-000081</td>\n",
       "      <td>10529</td>\n",
       "      <td>1518</td>\n",
       "      <td>212</td>\n",
       "      <td>38</td>\n",
       "      <td>119</td>\n",
       "      <td>134</td>\n",
       "      <td>58</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>80.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0000002178-18-000022</td>\n",
       "      <td>8445</td>\n",
       "      <td>1327</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>88</td>\n",
       "      <td>98</td>\n",
       "      <td>61</td>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>75.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000002178-18-000057</td>\n",
       "      <td>10439</td>\n",
       "      <td>1437</td>\n",
       "      <td>116</td>\n",
       "      <td>47</td>\n",
       "      <td>100</td>\n",
       "      <td>109</td>\n",
       "      <td>80</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>73.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55904</td>\n",
       "      <td>0001781755-19-000007</td>\n",
       "      <td>16988</td>\n",
       "      <td>1804</td>\n",
       "      <td>118</td>\n",
       "      <td>68</td>\n",
       "      <td>250</td>\n",
       "      <td>67</td>\n",
       "      <td>105</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>20.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55905</td>\n",
       "      <td>0001784031-19-000003</td>\n",
       "      <td>17329</td>\n",
       "      <td>1741</td>\n",
       "      <td>197</td>\n",
       "      <td>44</td>\n",
       "      <td>157</td>\n",
       "      <td>116</td>\n",
       "      <td>133</td>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>19.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55906</td>\n",
       "      <td>0001784031-19-000010</td>\n",
       "      <td>17306</td>\n",
       "      <td>1741</td>\n",
       "      <td>204</td>\n",
       "      <td>42</td>\n",
       "      <td>150</td>\n",
       "      <td>123</td>\n",
       "      <td>132</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>18.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55907</td>\n",
       "      <td>0001785982-19-000066</td>\n",
       "      <td>1969</td>\n",
       "      <td>525</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>19.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55908</td>\n",
       "      <td>9999999997-17-006760</td>\n",
       "      <td>190</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55909 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     accnum     nw  nvocab  n_neg  n_pos  n_uctt  n_lit  \\\n",
       "0      0000002178-17-000038   8563    1462    150     34     118    128   \n",
       "1      0000002178-17-000050   9100    1494    163     37     107    121   \n",
       "2      0000002178-17-000081  10529    1518    212     38     119    134   \n",
       "3      0000002178-18-000022   8445    1327    100     36      88     98   \n",
       "4      0000002178-18-000057  10439    1437    116     47     100    109   \n",
       "...                     ...    ...     ...    ...    ...     ...    ...   \n",
       "55904  0001781755-19-000007  16988    1804    118     68     250     67   \n",
       "55905  0001784031-19-000003  17329    1741    197     44     157    116   \n",
       "55906  0001784031-19-000010  17306    1741    204     42     150    123   \n",
       "55907  0001785982-19-000066   1969     525     16      4      16     15   \n",
       "55908  9999999997-17-006760    190      74      0      0       0      1   \n",
       "\n",
       "       n_cstr  n_modal_week  n_modal_moderate  n_modal_strong  n_negation  \\\n",
       "0          54            24                26              32           0   \n",
       "1          53            25                27              25           0   \n",
       "2          58            23                26              27           0   \n",
       "3          61            20                28              26           0   \n",
       "4          80            29                31              26           0   \n",
       "...       ...           ...               ...             ...         ...   \n",
       "55904     105            32                32              37           3   \n",
       "55905     133            42                32              62           0   \n",
       "55906     132            40                31              53           0   \n",
       "55907      15             3                 8               6           0   \n",
       "55908       0             0                 0               0           0   \n",
       "\n",
       "        READ  \n",
       "0      66.03  \n",
       "1      70.08  \n",
       "2      80.45  \n",
       "3      75.11  \n",
       "4      73.94  \n",
       "...      ...  \n",
       "55904  20.16  \n",
       "55905  19.70  \n",
       "55906  18.13  \n",
       "55907  19.36  \n",
       "55908  53.16  \n",
       "\n",
       "[55909 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Create Data Frame: full document\n",
    "d = {'accnum': accnum, 'nw': nw, 'nvocab': nvocab, 'n_neg': n_neg, 'n_pos': n_pos, 'n_uctt': n_uctt, 'n_lit': n_lit, 'n_cstr': n_cstr, \\\n",
    "     'n_modal_week': n_modal1, 'n_modal_moderate': n_modal2, 'n_modal_strong': n_modal3, 'n_negation': n_negation, 'READ': READ}\n",
    "\n",
    "text_data = pd.DataFrame(data=d)\n",
    "text_data.to_csv('..\\\\filings\\\\text_data_' + obj_type + '_' + str(period_start) + '-' + str(period_end) + '.csv', index=False)\n",
    "\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [06:04<00:00,  2.89it/s]\n"
     ]
    }
   ],
   "source": [
    "############ MDA and NOTE Raw Count\n",
    "accnum = list()\n",
    "\n",
    "nw_mda = list()\n",
    "nvocab_mda = list()\n",
    "\n",
    "n_neg_mda = list()\n",
    "n_pos_mda = list()\n",
    "n_uctt_mda = list()\n",
    "n_lit_mda = list()\n",
    "n_cstr_mda = list()\n",
    "n_modal1_mda = list()\n",
    "n_modal2_mda = list()\n",
    "n_modal3_mda = list()\n",
    "n_negation_mda = list()\n",
    "\n",
    "READ_mda = list()\n",
    "\n",
    "nw_note = list()\n",
    "nvocab_note = list()\n",
    "\n",
    "n_neg_note = list()\n",
    "n_pos_note = list()\n",
    "n_uctt_note = list()\n",
    "n_lit_note = list()\n",
    "n_cstr_note = list()\n",
    "n_modal1_note = list()\n",
    "n_modal2_note = list()\n",
    "n_modal3_note = list()\n",
    "n_negation_note = list()\n",
    "\n",
    "READ_note = list()\n",
    "\n",
    "############ Word Tokenization, count nword and nvocab, count negative, positive, uncertainty, litigious, constraining and modal words\n",
    "for text in tqdm(processed):\n",
    "    ############# Create an array of accession number\n",
    "    a = text.split(\"\\\\\")[6].split(\".\")[0]\n",
    "    accnum.append(a)\n",
    "    \n",
    "    ############# Read processed txt file\n",
    "    with open(text, 'r',  encoding = \"utf-8\") as file:\n",
    "        contents = file.read().replace('\\n', ' ').replace(u'\\xa0', u' ')\n",
    "        ############################## TO EXTRACT MDA AND NOTES SECTION, UNCOMMENT THIS SECTION ################################\n",
    "        try:\n",
    "            mda = contents[contents.index(\"ITEM 2.\"):contents.index(\"ITEM 3.\")]\n",
    "        except:\n",
    "            try:\n",
    "                mda = contents[contents.index(\"Item 2.\"):contents.index(\"Item 3.\")]\n",
    "            except:\n",
    "                try:\n",
    "                    mda = contents[contents.index(\"ITEM 2\"):contents.index(\"ITEM 3\")]\n",
    "                except:\n",
    "                    try:\n",
    "                        mda = contents[contents.index(\"Item 2\"):contents.index(\"Item 3\")]\n",
    "                    except:\n",
    "                        mda = ''\n",
    "                        pass\n",
    "                    \n",
    "        try:\n",
    "            note = contents[contents.index(\"NOTES TO\"):contents.index(\"ITEM 2.\")]\n",
    "        except:\n",
    "            try:\n",
    "                note = contents[contents.index(\"NOTES TO\"):contents.index(\"ITEM 2\")]\n",
    "            except:\n",
    "                try:\n",
    "                    note = contents[contents.index(\"Notes to\"):contents.index(\"Item 2.\")]\n",
    "                except:\n",
    "                    try:\n",
    "                        note = contents[contents.index(\"Notes to\"):contents.index(\"Item 2\")]\n",
    "                    except:\n",
    "                        note = ''\n",
    "                        pass\n",
    "        ###########################################################################################################\n",
    "        \n",
    "        ############ Word Tokenization\n",
    "        ## Raw tokens: including punctuations, numbers etc.\n",
    "        tokens_mda = word_tokenize(mda)\n",
    "        tokens_note = word_tokenize(note)\n",
    "        \n",
    "        ####################################################################\n",
    "\n",
    "        ## Convert all words into small cases\n",
    "        ## Keep tokens that purely consist of alphabetic characters only\n",
    "        ## Delete single-character words except for 'I'\n",
    "        words_mda = [w.lower() for w in tokens_mda if w.isalpha() and len(w)>1 or w =='i']\n",
    "        words_note = [w.lower() for w in tokens_note if w.isalpha() and len(w)>1 or w =='i']\n",
    "        \n",
    "        ########### Delete words with lenth smaller than 1% and largr than 99% of the document\n",
    "        # wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "        # wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "        # words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "        vocab_mda = sorted(set(words_mda))\n",
    "        vocab_note = sorted(set(words_note))\n",
    "        \n",
    "        ########### Save text statistics\n",
    "        ##### 1. nw 2. nvocab 3. tone 4. readability\n",
    "        \n",
    "        ## 1. nw\n",
    "        a_mda = len(words_mda)\n",
    "        nw_mda.append(a_mda)\n",
    "        a_note = len(words_note)\n",
    "        nw_note.append(a_note)\n",
    "        \n",
    "        ## 2. nvocab\n",
    "#         b_mda = len(vocab_mda)\n",
    "#         nvocab_mda.append(b_mda)\n",
    "#         b_note = len(vocab_note)\n",
    "#         nvocab_note.append(b_note)\n",
    "        \n",
    "        ## 3. tone\n",
    "        neg_mda = count_occurrence(words_mda, lm_neg)\n",
    "        n_neg_mda.append(neg_mda)\n",
    "        pos_mda = count_occurrence(words_mda, lm_pos)\n",
    "        n_pos_mda.append(pos_mda)\n",
    "#         uctt_mda = count_occurrence(words_mda, lm_uctt)\n",
    "#         n_uctt_mda.append(uctt_mda)\n",
    "#         lit_mda = count_occurrence(words_mda, lm_lit)\n",
    "#         n_lit_mda.append(lit_mda)\n",
    "#         cstr_mda = count_occurrence(words_mda, lm_cstr)\n",
    "#         n_cstr_mda.append(cstr_mda)\n",
    "#         modal1_mda = count_occurrence(words_mda, lm_modal1)\n",
    "#         n_modal1_mda.append(modal1_mda)\n",
    "#         modal2_mda = count_occurrence(words_mda, lm_modal2)\n",
    "#         n_modal2_mda.append(modal2_mda)\n",
    "#         modal3_mda = count_occurrence(words_mda, lm_modal3)\n",
    "#         n_modal3_mda.append(modal3_mda)\n",
    "        negation_mda = count_negation(words_mda, lm_pos, gt_negation)\n",
    "        n_negation_mda.append(negation_mda)\n",
    "        \n",
    "        neg_note = count_occurrence(words_note, lm_neg)\n",
    "        n_neg_note.append(neg_note)\n",
    "        pos_note = count_occurrence(words_note, lm_pos)\n",
    "        n_pos_note.append(pos_note)\n",
    "#         uctt_note = count_occurrence(words_note, lm_uctt)\n",
    "#         n_uctt_note.append(uctt_note)\n",
    "#         lit_note = count_occurrence(words_note, lm_lit)\n",
    "#         n_lit_note.append(lit_note)\n",
    "#         cstr_note = count_occurrence(words_note, lm_cstr)\n",
    "#         n_cstr_note.append(cstr_note)\n",
    "#         modal1_note = count_occurrence(words_note, lm_modal1)\n",
    "#         n_modal1_note.append(modal1_note)\n",
    "#         modal2_note = count_occurrence(words_note, lm_modal2)\n",
    "#         n_modal2_note.append(modal2_note)\n",
    "#         modal3_note = count_occurrence(words_note, lm_modal3)\n",
    "#         n_modal3_note.append(modal3_note)\n",
    "        negation_note = count_negation(words_note, lm_pos, gt_negation)\n",
    "        n_negation_note.append(negation_note)\n",
    "        \n",
    "        ## 4. readability\n",
    "        read_mda = textstat.gunning_fog(mda)\n",
    "        READ_mda.append(read_mda)\n",
    "        read_note = textstat.gunning_fog(note)\n",
    "        READ_note.append(read_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of filings whose MDA and NOTES are successfully extracted: 0.5975261655566128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnum</th>\n",
       "      <th>nw_mda</th>\n",
       "      <th>n_neg_mda</th>\n",
       "      <th>n_pos_mda</th>\n",
       "      <th>n_negation_mda</th>\n",
       "      <th>nw_note</th>\n",
       "      <th>n_neg_note</th>\n",
       "      <th>n_pos_note</th>\n",
       "      <th>n_negation_note</th>\n",
       "      <th>READ_MDA</th>\n",
       "      <th>READ_NOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000004127-20-000007</td>\n",
       "      <td>1544</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2585</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>25.45</td>\n",
       "      <td>21.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0000004457-20-000027</td>\n",
       "      <td>8667</td>\n",
       "      <td>89</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>7863</td>\n",
       "      <td>73</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>21.31</td>\n",
       "      <td>16.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000006281-20-000013</td>\n",
       "      <td>3067</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>4851</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>27.21</td>\n",
       "      <td>24.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0000006951-20-000014</td>\n",
       "      <td>6441</td>\n",
       "      <td>75</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>7804</td>\n",
       "      <td>90</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>19.35</td>\n",
       "      <td>17.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0000008670-20-000007</td>\n",
       "      <td>5946</td>\n",
       "      <td>46</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>4037</td>\n",
       "      <td>44</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>22.05</td>\n",
       "      <td>20.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1043</td>\n",
       "      <td>0001744489-20-000046</td>\n",
       "      <td>6929</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>15947</td>\n",
       "      <td>196</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>21.28</td>\n",
       "      <td>20.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1044</td>\n",
       "      <td>0001748790-20-000010</td>\n",
       "      <td>4016</td>\n",
       "      <td>86</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>5728</td>\n",
       "      <td>103</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>22.71</td>\n",
       "      <td>21.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>0001757898-20-000003</td>\n",
       "      <td>6543</td>\n",
       "      <td>127</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>7777</td>\n",
       "      <td>119</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>19.39</td>\n",
       "      <td>18.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>0001772016-20-000018</td>\n",
       "      <td>2997</td>\n",
       "      <td>43</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>6041</td>\n",
       "      <td>74</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>25.24</td>\n",
       "      <td>20.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0001773383-20-000004</td>\n",
       "      <td>6134</td>\n",
       "      <td>40</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>7024</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>22.40</td>\n",
       "      <td>19.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>628 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    accnum  nw_mda  n_neg_mda  n_pos_mda  n_negation_mda  \\\n",
       "2     0000004127-20-000007    1544         13          8               1   \n",
       "3     0000004457-20-000027    8667         89         48               0   \n",
       "4     0000006281-20-000013    3067         10         18               1   \n",
       "6     0000006951-20-000014    6441         75         39               0   \n",
       "8     0000008670-20-000007    5946         46         81               0   \n",
       "...                    ...     ...        ...        ...             ...   \n",
       "1043  0001744489-20-000046    6929         80         30               0   \n",
       "1044  0001748790-20-000010    4016         86         28               0   \n",
       "1047  0001757898-20-000003    6543        127         53               1   \n",
       "1049  0001772016-20-000018    2997         43         20               0   \n",
       "1050  0001773383-20-000004    6134         40         46               2   \n",
       "\n",
       "      nw_note  n_neg_note  n_pos_note  n_negation_note  READ_MDA  READ_NOTE  \n",
       "2        2585          34           8                0     25.45      21.77  \n",
       "3        7863          73          36                0     21.31      16.73  \n",
       "4        4851          30          32                0     27.21      24.21  \n",
       "6        7804          90          47                0     19.35      17.19  \n",
       "8        4037          44          21                0     22.05      20.50  \n",
       "...       ...         ...         ...              ...       ...        ...  \n",
       "1043    15947         196          73                0     21.28      20.96  \n",
       "1044     5728         103          33                0     22.71      21.56  \n",
       "1047     7777         119          38                2     19.39      18.97  \n",
       "1049     6041          74          30                0     25.24      20.11  \n",
       "1050     7024          49          20                0     22.40      19.06  \n",
       "\n",
       "[628 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Create Data Frame: MDA and NOTES\n",
    "d = {'accnum': accnum, 'nw_mda': nw_mda, 'n_neg_mda': n_neg_mda, 'n_pos_mda': n_pos_mda, 'n_negation_mda': n_negation_mda, 'nw_note': nw_note, 'n_neg_note': n_neg_note, 'n_pos_note': n_pos_note, 'n_negation_note': n_negation_note, 'READ_MDA':READ_mda, 'READ_NOTE':READ_note}\n",
    "#      'nvocab_mda': nvocab_mda, 'n_uctt_mda': n_uctt_mda, 'n_lit_mda': n_lit_mda, 'n_cstr_mda': n_cstr_mda, \\\n",
    "#      'n_modal_strong_mda': n_modal1_mda, 'n_modal_moderate_mda': n_modal2_mda, 'n_modal_weak_mda': n_modal3_mda, \\\n",
    "#      'nvocab_note': nvocab_note, 'n_uctt_note': n_uctt_note, 'n_lit_note': n_lit_note, 'n_cstr_note': n_cstr_note, \\\n",
    "#      'n_modal_strong_note': n_modal1_note, 'n_modal_moderate_note': n_modal2_note, 'n_modal_weak_note': n_modal3_note}\n",
    "\n",
    "text_data = pd.DataFrame(data=d)\n",
    "print('percentage of filings whose MDA and NOTES are successfully extracted: ' + str(text_data[(text_data['nw_mda']!=0) & (text_data['nw_note']!=0)].shape[0]/text_data.shape[0]))\n",
    "text_data = text_data[(text_data['nw_mda']!=0) & (text_data['nw_note']!=0)]\n",
    "\n",
    "text_data.to_csv('..\\\\filings\\\\text_data_section_' + str(period_start) + '-' + str(period_end) + '.csv', index=False)\n",
    "text_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################################\n",
    "# ################### FOR SINGLE FILE INSPECTION ######################\n",
    "# #####################################################################\n",
    "\n",
    "# ############ Word Tokenization\n",
    "# ## Raw tokens: including punctuations, numbers etc.\n",
    "# with open(processed[1], 'r',  encoding = \"utf-8\") as file:\n",
    "#     contents = file.read().replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "# tokens = word_tokenize(contents)\n",
    "\n",
    "# #tokens\n",
    "\n",
    "# ## Convert all words into small cases\n",
    "# ## And keep tokens that purely consist of alphabetic characters only\n",
    "# words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# # words[2500:2600]\n",
    "# # vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_occurrence(tup, lst): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst: \n",
    "#             count+= 1\n",
    "      \n",
    "#     return count\n",
    "\n",
    "# count_occurrence(words, lm_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing\n",
    "\n",
    "# def count_negation(tup, lst, negation): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst:\n",
    "#             if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "#                 neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "#                 for neighborw in neighbor:\n",
    "#                     if neighborw in negation:\n",
    "#                         count+= 1\n",
    "\n",
    "#             if tup.index(item)-4 < 0:\n",
    "#                 pre = tup[0:tup.index(item)+4]\n",
    "#                 for prew in pre:\n",
    "#                     if prew in negation:\n",
    "#                         count+= 1\n",
    "                        \n",
    "#             if tup.index(item)+4 > len(tup):\n",
    "#                 post = tup[tup.index(item)-4:len(tup)]\n",
    "#                 for postw in post:\n",
    "#                     if postw in negation:\n",
    "#                         count+= 1\n",
    "#     return count\n",
    "\n",
    "# count_negation(words, lm_pos, gt_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########### Winsorize words with lenth smaller than 1% and largr than 99% of the document\n",
    "# wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "# wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "# words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### See the most common 20 words\n",
    "# fdist = nltk.FreqDist(words)\n",
    "# fdist.most_common(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
