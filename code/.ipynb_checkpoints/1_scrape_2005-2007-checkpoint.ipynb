{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\github\\\\narrative_conservatism\\\\code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### import packages\n",
    "import os, requests, sys, re, pandas as pd, time\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from time import process_time\n",
    "\n",
    "##########################################################\n",
    "##################### parameter ##########################\n",
    "##########################################################\n",
    "obj_type = '8-K'\n",
    "period_start = 2005 # included\n",
    "period_end = 2007 # included\n",
    "\n",
    "############### Set working directory to parent directory\n",
    "os.getcwd()\n",
    "# os.chdir('F:\\\\github\\\\narrative_conservatism\\\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323896"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### Access all fillings through SEC master index #################################\n",
    "####### indexes downloaded using python-edgar: https://github.com/edouardswiac/python-edgar #######\n",
    "#### open terminal, and run the following lines:\n",
    "#### cd F:\\github\\python-edgar-master (switch dir to where the run.py script is located)\n",
    "#### python run.py -y 1993 -d edgar_idx (downloading all quarterly master index from 1993 into folder edgar_idx)\n",
    "\n",
    "#### cd F:\\github\\python-edgar-master\\edgar-idx (switch dir to where the downloaded indexes are located)\n",
    "#### cat *.tsv > master.tsv (stitch all quarterly indexes into one master index)\n",
    "#### du -h master.tsv (inspect how large the master index file is)\n",
    "\n",
    "index_edgar = list()\n",
    "doc_url = list()\n",
    "\n",
    "# create an index of downloaded local quarterly master indexes\n",
    "for subdir, dirs, files in os.walk(\"F:\\\\github\\\\python-edgar-master\\\\edgar-idx\"):\n",
    "    for file in files:\n",
    "        file_year = int(file.split('-')[0])\n",
    "        if file_year >= period_start and file_year <= period_end:\n",
    "            index_edgar.append(os.path.join(subdir, file))\n",
    "\n",
    "# read each index file, select rows with matched file type, and store matched doc_links\n",
    "for filenameTSV in index_edgar:\n",
    "    tsv_read = pd.read_csv(filenameTSV, sep='|', header=None, encoding = \"utf-8\")\n",
    "    tsv_read.columns = ['1', '2', '3', '4', '5', '6']\n",
    "    \n",
    "    # select the rows with filetype equal to predefined type\n",
    "    tsv_type = tsv_read.loc[tsv_read['3'] == obj_type]\n",
    "    doc_link = tsv_type['6'].values.tolist()\n",
    "    doc_link = ['https://www.sec.gov/Archives/' + w for w in doc_link]\n",
    "    for doc in doc_link:\n",
    "        doc_url.append(doc)\n",
    "        \n",
    "len(doc_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### Access all fillings through SEC search engine ####################################\n",
    "# ################## NOT RECOMMENDED AT ALL #############################################################\n",
    "# cik = '0000051143'\n",
    "# obj_type = '8-K'\n",
    "# number of documents listed per page\n",
    "# count = '100'\n",
    "# # index of first document listed in the current page\n",
    "# start = '0'\n",
    "# # find filings prior to the date 2016y01m01d\n",
    "# dateb = ''\n",
    "\n",
    "# # Obtain url for intial search result page\n",
    "# base_url = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type={}&dateb={}&start={}&count={}\"\n",
    "# init_url = base_url.format(cik, obj_type, dateb, start, count)\n",
    "\n",
    "# # define a function that takes the input url and returns next search page url\n",
    "# def get_next_url(input_url):\n",
    "#     edgar_resp = requests.get(input_url)\n",
    "#     edgar_str = edgar_resp.text\n",
    "#     soup = BeautifulSoup(edgar_str, 'html.parser')\n",
    "\n",
    "#     div_tag = soup.find('div', style='margin-top: 5px; margin-bottom: 5px;')\n",
    "#     button = div_tag.find('td', style='text-align: right;')\n",
    "#     fbutton = button.find_all('input')[0]['value']\n",
    "#     if re.findall(r'Next', fbutton) == ['Next']:\n",
    "#         next_url = button.find_all('input')[0]['onclick'][:-1]\n",
    "#     elif len(button.find_all('input')) == 2:\n",
    "#         next_url = button.find_all('input')[1]['onclick'][:-1]\n",
    "#     else:\n",
    "#         next_url = 'NA'\n",
    "        \n",
    "#     next_url = next_url.replace('parent.location=\\'', 'https://www.sec.gov')\n",
    "#     return next_url\n",
    "\n",
    "# # create a search result page url list\n",
    "# search_url = [init_url]\n",
    "\n",
    "# while get_next_url(init_url) != 'NA':\n",
    "#     search_url.append(get_next_url(init_url))\n",
    "#     init_url = get_next_url(init_url)\n",
    "    \n",
    "# ############### Create a document link list of a given CIK and file type\n",
    "# doc_link = list()\n",
    "\n",
    "# for url in search_url:\n",
    "#     edgar_resp = requests.get(url)\n",
    "#     edgar_str = edgar_resp.text\n",
    "#     soup = BeautifulSoup(edgar_str, 'html.parser')\n",
    "#     table_tag = soup.find('table', class_='tableFile2')\n",
    "#     rows = table_tag.find_all('tr')\n",
    "\n",
    "#     for row in rows[1:]:\n",
    "#         cells = row.find_all('td')\n",
    "#         doc_link.append('https://www.sec.gov' + cells[1].a['href'])\n",
    "        \n",
    "# len(doc_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████████████████████████▏                       | 212067/323896 [17:09:00<8:02:11,  3.87it/s]"
     ]
    },
    {
     "ename": "SSLError",
     "evalue": "HTTPSConnectionPool(host='www.sec.gov', port=443): Max retries exceeded with url: /Archives/edgar/data/355429/0000355429-06-000376-index.html (Caused by SSLError(SSLError(\"bad handshake: SysCallError(10060, 'WSAETIMEDOUT')\")))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSysCallError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m                 \u001b[0mcnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    457\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWantReadError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_do_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1915\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36m_raise_ssl_error\u001b[1;34m(self, ssl, result)\u001b[0m\n\u001b[0;32m   1638\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0merrno\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mSysCallError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrorcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1640\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSysCallError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Unexpected EOF\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSysCallError\u001b[0m: (10060, 'WSAETIMEDOUT')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sock'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 839\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m             ssl_context=context)\n\u001b[0m\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir)\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mHAS_SNI\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mserver_hostname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname)\u001b[0m\n\u001b[0;32m    461\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mssl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bad handshake: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSSLError\u001b[0m: (\"bad handshake: SysCallError(10060, 'WSAETIMEDOUT')\",)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[1;32m--> 638\u001b[1;33m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[0;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.sec.gov', port=443): Max retries exceeded with url: /Archives/edgar/data/355429/0000355429-06-000376-index.html (Caused by SSLError(SSLError(\"bad handshake: SysCallError(10060, 'WSAETIMEDOUT')\")))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-070e7c44769f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdoc_resp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mdoc_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc_resp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m                 \u001b[1;31m# This branch is for urllib3 v1.22 and later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSSLError\u001b[0m: HTTPSConnectionPool(host='www.sec.gov', port=443): Max retries exceeded with url: /Archives/edgar/data/355429/0000355429-06-000376-index.html (Caused by SSLError(SSLError(\"bad handshake: SysCallError(10060, 'WSAETIMEDOUT')\")))"
     ]
    }
   ],
   "source": [
    "############### Extract file identification info from doc_url\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
    "\n",
    "accnum = list()\n",
    "fd = list()\n",
    "rp = list()\n",
    "name = list()\n",
    "cik = list()\n",
    "sic = list()\n",
    "file_type = list()\n",
    "fye = list()\n",
    "state = list()\n",
    "bazip = list()\n",
    "item8k = list()\n",
    "web_url = list()\n",
    "\n",
    "# t1_start = process_time()\n",
    "t1_start = time.time()\n",
    "\n",
    "for doc in tqdm(doc_url[212067:]):\n",
    "    doc_resp = requests.get(doc, headers=headers)\n",
    "    doc_str = doc_resp.text\n",
    "    soup = BeautifulSoup(doc_str, 'html.parser')\n",
    "    \n",
    "    # Save the SEC accession number (accnum)\n",
    "    try:\n",
    "        div_tag = soup.find('div', id='formHeader')\n",
    "        secnum = div_tag.find('div', id='secNum')\n",
    "        a = secnum.get_text().split()[3]\n",
    "        accnum.append(a)\n",
    "    except:\n",
    "        accnum.append('NA')\n",
    "        pass\n",
    "\n",
    "    # Save the Filing Date and Reporting Period\n",
    "    try:\n",
    "        div_tag = soup.find('div', class_='formContent')\n",
    "        dates = div_tag.find_all('div', class_='info')\n",
    "        # Filing Date\n",
    "        a = dates[0].get_text()\n",
    "        fd.append(a)\n",
    "    except:\n",
    "        fd.append('NA')\n",
    "        pass\n",
    "    \n",
    "        # Reporting Period\n",
    "    try:\n",
    "        b = dates[3].get_text()\n",
    "        rp.append(b)\n",
    "    except:\n",
    "        rp.append('NA')\n",
    "        pass\n",
    "    \n",
    "    # For 8K files, Save item info\n",
    "    try:\n",
    "        if obj_type == '8-K':\n",
    "            c = dates[4].get_text()\n",
    "            clist = re.findall(r'\\d.\\d\\d', c)\n",
    "            if clist != []:\n",
    "                c = ', '.join(clist)\n",
    "                item8k.append(c)\n",
    "            else:\n",
    "                clist = re.findall(r'\\d', c)\n",
    "                c = ', '.join(clist)\n",
    "                item8k.append(c)\n",
    "        else :\n",
    "            c = 'NA'\n",
    "            item8k.append(c)\n",
    "    except:\n",
    "        item8k.append('NA')\n",
    "        pass\n",
    "\n",
    "        # Save the Company name and CIK\n",
    "    try:\n",
    "        div_tag = soup.find('div', class_='companyInfo')\n",
    "        comname = div_tag.find('span', class_='companyName')\n",
    "        # Company Name\n",
    "        a = comname.get_text().split(\"\\n\")[0].replace(' (Filer)', '')\n",
    "        name.append(a)\n",
    "    except:\n",
    "        name.append('NA')\n",
    "        pass\n",
    "    \n",
    "        # CIK\n",
    "    try:\n",
    "        b = comname.get_text().split(\"\\n\")[1].replace('CIK: ', '').replace(' (see all company filings)', '')\n",
    "        cik.append(b)\n",
    "    except:\n",
    "        cik.append('NA')\n",
    "        pass\n",
    "\n",
    "        # Save Business Address ZIP \n",
    "    try:\n",
    "        div_tag = soup.find_all('div', class_='mailer')[1].find_all('span', class_='mailerAddress')[1]\n",
    "        ba = div_tag.get_text()\n",
    "        alist = re.findall(r'\\d\\d\\d\\d\\d', ba)\n",
    "        if alist == []:\n",
    "            div_tag = soup.find_all('div', class_='mailer')[1].find_all('span', class_='mailerAddress')[2]\n",
    "            ba = div_tag.get_text()\n",
    "            alist = re.findall(r'\\d\\d\\d\\d\\d', ba)\n",
    "        a = ', '.join(alist)\n",
    "        bazip.append(a)\n",
    "    except:\n",
    "        bazip.append('NA')\n",
    "        pass\n",
    "\n",
    "        # Save SIC, File Type, Fiscal Year End and State of Incorporation\n",
    "    try:\n",
    "        div_tag = soup.find('div', class_='companyInfo')\n",
    "        filinginfo = div_tag.find('p', class_='identInfo')\n",
    "        # SIC\n",
    "        a = filinginfo.get_text().split(\"|\")[5].split(\"SIC\")[1].split()[1]\n",
    "        sic.append(a)\n",
    "    except:\n",
    "        sic.append('NA')\n",
    "        pass\n",
    "    \n",
    "    # File Type\n",
    "    b = obj_type\n",
    "    file_type.append(b)\n",
    "        \n",
    "        # Fiscal Year End\n",
    "    try:\n",
    "        c = filinginfo.get_text().split(\"|\")[2].split(\"Type\")[0].split(\":\")[1]\n",
    "        fye.append(c)\n",
    "    except:\n",
    "        fye.append('NA')\n",
    "        pass\n",
    "    \n",
    "        # State\n",
    "    try:\n",
    "        d = filinginfo.get_text().split(\"|\")[1].split(\":\")[1]\n",
    "        state.append(d)\n",
    "    except:\n",
    "        state.append('NA')\n",
    "        pass\n",
    "\n",
    "    # Save the HTML/TXT website urls from doc_url to raw data folder\n",
    "    table_tag = soup.find('table', class_='tableFile', summary='Document Format Files')\n",
    "    rows = table_tag.find_all('tr')\n",
    "    cell_html = rows[1].find_all('td')\n",
    "    html = cell_html[2].a['href'].replace('ix?doc=/', '')\n",
    "    cell_txt = rows[-1].find_all('td')\n",
    "    txt = cell_txt[2].a['href']\n",
    "\n",
    "    if html.endswith(\"htm\") or html.endswith(\"txt\"):\n",
    "        web_url.append('https://www.sec.gov' + html)\n",
    "    else:\n",
    "        web_url.append('https://www.sec.gov' + txt)\n",
    "\n",
    "# t1_end = process_time()\n",
    "t1_end = time.time()\n",
    "print(\"Elapsed time during the whole program in seconds:\", t1_end - t1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnum</th>\n",
       "      <th>file_type</th>\n",
       "      <th>cik</th>\n",
       "      <th>name</th>\n",
       "      <th>sic</th>\n",
       "      <th>fd</th>\n",
       "      <th>rp</th>\n",
       "      <th>fye</th>\n",
       "      <th>item8k</th>\n",
       "      <th>bazip</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0001104659-05-006777</td>\n",
       "      <td>8-K</td>\n",
       "      <td>0001000015</td>\n",
       "      <td>META GROUP INC</td>\n",
       "      <td>8700</td>\n",
       "      <td>2005-02-15</td>\n",
       "      <td>2005-02-15</td>\n",
       "      <td>1231</td>\n",
       "      <td>2.02, 9.01</td>\n",
       "      <td>06912</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0000950123-05-003812</td>\n",
       "      <td>8-K</td>\n",
       "      <td>0001000015</td>\n",
       "      <td>META GROUP INC</td>\n",
       "      <td>8700</td>\n",
       "      <td>2005-03-30</td>\n",
       "      <td>2005-03-28</td>\n",
       "      <td>1231</td>\n",
       "      <td>1.01, 9.01</td>\n",
       "      <td>06912</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0001000045-05-000001</td>\n",
       "      <td>8-K</td>\n",
       "      <td>0001000045</td>\n",
       "      <td>NICHOLAS FINANCIAL INC</td>\n",
       "      <td>6153</td>\n",
       "      <td>2005-01-27</td>\n",
       "      <td>2005-01-27</td>\n",
       "      <td>0331</td>\n",
       "      <td>2.02, 9.01</td>\n",
       "      <td>33759</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0001000045-05-000002</td>\n",
       "      <td>8-K</td>\n",
       "      <td>0001000045</td>\n",
       "      <td>NICHOLAS FINANCIAL INC</td>\n",
       "      <td>6153</td>\n",
       "      <td>2005-02-22</td>\n",
       "      <td>2005-02-22</td>\n",
       "      <td>0331</td>\n",
       "      <td>8.01, 9.01</td>\n",
       "      <td>33759</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0001085037-05-000114</td>\n",
       "      <td>8-K</td>\n",
       "      <td>0001000157</td>\n",
       "      <td>DATAWAVE SYSTEMS INC</td>\n",
       "      <td>4812</td>\n",
       "      <td>2005-01-27</td>\n",
       "      <td>2005-01-26</td>\n",
       "      <td>0331</td>\n",
       "      <td>1.01, 3.02, 9.01</td>\n",
       "      <td>NA</td>\n",
       "      <td>B0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212062</td>\n",
       "      <td>0001193125-06-256301</td>\n",
       "      <td>8-K</td>\n",
       "      <td>0000035527</td>\n",
       "      <td>FIFTH THIRD BANCORP</td>\n",
       "      <td>6022</td>\n",
       "      <td>2006-12-19</td>\n",
       "      <td>2006-12-13</td>\n",
       "      <td>1231</td>\n",
       "      <td>7.01, 8.01, 9.01</td>\n",
       "      <td>45263</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212063</td>\n",
       "      <td>0001193125-06-259830</td>\n",
       "      <td>8-K</td>\n",
       "      <td>0000035527</td>\n",
       "      <td>FIFTH THIRD BANCORP</td>\n",
       "      <td>6022</td>\n",
       "      <td>2006-12-26</td>\n",
       "      <td>2006-12-19</td>\n",
       "      <td>1231</td>\n",
       "      <td>5.02, 9.01</td>\n",
       "      <td>45263</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212064</td>\n",
       "      <td>0000355300-06-000028</td>\n",
       "      <td>8-K</td>\n",
       "      <td>0000355300</td>\n",
       "      <td>ENDEVCO INC</td>\n",
       "      <td>1311</td>\n",
       "      <td>2006-11-28</td>\n",
       "      <td>2006-10-31</td>\n",
       "      <td>1231</td>\n",
       "      <td>5.02</td>\n",
       "      <td>77057</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212065</td>\n",
       "      <td>0001193125-06-202778</td>\n",
       "      <td>8-K</td>\n",
       "      <td>0000355356</td>\n",
       "      <td>IMPERIAL PETROLEUM INC</td>\n",
       "      <td>1381</td>\n",
       "      <td>2006-10-04</td>\n",
       "      <td>2006-09-29</td>\n",
       "      <td>0731</td>\n",
       "      <td>4.01, 9.01</td>\n",
       "      <td>47708</td>\n",
       "      <td>NV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212066</td>\n",
       "      <td>0000355429-06-000373</td>\n",
       "      <td>8-K</td>\n",
       "      <td>0000355429</td>\n",
       "      <td>PROTECTIVE LIFE CORP</td>\n",
       "      <td>6311</td>\n",
       "      <td>2006-10-11</td>\n",
       "      <td>2006-10-11</td>\n",
       "      <td>1231</td>\n",
       "      <td>5.02</td>\n",
       "      <td>35223</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212067 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      accnum file_type          cik                    name  \\\n",
       "0       0001104659-05-006777       8-K   0001000015          META GROUP INC   \n",
       "1       0000950123-05-003812       8-K   0001000015          META GROUP INC   \n",
       "2       0001000045-05-000001       8-K   0001000045  NICHOLAS FINANCIAL INC   \n",
       "3       0001000045-05-000002       8-K   0001000045  NICHOLAS FINANCIAL INC   \n",
       "4       0001085037-05-000114       8-K   0001000157    DATAWAVE SYSTEMS INC   \n",
       "...                      ...       ...          ...                     ...   \n",
       "212062  0001193125-06-256301       8-K   0000035527     FIFTH THIRD BANCORP   \n",
       "212063  0001193125-06-259830       8-K   0000035527     FIFTH THIRD BANCORP   \n",
       "212064  0000355300-06-000028       8-K   0000355300             ENDEVCO INC   \n",
       "212065  0001193125-06-202778       8-K   0000355356  IMPERIAL PETROLEUM INC   \n",
       "212066  0000355429-06-000373       8-K   0000355429    PROTECTIVE LIFE CORP   \n",
       "\n",
       "         sic          fd          rp    fye            item8k  bazip state  \n",
       "0       8700  2005-02-15  2005-02-15   1231        2.02, 9.01  06912   DE   \n",
       "1       8700  2005-03-30  2005-03-28   1231        1.01, 9.01  06912   DE   \n",
       "2       6153  2005-01-27  2005-01-27   0331        2.02, 9.01  33759   FL   \n",
       "3       6153  2005-02-22  2005-02-22   0331        8.01, 9.01  33759   FL   \n",
       "4       4812  2005-01-27  2005-01-26   0331  1.01, 3.02, 9.01     NA   B0   \n",
       "...      ...         ...         ...    ...               ...    ...   ...  \n",
       "212062  6022  2006-12-19  2006-12-13   1231  7.01, 8.01, 9.01  45263   OH   \n",
       "212063  6022  2006-12-26  2006-12-19   1231        5.02, 9.01  45263   OH   \n",
       "212064  1311  2006-11-28  2006-10-31   1231              5.02  77057   TX   \n",
       "212065  1381  2006-10-04  2006-09-29   0731        4.01, 9.01  47708   NV   \n",
       "212066  6311  2006-10-11  2006-10-11   1231              5.02  35223   DE   \n",
       "\n",
       "[212067 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Save web_url to local index\n",
    "path_web_url_index = '..\\\\filings\\\\web_url_index_'+ obj_type + '_' + str(period_start) + '-' + str(period_end) + '.txt'\n",
    "with open(path_web_url_index, \"w\") as f:\n",
    "    for s in web_url:\n",
    "        f.write(s +\"\\n\")\n",
    "        \n",
    "############### Scraping adjustments for some exceptional data\n",
    "for index, w in enumerate(state):\n",
    "    if re.findall(r'\\DType', w) != []:\n",
    "        state[index] = w.split('Type')[0]\n",
    "    if re.findall(r'\\dType', w) != []:\n",
    "        fye[index] = w.split('Type')[0]\n",
    "        state[index] = 'NA'\n",
    "    if w == ' 34 ':\n",
    "        state[index] = 'NA'\n",
    "        \n",
    "for index, date in enumerate(fye):\n",
    "    if re.findall(r'[A-Z]', date) != []:\n",
    "        state[index] = date\n",
    "        fye[index] = 'NA'\n",
    "    if re.findall('-', date) != []:\n",
    "        fye[index] = 'NA'\n",
    "    if date == ' 34 ':\n",
    "        fye[index] = 'NA'\n",
    "\n",
    "for index, zipcode in enumerate(bazip):\n",
    "    if zipcode == '00000' or zipcode == '':\n",
    "        bazip[index] = 'NA'\n",
    "        \n",
    "############### Create Data Frame\n",
    "d = {'accnum': accnum, 'file_type': file_type, 'cik': cik, 'name': name, 'sic': sic, 'fd': fd, 'rp': rp, 'fye': fye, 'item8k': item8k, \\\n",
    "     'bazip': bazip, 'state': state}\n",
    "id_data = pd.DataFrame(data=d)\n",
    "id_data.to_csv('..\\\\filings\\\\id_data_' + obj_type + '_' + str(period_start) + '-' + str(period_end) +'.csv', index=False)\n",
    "\n",
    "id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############### Download HTML into TXT files (NOT RECOMMANDED DUE TO LARGE FILE SIZE)\n",
    "# for link in web_url:\n",
    "#     if os.path.exists('..\\\\filings\\\\raw\\\\'+str(accnum[web_url.index(link)])+'.txt') == False:\n",
    "#         urllib.request.urlretrieve(link, '..\\\\filings\\\\raw\\\\'+str(accnum[web_url.index(link)])+'.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
