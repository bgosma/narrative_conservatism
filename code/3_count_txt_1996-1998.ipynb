{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\github\\\\narrative_conservatism\\\\code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### import packages\n",
    "import os, nltk, numpy as np, pandas as pd, time\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from time import process_time\n",
    "\n",
    "##########################################################\n",
    "##################### parameter ##########################\n",
    "##########################################################\n",
    "obj_type = '8-K'\n",
    "period_start = 1996 # included\n",
    "period_end = 1998 # included\n",
    "\n",
    "############### Set working directory to parent directory\n",
    "os.getcwd()\n",
    "# os.chdir('F:\\\\github\\\\narrative_conservatism\\\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Read LM disctionary\n",
    "LM = pd.read_excel('..\\\\LM\\\\LoughranMcDonald_MasterDictionary_2018.xlsx', encoding = \"utf-8\")\n",
    "\n",
    "############### Create negative, positive, uncertainty, litigious, constraining and modal word lists\n",
    "lm_neg = LM.loc[LM['Negative'] != 0]['Word'].values.tolist()\n",
    "lm_pos = LM.loc[LM['Positive'] != 0]['Word'].values.tolist()\n",
    "lm_uctt = LM.loc[LM['Uncertainty'] != 0]['Word'].values.tolist()\n",
    "lm_lit = LM.loc[LM['Litigious'] != 0]['Word'].values.tolist()\n",
    "lm_cstr = LM.loc[LM['Constraining'] != 0]['Word'].values.tolist()\n",
    "\n",
    "lm_modal1 = LM.loc[LM['Modal'] == 1]['Word'].values.tolist()\n",
    "lm_modal2 = LM.loc[LM['Modal'] == 2]['Word'].values.tolist()\n",
    "lm_modal3 = LM.loc[LM['Modal'] == 3]['Word'].values.tolist()\n",
    "\n",
    "lm_neg = [w.lower() for w in lm_neg]\n",
    "lm_pos = [w.lower() for w in lm_pos]\n",
    "lm_uctt = [w.lower() for w in lm_uctt]\n",
    "lm_lit = [w.lower() for w in lm_lit]\n",
    "lm_cstr = [w.lower() for w in lm_cstr]\n",
    "lm_modal1 = [w.lower() for w in lm_modal1]\n",
    "lm_modal2 = [w.lower() for w in lm_modal2]\n",
    "lm_modal3 = [w.lower() for w in lm_modal3]\n",
    "\n",
    "############## Read and create stop words list\n",
    "lm_stop = list()\n",
    "with open('..\\\\LM\\\\StopWords_Generic.txt', \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n', '')\n",
    "        lm_stop.append(line)\n",
    "        \n",
    "lm_stop = [w.lower() for w in lm_stop]\n",
    "\n",
    "############# Create a negation word list\n",
    "gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65978"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################\n",
    "#################### FOR ALL PROCESSED FILES LOOP ###################\n",
    "#####################################################################\n",
    "\n",
    "############# Create processed txt file index\n",
    "processed = list()\n",
    "for subdir, dirs, files in os.walk(\"H:\\\\data\\\\edgar\\\\processed\\\\\" + obj_type + '\\\\' + str(period_start) + '-' + str(period_end)):\n",
    "    for file in files:\n",
    "        processed.append(os.path.join(subdir, file))\n",
    "\n",
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 40308/40308 [6:59:36<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time during the whole program in seconds: 25177.013712644577\n"
     ]
    }
   ],
   "source": [
    "############ Word Tokenization, count nword and nvocab, count negative, positive, uncertainty, litigious, constraining and modal words\n",
    "accnum = list()\n",
    "\n",
    "nw = list()\n",
    "nvocab = list()\n",
    "\n",
    "### Define a function count_occurrence to count the number of words in tup that pertaining to a lst \n",
    "def count_occurrence(tup, lst): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst: \n",
    "            count+= 1\n",
    "      \n",
    "    return count\n",
    "\n",
    "### Define a function count_negation to count cases where negation occurs within four or fewer words from a word identified in list.\n",
    "def count_negation(tup, lst, negation): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst:\n",
    "            if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "                neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "                for neighborw in neighbor:\n",
    "                    if neighborw in negation:\n",
    "                        count+= 1\n",
    "\n",
    "            if tup.index(item)-4 < 0:\n",
    "                pre = tup[0:tup.index(item)+4]\n",
    "                for prew in pre:\n",
    "                    if prew in negation:\n",
    "                        count+= 1\n",
    "                        \n",
    "            if tup.index(item)+4 > len(tup):\n",
    "                post = tup[tup.index(item)-4:len(tup)]\n",
    "                for postw in post:\n",
    "                    if postw in negation:\n",
    "                        count+= 1\n",
    "    return count\n",
    "\n",
    "n_neg = list()\n",
    "n_pos = list()\n",
    "n_uctt = list()\n",
    "n_lit = list()\n",
    "n_cstr = list()\n",
    "n_modal1 = list()\n",
    "n_modal2 = list()\n",
    "n_modal3 = list()\n",
    "n_negation = list()\n",
    "net_pos = list()\n",
    "\n",
    "# t1_start = process_time()\n",
    "t1_start = time.time()\n",
    "\n",
    "for text in tqdm(processed[25670:]):\n",
    "    ############# Create an array of accession number\n",
    "    a = text.split(\"\\\\\")[6].split(\".\")[0]\n",
    "    accnum.append(a)\n",
    "    \n",
    "    ############# Read processed txt file\n",
    "    with open(text, 'r',  encoding = \"utf-8\") as file:\n",
    "        contents = file.read().replace('\\n', '').replace('\\xa0', '')\n",
    "        # print(repr(contents))\n",
    "        \n",
    "        ############ Word Tokenization\n",
    "        ## Raw tokens: including punctuations, numbers etc.\n",
    "        tokens = word_tokenize(contents)\n",
    "\n",
    "        ## Convert all words into small cases\n",
    "        ## Keep tokens that purely consist of alphabetic characters only\n",
    "        ## Delete single-character words except for 'I'\n",
    "        words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "        \n",
    "        ########### Delete words with lenth smaller than 1% and largr than 99% of the document\n",
    "        # wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "        # wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "        # words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "        vocab = sorted(set(words))\n",
    "        \n",
    "        ########### Save text statistics\n",
    "        ##### 1. nw: 1) nw 2) nw_mda 3) nw_notes\n",
    "        ##### 2. nvocab: 1) nvvocab 2) nvocab_mda 3) nvocab_notes\n",
    "        ##### 3. tone: 1) tone 2) tone_mda 3) tone_notes\n",
    "        \n",
    "        ## 1.1) nw\n",
    "        a = len(words)\n",
    "        nw.append(a)\n",
    "        \n",
    "        ## 2.1) nvocab\n",
    "        b = len(vocab)\n",
    "        nvocab.append(b)\n",
    "        \n",
    "        ## 3.1) tone\n",
    "        neg = count_occurrence(words, lm_neg)\n",
    "        n_neg.append(neg)\n",
    "        pos = count_occurrence(words, lm_pos)\n",
    "        n_pos.append(pos)\n",
    "        uctt = count_occurrence(words, lm_uctt)\n",
    "        n_uctt.append(uctt)\n",
    "        lit = count_occurrence(words, lm_lit)\n",
    "        n_lit.append(lit)\n",
    "        cstr = count_occurrence(words, lm_cstr)\n",
    "        n_cstr.append(cstr)\n",
    "        modal1 = count_occurrence(words, lm_modal1)\n",
    "        n_modal1.append(modal1)\n",
    "        modal2 = count_occurrence(words, lm_modal2)\n",
    "        n_modal2.append(modal2)\n",
    "        modal3 = count_occurrence(words, lm_modal3)\n",
    "        n_modal3.append(modal3)\n",
    "        negation = count_negation(words, lm_pos, gt_negation)\n",
    "        n_negation.append(negation)\n",
    "        netpos = pos - negation\n",
    "        net_pos.append(netpos)\n",
    "\n",
    "# t1_end = process_time()\n",
    "t1_end = time.time()\n",
    "print(\"Elapsed time during the whole program in seconds:\", t1_end - t1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnum</th>\n",
       "      <th>nw</th>\n",
       "      <th>nvocab</th>\n",
       "      <th>n_neg</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>n_uctt</th>\n",
       "      <th>n_lit</th>\n",
       "      <th>n_cstr</th>\n",
       "      <th>n_modal_week</th>\n",
       "      <th>n_modal_moderate</th>\n",
       "      <th>n_modal_strong</th>\n",
       "      <th>n_negation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0000912057-97-000264</td>\n",
       "      <td>81812</td>\n",
       "      <td>5154</td>\n",
       "      <td>1306</td>\n",
       "      <td>397</td>\n",
       "      <td>440</td>\n",
       "      <td>2798</td>\n",
       "      <td>516</td>\n",
       "      <td>258</td>\n",
       "      <td>93</td>\n",
       "      <td>312</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0000912057-97-000266</td>\n",
       "      <td>462</td>\n",
       "      <td>252</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000912057-97-000278</td>\n",
       "      <td>3902</td>\n",
       "      <td>940</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0000912057-97-000323</td>\n",
       "      <td>32623</td>\n",
       "      <td>3168</td>\n",
       "      <td>577</td>\n",
       "      <td>140</td>\n",
       "      <td>159</td>\n",
       "      <td>1298</td>\n",
       "      <td>216</td>\n",
       "      <td>84</td>\n",
       "      <td>36</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000912057-97-000333</td>\n",
       "      <td>1029</td>\n",
       "      <td>344</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40303</td>\n",
       "      <td>0001072758-98-000001</td>\n",
       "      <td>812</td>\n",
       "      <td>322</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40304</td>\n",
       "      <td>0001073339-98-000020</td>\n",
       "      <td>319</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40305</td>\n",
       "      <td>0001073339-98-000024</td>\n",
       "      <td>348</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40306</td>\n",
       "      <td>0001073339-98-000025</td>\n",
       "      <td>353</td>\n",
       "      <td>193</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40307</td>\n",
       "      <td>0001075946-98-000001</td>\n",
       "      <td>354</td>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40308 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     accnum     nw  nvocab  n_neg  n_pos  n_uctt  n_lit  \\\n",
       "0      0000912057-97-000264  81812    5154   1306    397     440   2798   \n",
       "1      0000912057-97-000266    462     252      1      4       0      4   \n",
       "2      0000912057-97-000278   3902     940     22      7      35     15   \n",
       "3      0000912057-97-000323  32623    3168    577    140     159   1298   \n",
       "4      0000912057-97-000333   1029     344      1      2       7      6   \n",
       "...                     ...    ...     ...    ...    ...     ...    ...   \n",
       "40303  0001072758-98-000001    812     322     22      1       0      5   \n",
       "40304  0001073339-98-000020    319     177      0      0       3      5   \n",
       "40305  0001073339-98-000024    348     196      0      1       1      5   \n",
       "40306  0001073339-98-000025    353     193      0      1       2      5   \n",
       "40307  0001075946-98-000001    354     167      0      0       0      5   \n",
       "\n",
       "       n_cstr  n_modal_week  n_modal_moderate  n_modal_strong  n_negation  \n",
       "0         516           258                93             312           9  \n",
       "1           1             1                 0               0           0  \n",
       "2           8            12                 6               8           1  \n",
       "3         216            84                36             131           6  \n",
       "4           2             0                 4               1           0  \n",
       "...       ...           ...               ...             ...         ...  \n",
       "40303       2             0                 0               0           0  \n",
       "40304       1             0                 0               0           0  \n",
       "40305       1             0                 0               0           0  \n",
       "40306       1             0                 0               0           0  \n",
       "40307       1             1                 0               0           0  \n",
       "\n",
       "[40308 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Create Data Frame\n",
    "d = {'accnum': accnum, 'nw': nw, 'nvocab': nvocab, 'n_neg': n_neg, 'n_pos': n_pos, 'n_uctt': n_uctt, 'n_lit': n_lit, 'n_cstr': n_cstr, \\\n",
    "     'n_modal_week': n_modal1, 'n_modal_moderate': n_modal2, 'n_modal_strong': n_modal3, 'n_negation': n_negation}\n",
    "\n",
    "text_data = pd.DataFrame(data=d)\n",
    "text_data.to_csv('..\\\\filings\\\\text_data_' + obj_type + '_' + str(period_start) + '-' + str(period_end) + '.csv', index=False)\n",
    "\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################################\n",
    "# ################### FOR SINGLE FILE INSPECTION ######################\n",
    "# #####################################################################\n",
    "\n",
    "# ############ Word Tokenization\n",
    "# ## Raw tokens: including punctuations, numbers etc.\n",
    "# with open(processed[1], 'r',  encoding = \"utf-8\") as file:\n",
    "#     contents = file.read().replace('\\n', '').replace('\\xa0', '')\n",
    "# tokens = word_tokenize(contents)\n",
    "\n",
    "# #tokens\n",
    "\n",
    "# ## Convert all words into small cases\n",
    "# ## And keep tokens that purely consist of alphabetic characters only\n",
    "# words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# # words[2500:2600]\n",
    "# # vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_occurrence(tup, lst): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst: \n",
    "#             count+= 1\n",
    "      \n",
    "#     return count\n",
    "\n",
    "# count_occurrence(words, lm_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing\n",
    "\n",
    "# def count_negation(tup, lst, negation): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst:\n",
    "#             if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "#                 neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "#                 for neighborw in neighbor:\n",
    "#                     if neighborw in negation:\n",
    "#                         count+= 1\n",
    "\n",
    "#             if tup.index(item)-4 < 0:\n",
    "#                 pre = tup[0:tup.index(item)+4]\n",
    "#                 for prew in pre:\n",
    "#                     if prew in negation:\n",
    "#                         count+= 1\n",
    "                        \n",
    "#             if tup.index(item)+4 > len(tup):\n",
    "#                 post = tup[tup.index(item)-4:len(tup)]\n",
    "#                 for postw in post:\n",
    "#                     if postw in negation:\n",
    "#                         count+= 1\n",
    "#     return count\n",
    "\n",
    "# count_negation(words, lm_pos, gt_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########### Winsorize words with lenth smaller than 1% and largr than 99% of the document\n",
    "# wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "# wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "# words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### See the most common 20 words\n",
    "# fdist = nltk.FreqDist(words)\n",
    "# fdist.most_common(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
